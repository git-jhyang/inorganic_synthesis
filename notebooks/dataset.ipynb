{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, pickle, gzip, copy, torch, numbers\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from src.data import ReactionDataset, ReactionGraphDataset\n",
    "from src.utils import ActiveElements, MetalElements, Element, AllElements, composit_parser\n",
    "from src.feature import composition_to_feature\n",
    "from pymatgen.core import Composition\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/unique_target.pkl.gz','rb') as f:\n",
    "    unique_target = pickle.load(f)\n",
    "with gzip.open('../data/screened_precursor.pkl.gz','rb') as f:\n",
    "    screened_precursor = pickle.load(f)\n",
    "num_eles = np.zeros((len(ActiveElements), 2))\n",
    "\n",
    "for data in unique_target:\n",
    "    for ele, frac in data['target_comp'].items():\n",
    "        i = ActiveElements.index(ele)\n",
    "        num_eles[i, 0] += data['count']\n",
    "for data in screened_precursor.values():\n",
    "    for ele, frac in data['precursor_comp'].items():\n",
    "        i = ActiveElements.index(ele)\n",
    "        num_eles[i, 1] += data['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmd = {}\n",
    "for fn in os.listdir('../src/elmd'):\n",
    "    if not fn.endswith('json'): continue\n",
    "    elmd[fn.split('.')[0]] = json.load(open(f'../src/elmd/{fn}'))\n",
    "\n",
    "for fn, data in sorted(elmd.items(), key=lambda x: x[0]):\n",
    "    dtype = []\n",
    "    dlen = []\n",
    "#    ele_not_in_table = []\n",
    "    for ele, feat in data.items():\n",
    "        dtype.append(str(type(feat)).split()[1].replace('>','').replace('\\'',''))\n",
    "        if isinstance(feat, (int, float)):\n",
    "            dlen.append(1)\n",
    "        else:\n",
    "            dlen.append(len(feat))\n",
    "#    for ele in elmd['atomic'].keys():\n",
    "#        if ele not in data.keys() and ele not in 'DTUue':\n",
    "#            ele_not_in_table.append((ele, Element(ele).number))\n",
    "    ele_not_in_data = {}\n",
    "    for i, ele in enumerate(ActiveElements):\n",
    "        if ele not in data.keys() and num_eles[i].sum() != 0:\n",
    "            ele_not_in_data[ele] = num_eles[i, :]\n",
    "    print(fn, dtype[0], dlen[0], len(dtype))\n",
    "#    print(ele_not_in_table)\n",
    "    if len(ele_not_in_data) != 0:\n",
    "        print(ele_not_in_data)\n",
    "#    print(ele_not_in_table)\n",
    "#    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ActiveElements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Composition featurization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/unique_ligand.pkl.gz','rb') as f:\n",
    "    unique_ligand = pickle.load(f)\n",
    "with gzip.open('../data/unique_precursor.pkl.gz','rb') as f:\n",
    "    unique_precursor = pickle.load(f)\n",
    "with gzip.open('../data/unique_target.pkl.gz','rb') as f:\n",
    "    unique_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature import composition_to_feature\n",
    "\n",
    "composition_to_feature({'Li':0.2, 'Co':0.2, 'O':0.6}, feature_type='magpie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs = {}\n",
    "mask = np.ones(len(unique_precursor), dtype=bool)\n",
    "for ele in MetalElements:\n",
    "    precs = []\n",
    "    for i, prec in enumerate(unique_precursor):\n",
    "        if ele not in prec['precursor_comp'].keys(): continue\n",
    "        precs.append([len(prec['years']), prec['precursor_comp']])\n",
    "        mask[i] = False\n",
    "    precs = sorted(precs, key=lambda x: x[0], reverse=True)\n",
    "    precs = sorted(precs, key=lambda x: len(list(x[1].keys())))\n",
    "    precs = sorted(precs, key=lambda x: tuple(list(x[1].keys())))\n",
    "    for prec in precs:\n",
    "        n = np.min(list(prec[1].values()))\n",
    "        p = '   '.join([f'{e:2s}:{f/n:.2f}' for e, f in prec[1].items()])\n",
    "        print('{:5d} : {}'.format(prec[0], p))\n",
    "    if len(prec) != 0:\n",
    "        print()\n",
    "\n",
    "precs = []\n",
    "for j in np.where(mask)[0]:\n",
    "    prec = unique_precursor[j]\n",
    "    precs.append([len(prec['years']), prec['precursor_comp']])\n",
    "            \n",
    "precs = sorted(precs, key=lambda x: x[0], reverse=True)\n",
    "precs = sorted(precs, key=lambda x: len(list(x[1].keys())))\n",
    "precs = sorted(precs, key=lambda x: tuple(list(x[1].keys())))\n",
    "for prec in precs:\n",
    "    n = np.min(list(prec[1].values()))\n",
    "    p = '   '.join([f'{e:2s}:{f/n:.2f}' for e, f in prec[1].items()])\n",
    "    print('{:5d} : {}'.format(prec[0], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs = {}\n",
    "for prec in unique_precursor:\n",
    "    n = len(prec['years'])\n",
    "    metals = {}\n",
    "    non_metals = {}\n",
    "    for e, f in prec['precursor_comp'].items():\n",
    "        if e in MetalElements:\n",
    "            metals.update({e:f})\n",
    "        else:\n",
    "            non_metals.update({e:f})\n",
    "    k_m = tuple(sorted(list(metals.keys()), key=lambda x: Element(x).number))\n",
    "    k_fg = tuple(sorted(list(non_metals.keys()), key=lambda x: Element(x).number))\n",
    "    if k_fg not in fgs.keys():\n",
    "        fgs[k_fg] = {'count':0, 'metals':{}}\n",
    "    if k_m not in fgs[k_fg]['metals']:\n",
    "        fgs[k_fg]['metals'][k_m] = {\n",
    "            'count':0, 'case':[]\n",
    "        }\n",
    "    fgs[k_fg]['count'] += n\n",
    "    fgs[k_fg]['metals'][k_m]['count'] += n\n",
    "    fgs[k_fg]['metals'][k_m]['case'].append([prec['precursor_comp'], n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs = {k:v for k,v in sorted(fgs.items(), key=lambda x: len(x[0]))}\n",
    "fgs = {k:v for k,v in sorted(fgs.items(), key=lambda x: x[0])}\n",
    "fgs = {k:v for k,v in sorted(fgs.items(), key=lambda x: x[1]['count'])}\n",
    "for k, v in fgs.items():\n",
    "    m = list(v['metals'].keys())\n",
    "    print(k, v['count'], f'{len(m)}/{len(MetalElements)}')\n",
    "    print(m)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_reaction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43munique_reaction\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_reaction' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precursor dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset()\n",
    "DS.from_file('../data/screened_unique_reaction.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding 및 label 관리를 위해 따로 만들 필요가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#완성 후 삭제함\n",
    "from src.feature import PrecursorDataset\n",
    "PDS = PrecursorDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precursor mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros((PDS.NUM_LABEL, len(MetalElements)+1), dtype=bool)\n",
    "for i, prec in enumerate(PDS.precursor_source):\n",
    "    for ele in prec['precursor_comp'].keys():\n",
    "        if ele in MetalElements:\n",
    "            j = MetalElements.index(ele)\n",
    "            mat[i, j] = True\n",
    "    if mat[i].sum() == 0:\n",
    "        mat[i, -1] = True\n",
    "mat = mat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.matmul(mat.T, mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "js = np.where(mat[k])[0]\n",
    "print(MetalElements[k] if k != -1 else 'None', mat[k].sum())\n",
    "print(js)\n",
    "for i, j in enumerate(js):\n",
    "    print('{:2d} {:4d} {:2d}  {}'.format(i, j, int(((z[j] & mat[k]) != mat[k]).sum() == 0), PDS.precursor_source[j]['precursor_comp']))\n",
    "#np.where(z[0]), np.where(z[43]), np.where(z[180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.where(mat[:, k])[0]:\n",
    "    print(PDS.precursor_source[j]['precursor_comp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/screened_unique_reaction.pkl.gz','rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "pstrs = []\n",
    "for data in dataset:\n",
    "    for prec in data['precursor_comp']:\n",
    "        pstr = composit_parser(prec)\n",
    "        if pstr not in pstrs:\n",
    "            pstrs.append(pstr)\n",
    "PDS.update(pstrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.where(PDS.precursor_mask[-1])[0]:\n",
    "    print(j, PDS.label_to_precursor[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_precursor_mask_from_target(self, target):\n",
    "    mask = self.precursor_mask[-1].copy()\n",
    "    for ele in target.keys():\n",
    "        if ele in MetalElements:\n",
    "            mask = mask | self.precursor_mask[MetalElements.index(ele)]\n",
    "    return mask\n",
    "\n",
    "target = dataset[0]['target_comp']\n",
    "mask = get_precursor_mask_from_target(PDS, target)\n",
    "print(target)\n",
    "for j in np.where(mask)[0]:\n",
    "#    print(j)\n",
    "    out = PDS.get_precursor_info(j)[1]\n",
    "    if isinstance(out, dict):\n",
    "        print(out['precursor_comp'])\n",
    "    else:\n",
    "        print(out)\n",
    "    #['precursor_comp']\n",
    "#PDS.label_to_precursor[j]\n",
    "#PDS.get_precursor_info(9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precursor mask의 적합성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/screened_unique_reaction.pkl.gz','rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    m = np.zeros((len(MetalElements), 2), dtype=int)\n",
    "    for ele in data['target_comp'].keys():\n",
    "        if ele not in MetalElements: continue\n",
    "        i = MetalElements.index(ele)\n",
    "        m[i,0] += 1 \n",
    "    for prec in data['precursor_comp']:\n",
    "        for ele in prec.keys():\n",
    "            if ele not in MetalElements: continue\n",
    "            i = MetalElements.index(ele)\n",
    "            m[i,1] += 1\n",
    "    i, j = m.sum(0)\n",
    "    if (i != j) and (np.sum(m[:, 1][m[:, 0] != 0] == 0) != 0):\n",
    "        print(m[m[:, 1] != 0].T)\n",
    "\n",
    "# target에 있는 모든 metal element는 precursor에 포함되어 있음.\n",
    "# precursor에 있는 모든 metal element는 target에 포함되어 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precursor 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/unique_reaction.pkl.gz','rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "labels = []\n",
    "coprecursorset = []\n",
    "for d in dataset:\n",
    "    precursor_set = {}\n",
    "    for p in d['precursor_comp']:\n",
    "        pstr = composit_parser(p)\n",
    "        if pstr not in PDS.precursor_to_label.keys(): continue\n",
    "        labels.append(PDS.precursor_to_label[pstr])\n",
    "        metal = [e for e in p.keys() if e in MetalElements]\n",
    "        if len(metal) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            metal = metal[0]\n",
    "        if metal not in precursor_set.keys():\n",
    "            precursor_set[metal] = []\n",
    "        precursor_set[metal].append(pstr)\n",
    "    for v in precursor_set.values():\n",
    "        if len(v) > 1:\n",
    "            coprecursorset.append(v)\n",
    "\n",
    "counts = np.zeros(500)\n",
    "for i, c in zip(*np.unique(labels, return_counts=True)):\n",
    "    counts[i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([len(c) for c in coprecursorset], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_precs = []\n",
    "# for data in dataset:\n",
    "#     for prec in data['precursor_comp']:\n",
    "#         pstr = composit_parser(prec)\n",
    "#         if pstr in active_precs:\n",
    "#             continue\n",
    "#         active_precs.append(pstr)\n",
    "# PDS.update(active_precs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature import MetalElements\n",
    "k = 0\n",
    "for i, mask in enumerate(PDS.precursor_mask):\n",
    "    if mask.sum() == 0: continue\n",
    "    js = np.where(mask)[0]\n",
    "    k += 1\n",
    "    #if i < len(MetalElements) and MetalElements[i] not in 'PuNp': continue\n",
    "    if len(js) < 10: continue\n",
    "    print(k, MetalElements[i] if i < len(MetalElements) else 'None', '-'*50)\n",
    "    print(''.join([f'{j:4d}' for j in js]))\n",
    "    print(''.join([f'{counts[j]:4.0f}' for j in js]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_types = ['composit', 'cgcnn', 'magpie_sc', 'mat2vec', 'matscholar', 'megnet16']\n",
    "full_vectors = {ftype: np.vstack([composition_to_feature(prec['precursor_comp'], ftype) for prec in PDS.precursor_source]) for ftype in feat_types}\n",
    "fg_vectors = {ftype: np.vstack([composition_to_feature({k:v for k,v in prec['precursor_comp'].items() if k not in MetalElements}, ftype) for prec in PDS.precursor_source]) for ftype in feat_types}\n",
    "fgs = np.array([composit_parser({k:v for k,v in prec['precursor_comp'].items() if k not in MetalElements}) for prec in PDS.precursor_source])\n",
    "element_source = [np.where(m)[0][0] if m.sum() != 0 else -1 for m in PDS.precursor_mask[:-1, :-2].T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fg_vectors['composit'] != 0).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "labels = {}\n",
    "for ftype in feat_types:\n",
    "    for n_cluster in [10, 15, 20, 25]:        \n",
    "        labels[f'{ftype}+full+{n_cluster}'] = []\n",
    "        labels[f'{ftype}+fg+{n_cluster}'] = []\n",
    "        for seed in range(1000):\n",
    "            full_labels = KMeans(n_cluster, random_state=seed, n_init='auto').fit_predict(full_vectors[ftype])\n",
    "            fg_labels = KMeans(n_cluster, random_state=seed, n_init='auto').fit_predict(fg_vectors[ftype])\n",
    "            labels[f'{ftype}+full+{n_cluster}'].append(full_labels)\n",
    "            labels[f'{ftype}+fg+{n_cluster}'].append(fg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data, cutoff=50):\n",
    "    matrix = np.zeros((len(data[0]), len(data[0])))\n",
    "    for labels in data[:cutoff]:\n",
    "        for i, label_i in enumerate(labels):\n",
    "            for j, label_j in enumerate(labels[i+1:]):\n",
    "                if label_i != label_j: continue\n",
    "                matrix[i,i+j+1] += 1\n",
    "                matrix[i+j+1,i] += 1\n",
    "    return matrix\n",
    "\n",
    "coassignment_matrices = {}\n",
    "consensus_labels = {}\n",
    "for ftype, label in labels.items():\n",
    "    for n in [50, 100, 200, 500, 1000]:\n",
    "        coassignment_matrices[f'{ftype}+{n}'] = get_matrix(label, n)\n",
    "        agc = AgglomerativeClustering(n_clusters=8, metric='precomputed', linkage='average')\n",
    "        consensus_labels[f'{ftype}+{n}'] = agc.fit_predict(1 - coassignment_matrices[f'{ftype}+{n}'] / n)\n",
    "\n",
    "with open('../dump/coassignment_matrices.pkl','wb') as f:\n",
    "    pickle.dump(coassignment_matrices, f)\n",
    "with open('../dump/consensus_labels.pkl','wb') as f:\n",
    "    pickle.dump(consensus_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.pcolormesh(coassignment_matrices['cgcnn+fg+10+100'])\n",
    "mat = get_matrix(np.vstack([v for v in labels.values()]), cutoff=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dump/coassignment_matrix_all.pkl','wb') as f:\n",
    "    pickle.dump(mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (prec, count) in enumerate(zip(*np.unique(fgs, return_counts=True))):\n",
    "    print(f'{i:3d} - {count:3d} : {prec:30s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agc = AgglomerativeClustering(n_clusters=5, metric='precomputed', linkage='average')\n",
    "label = agc.fit_predict(1 - mat / mat.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0.23529, 0.70588\n",
    "c = 1 - a - b\n",
    "1, 4, 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dump/coassignment_matrices.pkl','rb') as f:\n",
    "    coassignment_matrices = pickle.load(f)\n",
    "with open('../dump/consensus_labels.pkl','rb') as f:\n",
    "    consensus_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(consensus_labels['composit+full+20+500'], return_counts=True))\n",
    "print(np.unique(consensus_labels['composit+full+20+1000'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(consensus_labels['composit+fg+10+50'], return_counts=True))\n",
    "print(np.unique(consensus_labels['composit+fg+10+1000'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_labels['composit+fg+10+50'][consensus_labels['composit+fg+10+1000'] == 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def align_labels(labels_1, labels_2, num_clusters=8):\n",
    "    cost_matrix = np.zeros((num_clusters, num_clusters))\n",
    "    for i in range(num_clusters):\n",
    "        for j in range(num_clusters):\n",
    "            cost_matrix[i, j] = np.sum((labels_1 == i) & (labels_2 == j))\n",
    "    _, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "    new_labels_2 = np.zeros_like(labels_2)\n",
    "    for i, idx in enumerate(col_ind):\n",
    "        new_labels_2[labels_2 == i] = idx\n",
    "    return new_labels_2\n",
    "\n",
    "for desc_0 in consensus_labels.keys():\n",
    "    labels_1 = consensus_labels[desc_0]\n",
    "    prefix = '+'.join(desc_0.split('+')[:2])\n",
    "    for desc, labels in consensus_labels.items():\n",
    "        if desc == desc_0: continue\n",
    "        if not desc.startswith(prefix): continue\n",
    "#    if not desc.startswith('composit+full'):\n",
    "#        continue\n",
    "        labels_2 = align_labels(labels_1, labels)\n",
    "        consensus_labels[desc] = labels_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = consensus_labels.keys()\n",
    "for desc_i in descs:\n",
    "    prefix = '+'.join(desc_i.split('+')[:2])\n",
    "    labels_i = consensus_labels[desc_i]\n",
    "    for desc_j in descs:\n",
    "        if not desc_j.startswith(prefix): continue\n",
    "        labels_j = consensus_labels[desc_j]\n",
    "\n",
    "#labels_3 = align_labels(labels_1, labels_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eos and sos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDS.update(DS)\n",
    "#PDS.source_to_label[DS[0].labels]\n",
    "\n",
    "#PDS.label_to_precursor[412]\n",
    "#PDS.source_to_label[DS[0].labels], DS[0].labels\n",
    "PDS.get_precursor_data('SOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "path = '../data/screened_precursor.pkl.gz'\n",
    "with gzip.open(path, 'rb') as f:\n",
    "    unique_precursor = pickle.load(f)\n",
    "unique_precursor[:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Group Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/unique_reaction.pkl.gz','rb') as f:\n",
    "    unique_reactions = pickle.load(f)\n",
    "with gzip.open('../data/screened_precursor.pkl.gz','rb') as f:\n",
    "    screened_precursor = pickle.load(f)\n",
    "with gzip.open('../data/unique_precursor.pkl.gz','rb') as f:\n",
    "    unique_precursor = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.feature import MetalElements, AllElements\n",
    "\n",
    "class LigandTemplateDataset:\n",
    "    def __init__(self, \n",
    "                 feat_type:str = 'composit',\n",
    "                 by_fraction:bool = True,\n",
    "                 *args, **kwargs):\n",
    "        self._feat_type = feat_type\n",
    "        self._by_fraction = by_fraction\n",
    "\n",
    "#        path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../data/screened_precursor.pkl.gz')\n",
    "        path = '../data/screened_precursor.pkl.gz'\n",
    "\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            self._precursor_source = pickle.load(f)\n",
    "        self._active_precursors = [p['precursor_str'] for p in self._precursor_source]\n",
    "        self._precursor_to_source = {c:i for i,c in enumerate(self._active_precursors)}\n",
    "#        self._source_vecs = np.vstack([composition_to_feature(c) for c in self._active_precursors])\n",
    "        self._metal_indexer = {'none': 0}\n",
    "        self._metal_indexer.update({metal:i+1 for i,metal in enumerate(MetalElements)})\n",
    "        self.update()\n",
    "\n",
    "    def update(self, active_precursors=None):\n",
    "        if not isinstance(active_precursors, (list, np.ndarray, tuple, set)):\n",
    "            active_precursors = self._active_precursors\n",
    "        _active_precursors = []\n",
    "        self._ligand_dict = {}\n",
    "        for i_src, precursor in enumerate(self._precursor_source):\n",
    "            if precursor['precursor_str'] not in active_precursors:\n",
    "                continue\n",
    "            _active_precursors.append(precursor['precursor_str'])\n",
    "            n_total = np.sum(list(precursor['precursor_comp'].values()))\n",
    "            i_metal = 0\n",
    "            non_metal = {}\n",
    "            for ele, n in precursor['precursor_comp'].items():\n",
    "                if ele in MetalElements:\n",
    "                    i_metal = self._metal_indexer[ele]\n",
    "                else:\n",
    "                    non_metal[ele] = n/n_total\n",
    "            ligand_str = composit_parser(non_metal)\n",
    "            if ligand_str not in self._ligand_dict.keys():\n",
    "                self._ligand_dict[ligand_str] = {\n",
    "                    'label':None, \n",
    "                    'composition':non_metal,\n",
    "                    'embedding': composition_to_feature(composit_dict = non_metal, \n",
    "                                                        feature_type = self._feat_type,\n",
    "                                                        by_fraction = self._by_fraction,\n",
    "                                                        norm=False).reshape(1,-1),\n",
    "                    'metals':[]}\n",
    "            self._ligand_dict[ligand_str]['metals'].append((i_metal, i_src))\n",
    "\n",
    "        self.NUM_LABEL = len(self._ligand_dict)\n",
    "        self._active_precursors = _active_precursors.copy()\n",
    "\n",
    "        self._source_to_label = np.zeros((len(self._precursor_source), 2), dtype=int)\n",
    "        self._label_to_source = - np.ones((len(MetalElements)+1, self.NUM_LABEL), dtype=int)\n",
    "        self._ligand_str = list(self._ligand_dict.keys())\n",
    "        for j, ligand_info in enumerate(self._ligand_dict.values()):\n",
    "            ligand_info['label'] = j\n",
    "            for i, i_source in ligand_info['metals']:\n",
    "                self._label_to_source[i, j] = i_source\n",
    "                self._source_to_label[i_source] = i, j\n",
    "\n",
    "    def save(self, path, fn='precursor_data.json'):\n",
    "        info = {\n",
    "            'feat_type': self._feat_type,\n",
    "            'by_fraction': self._by_fraction,\n",
    "            'active_precursors': self._active_precursors,\n",
    "        }\n",
    "        with open(os.path.join(path, fn), 'w') as f:\n",
    "            json.dump(info, f, indent=4)\n",
    "    \n",
    "    def load(self, path, fn='precursor_data.json'):\n",
    "        with open(os.path.join(path, fn), 'r') as f:\n",
    "            info = json.load(f)\n",
    "        self.__init__(feat_type = info['feat_type'],\n",
    "                      by_fraction = info['by_fraction'],\n",
    "                      norm = info['norm'])\n",
    "        self.update(info['active_precursors'])\n",
    "        return self\n",
    "\n",
    "    def _check_valid_precursor(self, *args, exit=True):\n",
    "        i_src = None\n",
    "        if len(args) == 1:\n",
    "            if isinstance(args[0], dict):\n",
    "                div = np.sum(list(args[0].values()))\n",
    "                precursor = composit_parser({e:n/div for e,n in args[0].items()})\n",
    "            elif isinstance(args[0], str):\n",
    "                precursor = args[0]\n",
    "            elif exit:\n",
    "                raise ValueError('Precursor must be either `dict` or `str`, got', type(args[0]))\n",
    "            try:\n",
    "                i_src = self._precursor_to_source[precursor]\n",
    "            except:\n",
    "                i_src = None\n",
    "        elif len(args) > 1:\n",
    "            metal, ligand = args[:2]\n",
    "            i_metal, i_ligand = None, None\n",
    "            if isinstance(metal, str) and (metal in self._metal_indexer):\n",
    "                i_metal = self._metal_indexer[metal]\n",
    "            elif isinstance(metal, numbers.Integral) and metal < len(MetalElements) + 1:\n",
    "                i_metal = metal\n",
    "            if isinstance(ligand, str) and (ligand in self._ligand_dict.keys()):\n",
    "                i_ligand = self._ligand_dict[ligand]['label']\n",
    "            elif isinstance(ligand, numbers.Integral) and ligand < len(self._ligand_dict):\n",
    "                i_ligand = ligand\n",
    "            if (i_metal is None) or (i_ligand is None):\n",
    "                i_src = None\n",
    "            else:\n",
    "                i_src = self._label_to_source[i_metal, i_ligand]\n",
    "            if i_src == -1:\n",
    "                i_src = None\n",
    "        if (i_src is None) and exit:\n",
    "            raise ValueError('Invalid precursor', args)\n",
    "        return i_src        \n",
    "\n",
    "    def to_label(self, *args):\n",
    "        i_source = self._check_valid_precursor(args)\n",
    "        _, label = self._source_to_label[i_source]\n",
    "        return label\n",
    "\n",
    "    def get_embedding(self, *args):\n",
    "        i_src = self._check_valid_precursor(args, exit=False)\n",
    "        if i_src is None:\n",
    "            return composition_to_feature({}, feature_type=self._feat_type, by_fraction=self._by_fraction)\n",
    "        else:\n",
    "            _, i_ligand = self._source_to_label[i_src]\n",
    "            return self._ligand_dict[self._ligand_str[i_ligand]]['embedding']\n",
    "    \n",
    "    def get_info(self, *args):\n",
    "        i_source = self._check_valid_precursor(*args)\n",
    "        return self._precursor_source[i_source]\n",
    "    \n",
    "    def get_mask(self, metal):\n",
    "        if metal in MetalElements:\n",
    "            i_metal = MetalElements.index(metal) + 1\n",
    "        else:\n",
    "            i_metal = 0\n",
    "        return (self._label_to_source[i_metal] != -1).reshape(1,-1)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'feature_type': self._feat_type, \n",
    "            'NUM_LABEL': self.NUM_LABEL,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = LigandTemplateDataset()\n",
    "#test.update([p['precursor_str'] for p in screened_precursor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(test.get_embedding(), 4, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(test._label_to_source[0] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask[0]\n",
    "metals, mask = test.get_ligand_mask_from_target({'Ti':0.111, 'Li':0.3333, 'Co':0.3333, 'O':0.666667})\n",
    "for mele, m in zip(metals, mask):\n",
    "    for (ligand, metal), active in zip(test._ligand_dict.items(), m):\n",
    "        if not active: continue\n",
    "        print(mele in [_m[0] for _m in metal['metals']], mele, '   {:3f}   '.format(np.sum(list(metal['composition'].values()))), ligand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to manage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_precursor = {}\n",
    "for data in unique_reactions:\n",
    "    for p in data['precursor_comp']:\n",
    "        comp = Composition(p)\n",
    "        new_comp = str(Composition(comp.get_integer_formula_and_factor()[0]))\n",
    "        if new_comp not in count_precursor.keys():\n",
    "            count_precursor[new_comp] = 0\n",
    "        count_precursor[new_comp] += 1\n",
    "\n",
    "prec_comps = []\n",
    "for prec in screened_precursor:\n",
    "    comp = Composition(prec['precursor_comp'].items())\n",
    "    new_comp = Composition(comp.get_integer_formula_and_factor()[0]).as_dict()\n",
    "    prec_comps.append(new_comp)\n",
    "    for e, f in new_comp.items():\n",
    "        if e not in MetalElements: continue\n",
    "        if f != 1: print(new_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs_int = {}\n",
    "fgs_norm_full  = {}\n",
    "fgs_norm_metal = {}\n",
    "for p in prec_comps:\n",
    "    pstr = str(Composition(p))\n",
    "    pc = count_precursor[pstr]\n",
    "    ns = 0\n",
    "    metal_n = 1\n",
    "    others = {}\n",
    "    metal = 'none'\n",
    "    metal_ele = 'none'\n",
    "    for e,n in p.items():\n",
    "        ns += n\n",
    "        if e in MetalElements:\n",
    "            metal = f'{e}{n:.0f}'\n",
    "            metal_ele = e\n",
    "            metal_n = n\n",
    "        else:\n",
    "            others[e] = n    \n",
    "    norm_by_metal = {k:v/metal_n for k,v in others.items()}\n",
    "    norm_by_all = {k:v/ns for k,v in others.items()}\n",
    "    intkey = str(Composition(others))\n",
    "    mfrkey = str(Composition(norm_by_metal))\n",
    "    frkey = str(Composition(norm_by_all))\n",
    "\n",
    "    if intkey not in fgs_int.keys():\n",
    "        fgs_int[intkey] = []\n",
    "    if frkey not in fgs_norm_full.keys():\n",
    "        fgs_norm_full[frkey] = []\n",
    "    if mfrkey not in fgs_norm_metal.keys():\n",
    "        fgs_norm_metal[mfrkey] = []\n",
    "    fgs_int[intkey].append((metal, pc))\n",
    "    fgs_norm_full[frkey].append((metal_ele, pc))\n",
    "    fgs_norm_metal[mfrkey].append((metal_ele, pc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_int = {}\n",
    "metal_fr = {}\n",
    "for e in MetalElements:\n",
    "    metal_int[e] = {}\n",
    "    metal_fr[e] = {}\n",
    "    for prec in prec_comps:\n",
    "        if e not in prec.keys():\n",
    "            continue\n",
    "        ns = np.sum([v for v in prec.values()])\n",
    "        mc = [v for e,v in prec.items() if e in MetalElements][0]\n",
    "        prec_str = str(Composition(prec))\n",
    "        fg_int = str(Composition({e:v for e,v in prec.items() if e not in MetalElements}))\n",
    "        fg_fr  = str(Composition({e:v/ns for e,v in prec.items() if e not in MetalElements}))\n",
    "        if fg_int not in metal_int[e].keys():\n",
    "            metal_int[e][fg_int] = [0, []]\n",
    "        if fg_fr not in metal_fr[e].keys():\n",
    "            metal_fr[e][fg_fr] = [0, []]\n",
    "        metal_int[e][fg_int][0] += count_precursor[prec_str]\n",
    "        metal_int[e][fg_int][1].append(mc)\n",
    "        metal_fr[e][fg_fr][0] += count_precursor[prec_str]\n",
    "        metal_fr[e][fg_fr][1].append(mc)\n",
    "    if len(metal_int[e]) > 1:\n",
    "        print(e, '-' * 50)\n",
    "        for p, (c, mcs) in metal_int[e].items():\n",
    "            print(f'    {p:15s} : {c:4d} / {mcs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "num = re.compile(r'\\d')\n",
    "for data_dict in [fgs_int, fgs_norm_full, fgs_norm_metal]:\n",
    "    for fg, metals in data_dict.items():\n",
    "        for i, (metal_i, c_i) in enumerate(metals):\n",
    "            ele_i = num.sub('', metal_i)\n",
    "            for j, (metal_j, c_j) in enumerate(metals[i+1:]):\n",
    "                ele_j = num.sub('', metal_j)\n",
    "                if ele_i == ele_j:\n",
    "                    print(fg, '-', metal_i, c_i, metal_j, c_j)\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_by_fg(ax, data):\n",
    "    fgs, n_metals, n_precs = np.array([[fg, len(metals), np.sum([m[1] for m in metals])] for fg, metals in data.items()]).T\n",
    "    n_precs = n_precs.astype(float)\n",
    "    n_metals = n_metals.astype(float)\n",
    "    o1 = np.argsort(n_precs)[::-1]\n",
    "    o = sorted(o1, key=lambda x: n_metals[x], reverse=True)\n",
    "    x = np.arange(len(n_metals))\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(x, n_metals[o], '-o', color='b')\n",
    "    for i in range(0, len(n_metals), 10):\n",
    "        ax.axvline(i, ls='--', color='k', lw=0.5)\n",
    "    for i in range(0,70,10):\n",
    "        ax.axhline(i, ls='--', color='k', lw=0.5)\n",
    "    ax2.plot(x, n_precs[o], '-o', color='r')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim([2, 1e5])\n",
    "    ax.set_xlabel('Ligands', fontsize=14)\n",
    "    ax.set_ylabel('# of Metal elements', fontsize=14, color='b')\n",
    "    ax2.set_ylabel('Usage', fontsize=14, color='r')\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(11,4))\n",
    "draw_by_fg(ax, fgs_int)\n",
    "f, ax = plt.subplots(1,1,figsize=(11,4))\n",
    "draw_by_fg(ax, fgs_norm_metal)\n",
    "f, ax = plt.subplots(1,1,figsize=(11,4))\n",
    "draw_by_fg(ax, fgs_norm_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros((len(MetalElements)+1, len(fgs_norm_full))) + 1e-4\n",
    "for j, metals in enumerate(sorted(fgs_norm_full.values(), key=lambda x: len(x), reverse=True)):\n",
    "    for metal, v in metals:\n",
    "        if metal == 'none':\n",
    "            i = 0\n",
    "        else:\n",
    "            i = MetalElements.index(metal) + 1\n",
    "        mat[i,j] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "cmap.set_under([0.7,0.7,0.7])\n",
    "f, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "ax.pcolormesh(np.log10(mat), cmap=cmap, vmin=0)\n",
    "ax.set_ylabel('Metals', fontsize=14)\n",
    "ax.set_xlabel('Ligands', fontsize=14)\n",
    "for i in range(1, 12):\n",
    "    ax.axvline(i * 10, ls='--', color='k', lw=0.5)\n",
    "for i in range(1, 9):\n",
    "    ax.axhline(i * 10, ls='--', color='k', lw=0.5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제점\n",
    "  - 금속 기준으로 normalize를 할 경우 metal-free precursor의 예측이 어려워짐\n",
    "  - integer로 변경해서 사용 할 경우 큰 수가 나오는 경우가 있음 (200 이상)\n",
    "- intrinsic한 parsing error로 인한 한계도 있음. 그냥 fractional로 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_by_metal(ax, data):\n",
    "    _, n_fgs, n_precs = np.array([[metal, len(fgs), np.sum([m[0] for m in fgs.values()])] for metal, fgs in data.items() if len(fgs) != 0]).T\n",
    "    o1 = np.argsort(n_precs.astype(float))[::-1]\n",
    "    o = sorted(o1, key=lambda x: n_fgs.astype(float)[x], reverse=True)\n",
    "\n",
    "    x = np.arange(len(n_fgs))\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(x, n_fgs.astype(float)[o], '-o', color='b')\n",
    "    ax2.plot(x, n_precs.astype(float)[o], '-o', color='r')\n",
    "    ax2.set_yscale('log')\n",
    "    ax.set_ylim([-1,23])\n",
    "    ax2.set_ylim([2, 1e4])\n",
    "    for i in range(0, len(n_fgs), 5):\n",
    "        ax.axvline(i, ls='--', color='k', lw=0.5)\n",
    "    for i in range(0,2,5):\n",
    "        ax.axhline(i, ls='--', color='k', lw=0.5)\n",
    "    ax.set_ylabel('# of ligands', fontsize=14, color='b')\n",
    "    ax.set_xlabel('Metal elements', fontsize=14)\n",
    "    ax2.set_ylabel('Usage', fontsize=14, color='r')\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(11,4))\n",
    "draw_by_metal(ax, metal_int)\n",
    "f, ax = plt.subplots(1,1,figsize=(11,4))\n",
    "draw_by_metal(ax, metal_fr)\n",
    "f.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data object test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import torch, os, gzip, pickle\n",
    "from src.data import ReactionData\n",
    "from typing import Dict, List\n",
    "from src.utils import ActiveElements, MetalElements, Element\n",
    "from src.feature import composition_to_feature\n",
    "\n",
    "with gzip.open('../data/screened_conditional_reaction.pkl.gz','rb') as f:\n",
    "    cond_rxn = pickle.load(f)\n",
    "with gzip.open('../data/screened_unique_reaction.pkl.gz','rb') as f:\n",
    "    uniq_rxn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc1(x):\n",
    "    return (x + 1) ** 3\n",
    "def fnc2(x):\n",
    "    return (x / 2 + 1) ** 0.5\n",
    "def fnc3(x):\n",
    "    return (x - 1) ** 2 / 2\n",
    "def fnc4(x):\n",
    "    return (x + 2) ** 2\n",
    "\n",
    "def cfn1(n, x):\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    d3 = []\n",
    "    d4 = []\n",
    "    for i in range(n):\n",
    "        if x:\n",
    "            v2 = fnc2(i)\n",
    "            v1 = fnc1(i)\n",
    "            d1.append(v1)\n",
    "            d2.append(v2)\n",
    "        if not x:\n",
    "            v3 = fnc3(i)\n",
    "            v4 = fnc4(i)\n",
    "            d3.append(v3)\n",
    "            d4.append(v4)\n",
    "    return d1, d2, d3, d4\n",
    "\n",
    "def cfn2(n, x):\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    d3 = []\n",
    "    d4 = []    \n",
    "    if x:\n",
    "        d1 = [fnc1(i) for i in range(n)]\n",
    "        d2 = [fnc2(i) for i in range(n)]\n",
    "    if not x:\n",
    "        d3 = [fnc3(i) for i in range(n)]\n",
    "        d4 = [fnc4(i) for i in range(n)]\n",
    "    return d1, d2, d3, d4\n",
    "\n",
    "%timeit cfn1(500, True)\n",
    "%timeit cfn2(500, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i, rxn in enumerate(uniq_rxn):\n",
    "    eles = {}\n",
    "    for prec in rxn['precursor_comp']:\n",
    "        metal_ele = tuple([e for e in prec.keys() if e in MetalElements])\n",
    "        if metal_ele not in eles.keys():\n",
    "            eles[metal_ele] = 0\n",
    "        eles[metal_ele] += 1\n",
    "#    if np.max(list(eles.values())) > 2:\n",
    "#        print(i, eles)\n",
    "    if tuple([]) in eles:\n",
    "        cnt += 1\n",
    "        print(i, eles)\n",
    "print(cnt, len(uniq_rxn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rxn[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rxn[29022 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rxn[7168 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rxn[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = uniq_rxn[0]\n",
    "rxn_data = ReactionData(data = data,\n",
    "                        feat_type = 'composit',\n",
    "                        target_comp = data['target_comp'],\n",
    "                        precursor_comps = data['precursor_comp'],\n",
    "#                        conditions = ['heat_temp'],\n",
    "#                        condition_values = [data['target_comp']['heat_temp']['median']],\n",
    "                        weights = None)\n",
    "\n",
    "rxn_data.precursor_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxn_data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rxn_data.metal_feat\n",
    "#rxn_data.target_feat\n",
    "for f in rxn_data.precursor_feat.numpy():\n",
    "    ii = np.where(f)[0]\n",
    "    print(ii, [(ActiveElements[i], f[i]) for i in ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxn_data.precursor_feat[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData(ReactionData):\n",
    "    def __init__(self, \n",
    "                 data : Dict = {},\n",
    "                 feat_type : str = 'composit',\n",
    "                 target_comp : Dict = {},\n",
    "                 precursor_comps : List[Dict] = [],\n",
    "                 conditions : List[str] = [],\n",
    "                 condition_values : List[float] = [],\n",
    "                 weights : float = None,\n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        super().__init__(data = data,\n",
    "                         feat_type = feat_type,\n",
    "                         target_comp = target_comp,\n",
    "                         precursor_comps = precursor_comps,\n",
    "                         conditions = conditions,\n",
    "                         condition_values = condition_values,\n",
    "                         weights = weights,\n",
    "                         *args, **kwargs)\n",
    "\n",
    "        # graph \n",
    "        self._feature_attrs.extend(['edge_feat','edge_index'])\n",
    "        edge_index = []\n",
    "        edge_feat = []\n",
    "        for i, metal_i in enumerate(self.metal_comp):\n",
    "            for j, metal_j in enumerate(self.metal_comp):\n",
    "                edge_index.append([i,j])\n",
    "                if i == j:\n",
    "                    edge_feat.append(self.target_feat)\n",
    "                else:\n",
    "                    edge_comp = {}\n",
    "                    for e, f in self.target_comp.items():\n",
    "                        if e in metal_i.keys() or e in metal_j.keys():\n",
    "                            edge_comp.update({e:f})\n",
    "                        elif e in MetalElements:\n",
    "                            continue\n",
    "                        elif len(metal_i) == 0 or len(metal_j) == 0:\n",
    "                            edge_comp.update({e:f})\n",
    "                    edge_feat.append(\n",
    "                        composition_to_feature(composit_dict = edge_comp, \n",
    "                                               feature_type = feat_type, \n",
    "                                               by_fraction = True,\n",
    "                                               norm = True)\n",
    "                    )\n",
    "        self.edge_index = np.array(edge_index, dtype=int).T\n",
    "        self.edge_feat = np.vstack(edge_feat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_LABEL = 999\n",
    "class SequenceData(ReactionData):\n",
    "    def __init__(self, \n",
    "                 data : Dict = {},\n",
    "                 feat_type : str = 'composit',\n",
    "                 target_comp : Dict = {},\n",
    "                 precursor_comps : List[Dict] = [],\n",
    "                 conditions : List[str] = [],\n",
    "                 condition_values : List[float] = [],\n",
    "                 max_length : int = 8,\n",
    "                 weights : float = None,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(data = data,\n",
    "                         feat_type = feat_type,\n",
    "                         target_comp = target_comp,\n",
    "                         precursor_comps = precursor_comps,\n",
    "                         conditions = conditions,\n",
    "                         condition_values = condition_values,\n",
    "                         weights = weights,\n",
    "                         *args, **kwargs)\n",
    "        \n",
    "        self._feature_attrs.pop(self._feature_attrs.index('metal_feat'))\n",
    "        delattr(self, 'metal_feat')\n",
    "        self.n = max_length\n",
    "\n",
    "        # labels & precursor feat\n",
    "        pad = composition_to_feature({}, feature_type=feat_type)\n",
    "        if hasattr(self, 'precursor_feat'):\n",
    "            self.m = self.labels.shape[0]\n",
    "            self.precursor_feat = np.vstack([\n",
    "                pad.reshape(1,-1), self.precursor_feat, *[pad] * max_length\n",
    "            ])[:max_length]\n",
    "            self.labels = np.hstack([\n",
    "                self.labels.reshape(-1), [EOS_LABEL] * max_length\n",
    "            ])[:max_length].astype(int)\n",
    "        else:\n",
    "            self._feature_attrs.append('precursor_feat')\n",
    "            self.precursor_feat = pad.reshape(1,-1)\n",
    "        self.to_torch()\n",
    "\n",
    "    def shuffle(self):\n",
    "        if not hasattr(self, 'm'): \n",
    "            return\n",
    "        j = np.random.permutation(self.m)\n",
    "        i1 = np.arange(self.n)\n",
    "        i2 = np.arange(self.n)\n",
    "        i1[:self.m] = j\n",
    "        i2[1:self.m+1] = j+1\n",
    "        return self.precursor_feat[i2], self.labels[i1]\n",
    "\n",
    "seq_data = SequenceData(data = data,\n",
    "                        feat_type = 'composit',\n",
    "                        target_comp = data['target_comp'],\n",
    "                        precursor_comps = data['precursor_comp'],\n",
    "#                        conditions = ['heat_temp'],\n",
    "#                        condition_values = [data['target_comp']['heat_temp']['median']],\n",
    "                        )\n",
    "seq_data.precursor_feat.shape, seq_data.target_feat.shape, seq_data.labels\n",
    "seq_data.shuffle()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import composit_parser\n",
    "check = {}\n",
    "for i in range(10000):\n",
    "    comp, lbl = seq_data.shuffle()\n",
    "    for j, (c, l) in enumerate(zip(comp[1:].cpu().numpy(), lbl.cpu().numpy())):\n",
    "        if l == 999: break\n",
    "        pstr = composit_parser({ActiveElements[k]: c[k] for k in np.where(c)[0]})\n",
    "        if pstr not in check.keys():\n",
    "            check[pstr] = {'order':[0]*5}\n",
    "        if l not in check[pstr].keys():\n",
    "            check[pstr][l] = 0\n",
    "        check[pstr][l] += 1\n",
    "        check[pstr]['order'][j] += 1\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros(5)\n",
    "x = np.array([\n",
    "    [1,1,1,1,1],\n",
    "    [2,2,2,2,2],\n",
    "    [3,3,3,3,3],\n",
    "    [4,4,4,4,4],\n",
    "    [5,5,5,5,5],\n",
    "])\n",
    "np.vstack([\n",
    "    p, x, *[p] * 5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['precursor_comp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from src.data import ReactionGraphDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['cgcnn','elemnet','magpie','magpie_sc','mat2vec','matscholar','megnet16','oliynyk','oliynyk_sc']\n",
    "ds = ReactionGraphDataset(feat_type='composit')\n",
    "ds.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "             heat_temp_key=('heat_temp', 'median'))\n",
    "#ds.from_file('../data/surxn.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.num_meta_feat + ds.has_temp_info + ds.has_time_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1900\n",
    "for l,m in zip(ds[i].label, ds[i].label_mask):\n",
    "    print(l[m], m.sum())\n",
    "ds[i].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, info = ds.cfn(ds[80:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat['condition_feat'].shape, feat['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate_fnc design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfn(self, dataset):\n",
    "    info = []\n",
    "    rxn_id = []\n",
    "    meta_feat = []\n",
    "    edge_feat = []\n",
    "    edge_index = []\n",
    "    condition_feat = []\n",
    "    prec_feat = []\n",
    "    label = []\n",
    "    label_mask = []\n",
    "    weight = []\n",
    "    n = 0\n",
    "    for i, data in enumerate(dataset):\n",
    "        info.append(data.to_dict())\n",
    "        rxn_id.append([i] * data.n)\n",
    "        meta_feat.append(data.meta_feat)\n",
    "        edge_feat.append(data.edge_feat)\n",
    "        edge_index.append(data.edge_index + n)\n",
    "        n += data.n\n",
    "    rxn_id = np.hstack(rxn_id).astype(int)\n",
    "    meta_feat = torch.vstack(meta_feat).float()\n",
    "    edge_feat = torch.vstack(edge_feat).float()\n",
    "    edge_index = torch.vstack(edge_index).long()\n",
    "\n",
    "    if self.has_temp_info or self.has_time_info:\n",
    "        for data in dataset:\n",
    "            condition_feat.append(data.condition_feat.repeat(data.n, 1))\n",
    "        condition_feat = torch.vstack(condition_feat).float()\n",
    "\n",
    "    if self._train:\n",
    "        for data in dataset:\n",
    "            prec_feat.append(data.precursor_feat)\n",
    "            label.append(data.label)\n",
    "            label_mask.append(data.label_mask)\n",
    "            weight.append(data.weight.repeat(data.label.shape))\n",
    "        prec_feat = torch.vstack(prec_feat)\n",
    "        label = torch.vstack(label)\n",
    "        label_mask = torch.vstack(label_mask)\n",
    "        weight = torch.vstack(weight)\n",
    "\n",
    "    return {\n",
    "        'rxn_id' : rxn_id,\n",
    "        'x' : meta_feat,\n",
    "        'edge_attr' : edge_feat,\n",
    "        'edge_index' : edge_index,\n",
    "        'condition_feat' : condition_feat,\n",
    "        'prec_feat' : prec_feat,\n",
    "        'label' : label,\n",
    "        'label_mask' : label_mask,\n",
    "        'weight' : weight,\n",
    "    }, info\n",
    "\n",
    "feat, info = cfn(ds, ds[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [d.to_dict() for d in ds]\n",
    "info[88], info[89]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = {}\n",
    "for d in info:\n",
    "    id = d['id_urxn']\n",
    "    if id not in stat:\n",
    "        stat[id] = {'count':d['count'], 'ids':[]}\n",
    "    stat[id]['ids'].append(d['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([len(v['ids']) - v['count'] for k, v in stat.items()], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in stat.items():\n",
    "    if v['count'] - len(v['ids']) > 0:\n",
    "        print(v['count'], len(v['ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성 후 삭제함\n",
    "cfn = ds.cfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, info = cfn(ds, ds[:48])\n",
    "#feat, info = ds.cfn(ds[:48])\n",
    "for k,v in feat.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = torch.stack([torch.ones(10) * i for i in range(445)])\n",
    "embd[feat['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(445, 10, padding_idx=444)\n",
    "#embed.to('cuda')\n",
    "#embed = embed.from_pretrained(embd)\n",
    "embed.padding_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feat_type in ['composit','cgcnn','elemnet','magpie','magpie_sc','mat2vec','matscholar','megnet16','oliynyk','oliynyk_sc']:\n",
    "    ds = ReactionDataset(feat_type=feat_type)\n",
    "    ds.from_file('../data/screened_conditional_reaction.pkl.gz')\n",
    "    feat, info = ds.cfn(ds)\n",
    "    n = len(ds)\n",
    "    mask = np.hstack([np.zeros((n, 1), dtype=bool), (feat['label'] != EOS_LABEL)[..., :-1].cpu().numpy()]).reshape(-1)\n",
    "    inps = feat['inp'].numpy().reshape(n * 8, -1)[mask].T\n",
    "    conds = feat['condition'].numpy().reshape(n * 8, -1)[mask].T\n",
    "    f, ax = plt.subplots(1, 1, figsize=(25, 4))\n",
    "    f.suptitle(feat_type)\n",
    "    for i, (inp, cond) in enumerate(zip(inps, conds)):\n",
    "        ic = np.hstack([inp, cond])\n",
    "        if ic.max() - ic.min() < 1e-2: continue\n",
    "        y = np.linspace(ic.min(), ic.max(), 100)\n",
    "        for j, (v, lbl) in enumerate(zip([inp, cond], ['precursor','target'])):\n",
    "            x1 = gaussian_kde(v)(y)\n",
    "            x0 = np.ones_like(y) * i\n",
    "            if i == 0:\n",
    "                ax.fill_betweenx(y, x0, x1 / x1.max() + i, color=plt.cm.tab10(j), alpha=0.3, label=lbl)\n",
    "                ax.plot([i, i+1], [v.mean(), v.mean()], ls='--', color=plt.cm.tab10(j), lw=2, label='Avg.')\n",
    "            else:\n",
    "                ax.fill_betweenx(y, x0, x1 / x1.max() + i, color=plt.cm.tab10(j), alpha=0.3)\n",
    "                ax.plot([i, i+1], [v.mean(), v.mean()], ls='--', color=plt.cm.tab10(j), lw=2)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlim([-1, i+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conditions - Temp. & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ReactionDataset(feat_type='composit')\n",
    "ds.from_file('../data/screened_conditional_reaction.pkl.gz', heat_temp_key=['heat_temp','median'])\n",
    "temp_median = ds.cfn(ds)[0]['condition'][:, 0, -1]\n",
    "ds.from_file('../data/screened_conditional_reaction.pkl.gz', heat_temp_key=['heat_time','median'])\n",
    "time_median = ds.cfn(ds)[0]['condition'][:, 0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(4, 1, figsize=(10, 8))\n",
    "for i, (val, fnc) in enumerate([(temp_median, lambda x: x / 1000 - 1), \n",
    "                                (time_median, lambda x: np.log10(x) - 1)]):\n",
    "    for j, v in enumerate([val.numpy(), fnc(val.numpy())]):\n",
    "        vmax, vmin = v.max(), v.min()\n",
    "        y = np.linspace(vmin - (vmax - vmin) * 0.1, vmax + (vmax - vmin) * 0.1, 100)\n",
    "        x1 = gaussian_kde(v)(y)\n",
    "        x0 = np.zeros_like(y) \n",
    "        axs[i * 2 + j].fill_between(y, x0, x1 / x1.max())\n",
    "        axs[i * 2 + j].plot([vmax, vmax], [0, 1])\n",
    "        axs[i * 2 + j].plot([vmin, vmin], [0, 1])\n",
    "        axs[i * 2 + j].set_ylim([0, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_median.min(), temp_mean.min(), time_median.min(), time_mean.min(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, os\n",
    "sys.path.append('..')\n",
    "from src.networks import BaseNetwork, TransformerDecoderBlock\n",
    "from src.data import ReactionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReactionDataset()\n",
    "train_ds.from_file('../data/screened_conditional_reaction.pkl.gz', heat_temp_key=('heat_temp','median'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funcionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb = TransformerDecoderBlock(72)\n",
    "tdb.device, tdb._dummy, tdb.state_dict()['target_embed_layer.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdb.to('cuda')\n",
    "#tdb._dummy = torch.tensor([0])\n",
    "#tdb._dummy.to('cuda')\n",
    "#tdb.positional_encoding.pe = tdb.positional_encoding.pe.to('cuda')\n",
    "tdb.positional_encoding.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### positional encoding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "hidden_dim = 64\n",
    "\n",
    "pe = torch.zeros(1, max_len, hidden_dim)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-np.log(10000.0) / hidden_dim))\n",
    "pe[..., 0::2] = torch.sin(position * div_term)\n",
    "pe[..., 1::2] = torch.cos(position * div_term)\n",
    "for i in range(9):\n",
    "    plt.plot(pe[0,i,::2], color=plt.cm.viridis(i/8))\n",
    "    plt.plot(pe[0,i,1::2], ls='--', color=plt.cm.viridis(i/8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pec = PositionalEncoding(hidden_dim)\n",
    "print(np.max(np.abs(pec(100)[0].numpy() - pe.numpy()[0]).reshape(-1)))\n",
    "plt.hist(np.abs(pec(100)[0].numpy() - pe.numpy()[0]).reshape(-1))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(pec(100)[0].numpy() - pe.numpy()[0], cmap='RdBu', vmin=-1e-6, vmax=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, info = train_ds.cfn(train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for d in train_ds[:10]:\n",
    "    test.append(d.precursor_feat)\n",
    "\n",
    "torch.concat(test).shape\n",
    "#d.precursor_feat.shape\n",
    "#d.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + out[1]\n",
    "y[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forwarding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TransformerDecoderBlock(NUM_LABEL, 98)\n",
    "out = Model(feat['labels'], feat['conditions'])\n",
    "#out[1]\n",
    "#out[1].shape\n",
    "#\n",
    "#x = torch.ones(10, 8, 32) * torch.arange(0, 10, 1).view(-1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = feat['conditions']\n",
    "\n",
    "output_seq = torch.ones(context.shape[0], 1).long() * SOS_LABEL\n",
    "for i in range(5):\n",
    "    o = Model(output_seq, feat['conditions'])\n",
    "    output_seq = torch.hstack([output_seq, o.argmax(-1)[:, -1:]])\n",
    "output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- done in sequence.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import ReactionGraphDataset, ReactionDataset\n",
    "from src.networks import GraphCVAE, CVAE, GraphConvolutionBlock, GraphAttentionBlock\n",
    "from src.trainer import VAETrainer, BaseTrainer\n",
    "\n",
    "GDS = ReactionGraphDataset(feat_type='cgcnn')\n",
    "GDS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "              heat_temp_key=('heat_temp','median'))\n",
    "\n",
    "# DS = ReactionDataset()\n",
    "# DS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "#              heat_temp_key=('heat_temp','median'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 /   0.0875   0.0072   0.0053 /    31.6707    26.3887\n",
      "   1 /   0.0081   0.0085   0.0049 /    37.5922    28.6300\n",
      "   2 /   0.0024   0.0013   0.0012 /    37.9308    28.7833\n",
      "   3 /   0.0019   0.0011   0.0009 /    37.5561    29.9825\n",
      "   4 /   0.0009   0.0007   0.0007 /    38.3305    29.5627\n",
      "   5 /   0.0006   0.0008   0.0004 /    35.4856    27.1718\n",
      "   6 /   0.0004   0.0003   0.0003 /    37.0032    28.4475\n",
      "   7 /   0.0017   1.4812   0.5392 / 13628.4580  5052.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhyang/anaconda3/envs/isyn2/lib/python3.9/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 /   0.0550 78508.1088 14256229376.0144 /        inf        inf\n",
      "   9 /   0.0308   0.0658 36437.9225 / 143999912706324484179684266868736.0000        inf\n",
      "  10 /   0.0252   0.0616 298211.9593 / 11139675244031772955735713478868992.0000        inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 77\u001b[0m\n\u001b[1;32m     68\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# for j, batch in enumerate(train_dl):\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     _loss, _output = tr._eval_batch(batch, True, 1e-4)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m#     tr.opt.step()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# if _loss.isnan().item(): break\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     valid_loss, valid_output \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mtest(valid_dl)\n\u001b[1;32m     79\u001b[0m     test_loss, test_output \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mtest(test_dl)\n",
      "File \u001b[0;32m~/WORKSPACES/CODES/inorganic_synthesis/notebooks/../src/trainer.py:21\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, dataloader, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 21\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[97], line 43\u001b[0m, in \u001b[0;36mTestTR._eval_batch\u001b[0;34m(self, batch, compute_loss, beta, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m _feat, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# x = _feat['condition'].to('cuda')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# edge_attr = _feat['edge_attr'].to('cuda')\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# edge_index = _feat['edge_index'].to('cuda')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# pred = self.model(x=x, edge_attr=edge_attr, edge_index=edge_index)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# output = [pred.detach()]\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m pred, kld, l, z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m mu, log_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(l\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# output = [pred.detach(), kld.detach(), mu, log_var.exp(), z.detach()]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/WORKSPACES/CODES/inorganic_synthesis/notebooks/../src/networks.py:649\u001b[0m, in \u001b[0;36mGraphCVAE.forward\u001b[0;34m(self, x, edge_index, edge_attr, condition, *args, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x \u001b[38;5;241m=\u001b[39m x, edge_index \u001b[38;5;241m=\u001b[39m edge_index, edge_attr \u001b[38;5;241m=\u001b[39m edge_attr)\n\u001b[1;32m    648\u001b[0m z, kld \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterization(l)\n\u001b[0;32m--> 649\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, kld, l, z\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/WORKSPACES/CODES/inorganic_synthesis/notebooks/../src/networks.py:242\u001b[0m, in \u001b[0;36mGraphConvolutionBlock.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    240\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_embed_layer(x)\n\u001b[1;32m    241\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_embed_layer(edge_attr)\n\u001b[0;32m--> 242\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(h)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.sequential_52d1e7_gamy54z1.py:37\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_4(x, edge_index, edge_attr)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_5(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_7(x)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch_geometric/nn/conv/cg_conv.py:88\u001b[0m, in \u001b[0;36mCGConv.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(out)\n\u001b[1;32m     90\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:538\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    536\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 538\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mcollect_param_data(\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/anaconda3/envs/isyn2/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:421\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madj_t\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    420\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[0;32m--> 421\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index_i\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    422\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index_j\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m edge_index[j]\n\u001b[1;32m    423\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mptr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "years = np.array([d.year for d in GDS])\n",
    "train_idx = np.where(years < 2017)[0]\n",
    "valid_idx = np.where((years > 2016) & (years < 2019))[0]\n",
    "test_idx = np.where(years > 2018)[0]\n",
    "\n",
    "train_dl = DataLoader(GDS, batch_size=256, sampler=SubsetRandomSampler(train_idx), collate_fn=GDS.cfn)\n",
    "valid_dl = DataLoader(GDS, batch_size=2048, sampler=valid_idx, collate_fn=GDS.cfn)\n",
    "test_dl = DataLoader(GDS, batch_size=2048, sampler=test_idx, collate_fn=GDS.cfn)\n",
    "\n",
    "model = GraphCVAE(\n",
    "    input_dim=GDS.num_precursor_feat, latent_dim=16, \n",
    "    condition_dim=GDS.num_meta_feat + GDS.has_temp_info + GDS.has_time_info, \n",
    "    edge_dim=GDS.num_edge_feat, output_dim=GDS.NUM_LABEL + 5, graph='conv',\n",
    "    encoder_hidden_dim=128, encoder_hidden_layers=4,\n",
    "    decoder_hidden_dim=128, decoder_hidden_layers=4,\n",
    "    batch_norm=True, dropout=0,\n",
    ")\n",
    "\n",
    "class TestTR(BaseTrainer):\n",
    "    def __init__(self, model, lr, device='cuda', crit=torch.nn.CrossEntropyLoss(reduction='none')):\n",
    "        super().__init__(model, lr, device, crit,\n",
    "                         feat_keys=['label','label_mask','rxn_id','weight'],\n",
    "                        #  output_keys=['pred','kld','mu','log_var','z']\n",
    "                        output_keys=['n_label','pred','kld','mu','log_var','z']\n",
    "#                         output_keys=['pred',]\n",
    "                         )\n",
    "    \n",
    "    def _eval_batch(self, batch, compute_loss=True, beta=0.01, *args, **kwargs):\n",
    "        _feat, _ = batch\n",
    "        # x = _feat['condition'].to('cuda')\n",
    "        # edge_attr = _feat['edge_attr'].to('cuda')\n",
    "        # edge_index = _feat['edge_index'].to('cuda')\n",
    "        # pred = self.model(x=x, edge_attr=edge_attr, edge_index=edge_index)\n",
    "        # output = [pred.detach()]\n",
    "\n",
    "        pred, kld, l, z = self.model(**{k:v.to('cuda') for k,v in _feat.items() if isinstance(v, torch.Tensor)})\n",
    "        mu, log_var = torch.chunk(l.detach().cpu(), 2, -1)\n",
    "        # output = [pred.detach(), kld.detach(), mu, log_var.exp(), z.detach()]\n",
    "        pred_num = pred[:, :5]\n",
    "        pred_vec = pred[:, 5:]\n",
    "        output = [pred_num.detach(), pred_vec.detach(), kld.detach(), mu, log_var.exp(), z.detach()]\n",
    "        if compute_loss:\n",
    "            label = _feat['label'].to('cuda')\n",
    "            num_label = _feat['label'].sum(1).long().to('cuda')\n",
    "            pred_vec * _feat['label_mask'].to('cuda') * num_label\n",
    "                \n",
    "            loss_num = self.crit(pred_num, num_label)\n",
    "            return loss_num.mean(), output\n",
    "            # alpha = (label[_feat['label_mask']] - 0.25).abs()\n",
    "            # ce_loss = (self.crit(pred, feat['label']) * (feat['label_mask'] + 1e-4)).mean()\n",
    "            # bce_loss = self.crit(pred, label)[_feat['label_mask']]\n",
    "            # focal_loss = alpha * (1 - torch.exp(-bce_loss)) ** 2 * bce_loss\n",
    "            # mse = torch.mean(torch.sum(torch.square(feat['x'] - pvec), -1))\n",
    "            # loss = focal_loss + beta * kld.sum()\n",
    "            # return loss.mean(), output\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "tr = TestTR(model, lr=1e-3)\n",
    "best_valid_loss = 1e5\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    # for j, batch in enumerate(train_dl):\n",
    "    #     _loss, _output = tr._eval_batch(batch, True, 1e-4)\n",
    "    #     if _loss.isnan().item(): break\n",
    "    #     tr.opt.zero_grad()\n",
    "    #     _loss.backward()\n",
    "    #     tr.opt.step()\n",
    "    # if _loss.isnan().item(): break\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, valid_output = tr.test(valid_dl)\n",
    "    test_loss, test_output = tr.test(test_dl)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_output = valid_output\n",
    "        best_test_output = test_output\n",
    "        best_valid_loss = valid_loss\n",
    "        count = 0\n",
    "    if count > 25:\n",
    "        break\n",
    "    print('{:4d} / {:8.4f} {:8.4f} {:8.4f} / {:10.4f} {:10.4f}'.format(\n",
    "        i, train_loss, valid_loss, test_loss, np.vstack(valid_output['kld']).mean(), np.vstack(test_output['kld']).mean()))\n",
    "    # print('{:4d} / {:12.8f} {:12.8f} {:12.8f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import VAETrainer\n",
    "\n",
    "class VAEMaskedClassTrainer(VAETrainer):\n",
    "    def __init__(self, model, lr, device='cuda', crit=torch.nn.BCEWithLogitsLoss(reduction='none')):\n",
    "        super().__init__(model, lr, device)\n",
    "    \n",
    "    def _eval_batch(self, batch, compute_loss=True, beta=0.1):\n",
    "        feat, _ = batch\n",
    "        pred, kld, l, z = self.model(**feat)\n",
    "        mu, log_var = torch.chunk(l.detach().cpu(), 2, -1)\n",
    "        output = [pred.detach().cpu().numpy(), kld.detach().cpu().numpy(), mu.numpy(), log_var.exp().numpy(), z.detach().cpu().numpy()]\n",
    "        if compute_loss:\n",
    "            bce_loss = self.crit(pred, feat['label'])[feat['label_mask']].mean()\n",
    "            loss = bce_loss + beta * kld.sum()\n",
    "            return loss, output\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "crit2 = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "feat, info = next(iter(valid_dl))\n",
    "\n",
    "y = feat['label'].float()\n",
    "x = torch.zeros_like(y)\n",
    "loss = crit1(x, y)[feat['label_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape, 8444 * 94, crit1(x, y).view(-1)[feat['label_mask'].view(-1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "years = np.array([d.year for d in DS])\n",
    "DS.to('cuda')\n",
    "train_idx = np.where(years < 2017)[0]\n",
    "valid_idx = np.where((years > 2016) & (years < 2019))[0]\n",
    "test_idx = np.where(years > 2018)[0]\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=256, sampler=SubsetRandomSampler(train_idx), collate_fn=DS.cfn)\n",
    "valid_dl = DataLoader(DS, batch_size=2048, sampler=valid_idx, collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=2048, sampler=valid_idx, collate_fn=DS.cfn)\n",
    "\n",
    "model = DNNCVAE(DS.num_ligand_feat, 16, DS.num_target_feat + DS.num_metal_feat,\n",
    "                encoder_hidden_dim=128, encoder_hidden_layers=4,\n",
    "                decoder_hidden_dim=128, decoder_hidden_layers=4)\n",
    "\n",
    "tr = VAETrainer(model, 1e-5)\n",
    "\n",
    "path = '/home/jhyang/WORKSPACES/MODELS/isyn/VAE/dnn_16_cgcnn_test'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "writer = SummaryWriter(path)\n",
    "for i in range(50):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    writer.add_scalar('Loss/Train', train_loss, i+1)\n",
    "    valid_loss, valid_output = tr.test(valid_dl)\n",
    "    test_loss, test_output = tr.test(test_dl)\n",
    "\n",
    "    writer.add_scalar('Loss/Valid', valid_loss, i+1)\n",
    "    writer.add_scalar('Loss/Test', test_loss, i+1)\n",
    "    writer.add_scalar('KLD/Valid', valid_output['kld'], i+1)\n",
    "    writer.add_scalar('KLD/Test', test_output['kld'], i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature import feature_to_composit, ligand_label\n",
    "def feature_to_ligand_index(feat_vec, tol=0.5):\n",
    "    comps = feature_to_composit(feat_vec, tol)\n",
    "    out = []\n",
    "    for comp in comps:\n",
    "        eles = '-'.join(comp.keys())\n",
    "        i = ligand_label.get(eles)\n",
    "        out.append(-1 if i is None else i)\n",
    "    return out\n",
    "\n",
    "inp = feature_to_ligand_index(valid_output['input'])\n",
    "prd = feature_to_ligand_index(valid_output['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _feature_to_composit(feat_vec, tol=0.5, to_string=False, to_idx=False):\n",
    "    n_feat = feat_vec.shape[-1]\n",
    "    if n_feat not in [97, 87, 12]:\n",
    "        raise TypeError(f'feature type is not supported', composit_fnc)\n",
    "    if n_feat == 97: # active_composit\n",
    "        ref = ActiveElements\n",
    "    elif n_feat == 87: # metal_composit\n",
    "        ref = ['None'] + MetalElements\n",
    "    elif n_feat == 12: # ligand_composit\n",
    "        ref = ['Metal'] + LigandElements\n",
    "    mask = feat_vec > tol\n",
    "    out = [tuple([ref[i] for i in np.where(vec)[0]]) for vec in mask]\n",
    "    if to_string or to_idx:\n",
    "        out = ['-'.join(o) for o in out]\n",
    "    if to_idx:\n",
    "        out = [ligand_label[o] if o in ligand_label.keys() else -1 for o in out]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(inp, prd, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.'.join(ligand_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import ReactionDataset\n",
    "from src.feature import init_info\n",
    "\n",
    "init_info(0)\n",
    "rd = ReactionDataset(precursor_feat_type='active_composit')\n",
    "rd.from_file()\n",
    "len(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.to('cpu')\n",
    "feat, info = rd.cfn(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(feat['label'].numpy(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
