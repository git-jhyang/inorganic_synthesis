{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as tnn\n",
    "from torch_geometric import nn as gnn\n",
    "from typing import Union, List\n",
    "from collections import OrderedDict\n",
    "import torch, os, pickle\n",
    "\n",
    "class BaseNetwork(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "        self._model_attrs = [None]\n",
    "\n",
    "    def _save(self, path, file_name, attr=None, overwrite=True):\n",
    "        model_path = os.path.join(path, file_name)\n",
    "        if not overwrite and os.path.isfile(model_path):\n",
    "            raise FileExistsError(model_path)\n",
    "        \n",
    "        if attr is None:\n",
    "            state_dict = self.state_dict()\n",
    "            model_param = getattr(self, '_model_param')\n",
    "        elif isinstance(attr, str) and hasattr(self, attr):\n",
    "            state_dict = getattr(self, attr).state_dict()\n",
    "            model_param = getattr(getattr(self, attr), '_model_param')\n",
    "        else:\n",
    "            raise AttributeError(attr)\n",
    "        \n",
    "        state_dict = OrderedDict({k:v.cpu().numpy() for k, v in state_dict.items()})\n",
    "        with open(model_path,'wb') as f:\n",
    "            pickle.dump({'model_param':model_param, 'state_dict':state_dict}, f)\n",
    "        \n",
    "    def _load(self, path, file_name, attr=None, requires_grad=False):\n",
    "        model_path = os.path.join(path, file_name)\n",
    "        \n",
    "        with open(model_path,'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        model_param = obj['model_param']\n",
    "        model_state_dict = OrderedDict({k:torch.from_numpy(v) for k,v in obj['state_dict'].items()})\n",
    "        \n",
    "        if attr is None:\n",
    "            self.__init__(**model_param)\n",
    "            self.load_state_dict(model_state_dict)\n",
    "            self.requires_grad_(requires_grad=requires_grad)\n",
    "        elif isinstance(attr, str) and hasattr(self, attr):\n",
    "            cls = getattr(self, attr)\n",
    "            cls.__init__(**model_param)\n",
    "            cls.load_state_dict(model_state_dict)\n",
    "            cls.requires_grad_(requires_grad=requires_grad)\n",
    "            setattr(self, attr, cls)\n",
    "        return self\n",
    "\n",
    "    def save(self, path, pfx, sfx='model', overwrite=True):\n",
    "        for attr in self._model_attrs:\n",
    "            file_name = f'{pfx}.{sfx}' if attr is None else f'{pfx}_{attr}.{sfx}'\n",
    "            self._save(path, file_name, attr, overwrite)\n",
    "        \n",
    "    def load_module(self, path, pfx, attr, sfx='model', requires_grad=False):\n",
    "        if not hasattr(self, attr):\n",
    "            raise AttributeError(attr, self._model_attrs)\n",
    "        self = self._load(path, file_name=f'{pfx}_{attr}.{sfx}', attr=attr, requires_grad=requires_grad)\n",
    "        return self\n",
    "        \n",
    "    def load(self, path, pfx, sfx='model', requires_grad=False):\n",
    "        for attr in self._model_attrs:\n",
    "            file_name = f'{pfx}.{sfx}' if attr is None else f'{pfx}_{attr}.{sfx}'\n",
    "            self = self._load(path, file_name, attr, requires_grad)\n",
    "        return self\n",
    "\n",
    "class DNNBlock(BaseNetwork):\n",
    "    def __init__(self, \n",
    "                 input_dim:int, \n",
    "                 output_dim:int, \n",
    "                 hidden_dim:int = 32,\n",
    "                 hidden_layers:int = 2,\n",
    "                 batch_norm:bool = True, \n",
    "                 dropout:float = 0,\n",
    "                 activation:str = 'LeakyReLU',\n",
    "                 **kwargs): \n",
    "        super(DNNBlock, self).__init__()\n",
    "        self._model_param = {\n",
    "                 'input_dim':input_dim,\n",
    "                 'output_dim':output_dim,\n",
    "                 'hidden_dim':hidden_dim,\n",
    "                 'hidden_layers':hidden_layers,\n",
    "                 'batch_norm':batch_norm,\n",
    "                 'dropout':dropout,\n",
    "                 'activation':activation\n",
    "        }\n",
    "        \n",
    "        self.embed_layer = tnn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden_layer = tnn.ModuleList()\n",
    "        for _ in range(hidden_layers):\n",
    "            layer = [tnn.Linear(hidden_dim, hidden_dim)]\n",
    "            if batch_norm:\n",
    "                layer.append(tnn.BatchNorm1d(hidden_dim))\n",
    "            if dropout > 0:\n",
    "                layer.append(tnn.Dropout(dropout))\n",
    "            layer.append(eval(f'tnn.{activation}()'))\n",
    "            self.hidden_layer.append(tnn.Sequential(*layer))\n",
    "        \n",
    "        self.output_layer = tnn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.embed_layer(x)\n",
    "        for hidden_layer in self.hidden_layer:\n",
    "            h = hidden_layer(h)\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "class AE(BaseNetwork):\n",
    "    def __init__(self, \n",
    "                 input_dim:int, \n",
    "                 latent_dim:int, \n",
    "                 hidden_dim:int = 32,\n",
    "                 hidden_layers:int = 2,\n",
    "                 batch_norm:bool = True, \n",
    "                 dropout:float = 0,\n",
    "                 activation:str = 'LeakyReLU',\n",
    "                 **kwargs): \n",
    "        super(AE, self).__init__()\n",
    "        self._model_attrs = ['encoder','decoder']\n",
    "        self.encoder = DNNBlock(input_dim, latent_dim, hidden_dim, hidden_layers,\n",
    "                                batch_norm, dropout, activation, **kwargs)\n",
    "        self.decoder = DNNBlock(latent_dim, input_dim, hidden_dim, hidden_layers,\n",
    "                                batch_norm, dropout, activation, **kwargs)\n",
    "    def forward(self, x):\n",
    "        l = self.encoder(x)\n",
    "        y = self.decoder(l)\n",
    "        return l, y\n",
    "\n",
    "class CVAE(BaseNetwork):\n",
    "    def __init__(self, \n",
    "                 input_dim:int, \n",
    "                 latent_dim:int, \n",
    "                 condition_vector_dim:int, \n",
    "                 hidden_dim:int = 32,\n",
    "                 hidden_layers:int = 2,\n",
    "                 batch_norm:bool = True, \n",
    "                 dropout:float = 0,\n",
    "                 activation:str = 'LeakyReLU',\n",
    "                 **kwargs): \n",
    "        super(CVAE, self).__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = torch.rand((2,8,4))\n",
    "na = a / torch.sqrt(torch.sum(torch.square(a), dim=-1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9451],\n",
       "        [0.9343],\n",
       "        [0.9404],\n",
       "        [0.9912],\n",
       "        [0.7958],\n",
       "        [0.7436],\n",
       "        [0.8241],\n",
       "        [0.9621]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.sum(a * b, 1, keepdim=True) \n",
    "y = (\n",
    "    torch.sqrt(torch.sum(torch.square(a), dim=-1, keepdim=True)) * \\\n",
    "    torch.sqrt(torch.sum(torch.square(b), dim=-1, keepdim=True))\n",
    ")\n",
    "x/y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 4, 5, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = torch.chunk(p, 2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape\n",
    "torch.concat([a, b], -1) - p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
