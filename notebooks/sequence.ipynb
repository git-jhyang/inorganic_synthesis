{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, os\n",
    "sys.path.append('..')\n",
    "from src.networks import BaseNetwork\n",
    "from src.data import ReactionDataset\n",
    "from src.feature import PrecursorDataset\n",
    "from src.trainer import BaseTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset(feat_type='composit', shuffle_sequence=False, include_eos=1)\n",
    "test = False\n",
    "if test:\n",
    "    DS.from_file('../data/surxn.pkl.gz', \n",
    "    )\n",
    "else:\n",
    "    DS.from_file('../data/screened_unique_reaction.pkl.gz')\n",
    "#    DS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "#                 heat_temp_key=('heat_temp','median'))\n",
    "\n",
    "years = np.array([d.year for d in DS])\n",
    "train_mask = years < 2016\n",
    "valid_mask = (years >= 2016) & (years < 2018)\n",
    "test_mask = years >= 2018\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=256, sampler=SubsetRandomSampler(np.where(train_mask)[0]), collate_fn=DS.cfn)\n",
    "valid_dl = DataLoader(DS, batch_size=2048, sampler=np.where(valid_mask)[0], collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=2048, sampler=np.where(test_mask)[0], collate_fn=DS.cfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2048, 7, 88]) <class 'torch.Tensor'>\n",
      "label torch.Size([14336]) <class 'torch.Tensor'>\n",
      "neg_label torch.Size([14336]) <class 'torch.Tensor'>\n",
      "context torch.Size([2048, 88]) <class 'torch.Tensor'>\n",
      "weight torch.Size([14336]) <class 'torch.Tensor'>\n",
      "sequence_mask torch.Size([14336]) <class 'torch.Tensor'>\n",
      "precursor_mask torch.Size([2048, 1, 414]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "feat, info = next(iter(valid_dl))\n",
    "for k,v in feat.items():\n",
    "    print(k, v.shape, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tested classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성 후 삭제\n",
    "\n",
    "from src.trainer import BaseTrainer\n",
    "from src.networks import BaseNetwork, PositionalEncoding, TransformerDecoderBlock\n",
    "from src.trainer import SequenceTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   3.67506   3.23439   3.23429 / 3.91\n",
      "   1   3.23277   3.00086   3.00070 / 3.93\n",
      "   2   3.04168   2.84369   2.84370 / 3.84\n",
      "   3   2.88790   2.71018   2.71016 / 3.46\n",
      "   4   2.77160   2.60471   2.60460 / 3.83\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = TransformerDecoderBlock(\n",
    "    feature_dim = DS.num_precursor_feat,\n",
    "    context_dim = DS.num_condition_feat, \n",
    "    vocab_dim = DS.NUM_LABEL, \n",
    "    num_heads = 4, hidden_dim = 64, hidden_layers = 4)\n",
    "\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(5):\n",
    "    t1 = time.time()\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f} / {:.2f}'.format(i, train_loss, valid_loss, test_loss, time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   3.64891   3.24302   3.24302 / 3.75\n",
      "   1   3.23010   2.99318   2.99315 / 3.87\n",
      "   2   3.03491   2.84142   2.84157 / 3.43\n",
      "   3   2.88542   2.71026   2.71019 / 3.84\n",
      "   4   2.76733   2.59806   2.59782 / 3.95\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "model = TransformerDecoderBlock(\n",
    "    feature_dim = DS.num_precursor_feat,\n",
    "    context_dim = DS.num_condition_feat, \n",
    "    vocab_dim = DS.NUM_LABEL, \n",
    "    num_heads = 4, hidden_dim = 64, hidden_layers = 4)\n",
    "model.to('cuda')\n",
    "#compiled_model = torch.compile(model)\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(5):\n",
    "    t1 = time.time()\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f} / {:.2f}'.format(i, train_loss, valid_loss, test_loss, time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "z1 = model(target = feat['precursor_feat'], context = feat['context'])\n",
    "z2 = model(target = DS.get_embedding(feat['target']), context=feat['context'])\n",
    "(z1 - z2).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27967, (27967,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = out['pred'].argmax(-1)\n",
    "label = out['label']\n",
    "weight = out['weight']\n",
    "n_data, l_seq = pred.shape\n",
    "mask = np.hstack([np.ones((n_data, 1), dtype=bool), (label != DS.EOS_LABEL)[..., :-1]])\n",
    "acc_rxn = []\n",
    "for p, l, m, w in zip(pred, label, mask, weight):\n",
    "    hit = (p[m] != l[m]).sum() == 0\n",
    "    acc_rxn.append(hit)\n",
    "len(acc_rxn), weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True, False, False, False, False]),\n",
       " array([  2,   5, 378, 378, 378, 378, 378]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0], label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.99951   1.44498   1.44493\n",
      "   1   1.32844   1.18909   1.18908\n",
      "   2   1.15493   1.06664   1.06657\n",
      "   3   1.05857   0.98836   0.98805\n",
      "   4   0.99526   0.93284   0.93303\n",
      "   5   0.94666   0.88761   0.88701\n",
      "   6   0.90799   0.85305   0.85327\n",
      "   7   0.87553   0.82254   0.82096\n",
      "   8   0.85084   0.79534   0.79469\n",
      "   9   0.82855   0.77896   0.77786\n",
      "  10   0.80872   0.75424   0.75411\n",
      "  11   0.79169   0.73576   0.73554\n",
      "  12   0.77698   0.72410   0.72531\n",
      "  13   0.76392   0.70815   0.70855\n",
      "  14   0.75060   0.69569   0.69644\n",
      "  15   0.73886   0.68417   0.68380\n",
      "  16   0.73034   0.67255   0.67398\n",
      "  17   0.72001   0.66320   0.66334\n",
      "  18   0.71236   0.65547   0.65561\n",
      "  19   0.70242   0.64979   0.64865\n",
      "  20   0.69550   0.64075   0.64030\n",
      "  21   0.69033   0.63113   0.63170\n",
      "  22   0.68170   0.62679   0.62562\n",
      "  23   0.67514   0.61784   0.61814\n",
      "  24   0.66914   0.61233   0.61180\n",
      "  25   0.66434   0.60926   0.60969\n",
      "  26   0.65751   0.60311   0.60178\n",
      "  27   0.65465   0.59412   0.59472\n",
      "  28   0.64847   0.59000   0.59011\n",
      "  29   0.64517   0.58822   0.58704\n",
      "  30   0.63950   0.58575   0.58497\n",
      "  31   0.63599   0.57945   0.57940\n",
      "  32   0.63208   0.57600   0.57547\n",
      "  33   0.62906   0.57450   0.57423\n",
      "  34   0.62433   0.56648   0.56641\n",
      "  35   0.62137   0.56111   0.56050\n",
      "  36   0.61803   0.55912   0.55978\n",
      "  37   0.61653   0.55797   0.55798\n",
      "  38   0.61084   0.55661   0.55716\n",
      "  39   0.60936   0.55276   0.55382\n",
      "  40   0.60655   0.55022   0.55036\n",
      "  41   0.60315   0.54482   0.54513\n",
      "  42   0.60058   0.54696   0.54719\n",
      "  43   0.59875   0.54501   0.54520\n",
      "  44   0.59448   0.53836   0.53844\n",
      "  45   0.59076   0.53233   0.53326\n",
      "  46   0.58934   0.52979   0.52889\n",
      "  47   0.58716   0.53203   0.53233\n",
      "  48   0.58503   0.52671   0.52761\n",
      "  49   0.58242   0.52384   0.52404\n",
      "  50   0.58009   0.52084   0.52117\n",
      "  51   0.57880   0.52606   0.52457\n",
      "  52   0.57587   0.51772   0.51758\n",
      "  53   0.57417   0.51880   0.51896\n",
      "  54   0.57150   0.51318   0.51399\n",
      "  55   0.57121   0.51275   0.51327\n",
      "  56   0.57068   0.50851   0.50873\n",
      "  57   0.56630   0.50676   0.50752\n",
      "  58   0.56614   0.50819   0.50707\n",
      "  59   0.56388   0.50407   0.50411\n",
      "  60   0.56163   0.50279   0.50174\n",
      "  61   0.56025   0.50586   0.50551\n",
      "  62   0.55872   0.50086   0.50016\n",
      "  63   0.55761   0.49874   0.49882\n",
      "  64   0.55613   0.50020   0.50024\n",
      "  65   0.55494   0.49490   0.49479\n",
      "  66   0.55083   0.49229   0.49129\n",
      "  67   0.55134   0.49408   0.49454\n",
      "  68   0.55016   0.49049   0.48975\n",
      "  69   0.54970   0.48922   0.48841\n",
      "  70   0.54782   0.48929   0.48865\n",
      "  71   0.54489   0.48741   0.48870\n",
      "  72   0.54564   0.48565   0.48569\n",
      "  73   0.54372   0.48766   0.48704\n",
      "  74   0.54349   0.48518   0.48502\n",
      "  75   0.53987   0.48115   0.48079\n",
      "  76   0.53964   0.48018   0.47986\n",
      "  77   0.53940   0.48518   0.48496\n",
      "  78   0.53763   0.47931   0.47872\n",
      "  79   0.53708   0.47658   0.47676\n",
      "  80   0.53553   0.47503   0.47396\n",
      "  81   0.53260   0.47430   0.47522\n",
      "  82   0.53346   0.47301   0.47389\n",
      "  83   0.53290   0.47494   0.47383\n",
      "  84   0.53128   0.47643   0.47676\n",
      "  85   0.53098   0.46930   0.47010\n",
      "  86   0.52902   0.46807   0.46844\n",
      "  87   0.52821   0.46770   0.46837\n",
      "  88   0.52718   0.46771   0.46738\n",
      "  89   0.52610   0.46675   0.46745\n",
      "  90   0.52580   0.46681   0.46685\n",
      "  91   0.52480   0.46604   0.46637\n",
      "  92   0.52458   0.46370   0.46419\n",
      "  93   0.52457   0.46431   0.46415\n",
      "  94   0.51971   0.46270   0.46372\n",
      "  95   0.52000   0.46052   0.45975\n",
      "  96   0.51942   0.45893   0.45915\n",
      "  97   0.51971   0.45970   0.45935\n",
      "  98   0.51883   0.45833   0.45912\n",
      "  99   0.51738   0.45895   0.45705\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderBlock(DS.num_condition_feat, num_heads=4, hidden_dim=64, hidden_layers=4)\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(100):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, 5, 8, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5],\n",
       "       [5, 5, 5, 5, 8, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5],\n",
       "       [5, 5, 5, 8, 5, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(self, context, max_len=20):\n",
    "        output_seq = torch.ones(context.shape[0], 1).long().to(self.device) * 443\n",
    "        for _ in range(max_len - 1):\n",
    "            output = self.forward(output_seq, context)\n",
    "            output_seq = torch.hstack([output_seq, output.argmax(-1)[:, -1:]])\n",
    "        seq = output_seq.cpu().numpy()[:, 1:]\n",
    "        j = (seq != EOS_LABEL).sum(1).max()\n",
    "        return seq[:, :j]\n",
    "\n",
    "generate(model, feat['context'][:3].to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5] [  5 148]\n",
      "[2 5] [  5 148]\n",
      "[2 5] [  2 107]\n",
      "[2 5] [  5 108]\n",
      "[5 8] [  8 124]\n",
      "[  5 106] [  5 189]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 182]\n",
      "[ 0 16 29] [  0  16 182]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  88 303]\n",
      "[ 7 18 29] [  7  88 113]\n",
      "[ 7 18 29] [  7  88 113]\n",
      "[ 7 18 29] [  7  88 113]\n"
     ]
    }
   ],
   "source": [
    "for _prd, _lbl in zip(out_gen, feat['label'].cpu().numpy()):\n",
    "    prd = np.array(sorted(_prd[_prd != EOS_LABEL]))\n",
    "    lbl = np.array(sorted(_lbl[_lbl != EOS_LABEL]))\n",
    "    if len(prd) != len(lbl):\n",
    "        print(prd, lbl)\n",
    "    elif np.sum(prd != lbl) != 0:\n",
    "        print(prd, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,   2, 444, 444],\n",
       "       [  5,   2, 444, 444],\n",
       "       [  5,   2, 444, 444],\n",
       "       ...,\n",
       "       [ 12,   9, 444, 444],\n",
       "       [ 12,   9, 444, 444],\n",
       "       [ 12,   9, 444, 444]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,  13,  14,   4, 444]) tensor([  0,  13,  14,   4, 444, 444, 444], device='cuda:0')\n",
      "tensor([  5,  65, 444]) tensor([  5,  65, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,  76, 444]) tensor([  4,  76, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 27,  49, 444]) tensor([ 27,  49, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6, 197,  18, 444]) tensor([  6, 197,  18, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 50,   2,   5,   0, 444]) tensor([ 50,   2,   5,   0, 444, 444, 444], device='cuda:0')\n",
      "tensor([  1,  32,  25, 444]) tensor([  1,  32,  25, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,   0,   1, 444]) tensor([  3,   0,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   7,  13, 444]) tensor([  2,   7,  13, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 79,  27,  49, 444]) tensor([ 79,  27,  49, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   1,  30, 444]) tensor([  6,   1,  30, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,   8, 444]) tensor([ 10,   8, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 14, 108,   9, 444]) tensor([ 14, 108,   9, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  14,   4, 444]) tensor([  0,  14,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 13,  20,  25, 444]) tensor([ 13,  20,  25, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 16,  11,   1,  30, 444]) tensor([ 16,  11,   1,  30, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,  10, 444]) tensor([  4,  10, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 61,  45, 444]) tensor([ 61,  45, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,  42, 444]) tensor([  6,  42, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 75,  18,   0,  11, 444]) tensor([ 75,  18,   0,  11, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 18,  10,   4, 444]) tensor([ 18,  10,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,  15,   4, 444]) tensor([  3,  15,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 11,  20, 444]) tensor([ 11,  20, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  50, 444]) tensor([  8,  50, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  1,  50,   5, 444]) tensor([  1,  50,   5, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 30,   0,   3,   7,   2, 444]) tensor([ 30,   0,   3,   7,   2, 444, 444], device='cuda:0')\n",
      "tensor([  7,  29,  18, 444]) tensor([  7,  29,  18, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  11,   7, 444]) tensor([  0,  11,   7, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,   0,  14, 444]) tensor([  4,   0,  14, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 80,   5, 444]) tensor([ 80,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   5, 444]) tensor([  6,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 38,   4,  13,   3, 444]) tensor([ 38,   4,  13,   3, 444, 444, 444], device='cuda:0')\n",
      "tensor([  7,   4,  22,  16, 444]) tensor([  7,   4,  22,  16, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   7,   6,  22,   1, 444]) tensor([  2,   7,   6,  22,   1, 444, 444], device='cuda:0')\n",
      "tensor([ 53,  58,   0, 444]) tensor([ 53,  58,   0, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,   2, 444]) tensor([  0,   2, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 33,   4, 444]) tensor([ 33,   4, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 11,  12, 444]) tensor([ 11,  12, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,  12,  16,  68, 444]) tensor([  3,  12,  16,  68, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  84,   5, 444]) tensor([  8,  84,   5, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 35,  33,  29, 444]) tensor([ 35,  33,  29, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 35,  68, 444]) tensor([ 35,  68, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   2, 444]) tensor([  6,   2, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   5, 444]) tensor([  2,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,   6, 444]) tensor([ 10,   6, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  35, 444]) tensor([  8,  35, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 23,  27,  52, 444]) tensor([ 23,  27,  52, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 32,  20,  25,   2, 444]) tensor([ 32,  20,  25,   2, 444, 444, 444], device='cuda:0')\n",
      "tensor([135,  79,  40, 444]) tensor([135,  79,  40, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([168,   7, 444]) tensor([168,   7, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,   2,  32, 444]) tensor([  0,   2,  32, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 44,  20,   1, 444]) tensor([ 44,  20,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([370,  50,   3,   5, 444]) tensor([370,  50,   3,   5, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  46,  16, 444]) tensor([  0,  46,  16, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 17,   0, 444]) tensor([ 17,   0, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  24, 444]) tensor([  8,  24, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  5,  13,  46, 444]) tensor([  5,  13,  46, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,  18,   1, 444]) tensor([ 10,  18,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 14,   1,   3,   5, 444]) tensor([ 14,   1,   3,   5, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 29,   6,  42, 444]) tensor([ 29,   6,  42, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 70,  27, 444]) tensor([ 70,  27, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 15,  11,   9, 444]) tensor([ 15,  11,   9, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([149,  98, 444]) tensor([149,  98, 444, 444, 444, 444, 444], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for l, m in zip(feat['label'], feat['target'] != 444):\n",
    "    print(l[m].cpu(), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성 후 삭제\n",
    "from src.networks import LSTMDecoderBlock, TransformerDecoderBlock\n",
    "from src.trainer import BaseTrainer\n",
    "\n",
    "class SequenceTrainer(BaseTrainer):\n",
    "    def __init__(self, model, lr, device='cuda', crit=torch.nn.CrossEntropyLoss(reduction='none')):\n",
    "        super().__init__(model, lr, device, crit)\n",
    "    \n",
    "    def _eval_batch(self, batch, compute_loss=True):\n",
    "        _feat, _ = batch\n",
    "        feat = {k:v.to(self.device) for k,v in _feat.items()}\n",
    "        pred = self.model(**feat)\n",
    "        B, S, L = pred.shape\n",
    "        if compute_loss:\n",
    "            pos = self.crit((pred * feat['precursor_mask']).view(B * S, -1), feat['label']) * feat['weight']\n",
    "            neg = torch.exp(-self.crit((pred * feat['precursor_mask']).view(B * S, -1), feat['neg_label'])) * feat['weight'] \n",
    "#            _loss = self.crit(pred.view(feat['label'].shape[0], -1), feat['label'])\n",
    "#            print(_loss.shape, feat['weight'].shape, feat['mask'].shape)\n",
    "            #loss = pos[feat['sequence_mask']].mean() #+ neg[feat['sequence_mask']].mean()\n",
    "            loss = (self.crit(pred.view(B * S, -1), feat['label']) * feat['weight'])[feat['sequence_mask']].mean()\n",
    "            return loss, [pos[feat['sequence_mask']].mean().item(), \n",
    "                          neg[feat['sequence_mask']].mean().item(), \n",
    "                          (pred * feat['precursor_mask']).detach().cpu().numpy()]\n",
    "        else:\n",
    "            return [pos[feat['sequence_mask']].mean().item(), \n",
    "                    neg[feat['sequence_mask']].mean().item(), \n",
    "                    (pred * feat['precursor_mask']).detach().cpu().numpy()]\n",
    "    \n",
    "    def _parse_output(self, batch, output):\n",
    "        feat, info = batch\n",
    "        pos, neg, pred = output\n",
    "        if self._output is None:\n",
    "            self._output = {\n",
    "                'info' : info,\n",
    "                'pos' : [pos],\n",
    "                'neg' : [neg],\n",
    "                'pred' : pred\n",
    "            }\n",
    "            if feat['weight'] is not None:\n",
    "                n = feat['context'].shape[0]\n",
    "                self._output.update({\n",
    "                    'label': feat['label'].cpu().numpy().reshape(n, -1),\n",
    "                    'weight': feat['weight'].cpu().numpy().reshape(n, -1)[:, 0],\n",
    "                })\n",
    "        else:\n",
    "            self._output['info'].extend(info)\n",
    "            self._output['pred'] = np.vstack([self._output['pred'], pred])\n",
    "            self._output['pos'].append(pos)\n",
    "            self._output['neg'].append(neg)\n",
    "            if feat['weight'] is not None:\n",
    "                n = feat['context'].shape[0]\n",
    "                self._output['label'] = np.vstack([self._output['label'], feat['label'].cpu().numpy().reshape(n, -1)])\n",
    "                self._output['weight'] = np.hstack([self._output['weight'], feat['weight'].cpu().numpy().reshape(n, -1)[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    4.095 |    3.813    3.957    0.011 |    3.839    3.990    0.011\n",
      "  10    2.070 |    2.250    1.706    0.079 |    2.264    1.744    0.080\n",
      "  20    1.739 |    1.912    1.436    0.058 |    1.968    1.518    0.055\n",
      "  30    1.571 |    1.726    1.304    0.051 |    1.796    1.398    0.048\n",
      "  40    1.475 |    1.632    1.242    0.047 |    1.705    1.333    0.044\n",
      "  50    1.409 |    1.560    1.203    0.040 |    1.637    1.299    0.036\n",
      "  60    1.364 |    1.510    1.155    0.041 |    1.582    1.245    0.038\n",
      "  70    1.332 |    1.487    1.140    0.042 |    1.566    1.235    0.039\n",
      "  80    1.301 |    1.449    1.127    0.035 |    1.530    1.228    0.031\n",
      "  90    1.280 |    1.434    1.108    0.034 |    1.512    1.201    0.030\n",
      " 100    1.267 |    1.418    1.125    0.031 |    1.506    1.230    0.027\n",
      " 110    1.255 |    1.409    1.107    0.033 |    1.485    1.198    0.030\n",
      " 120    1.241 |    1.398    1.113    0.027 |    1.488    1.218    0.025\n",
      " 130    1.234 |    1.385    1.116    0.026 |    1.480    1.230    0.023\n",
      " 140    1.220 |    1.374    1.105    0.026 |    1.467    1.214    0.023\n",
      " 150    1.213 |    1.371    1.098    0.026 |    1.463    1.203    0.024\n",
      " 160    1.208 |    1.371    1.085    0.034 |    1.455    1.184    0.030\n",
      " 170    1.200 |    1.364    1.111    0.025 |    1.456    1.220    0.022\n",
      " 180    1.188 |    1.359    1.094    0.025 |    1.460    1.208    0.022\n",
      " 190    1.185 |    1.349    1.107    0.023 |    1.450    1.220    0.020\n",
      " 200    1.178 |    1.346    1.086    0.027 |    1.434    1.189    0.024\n",
      " 210    1.178 |    1.338    1.077    0.027 |    1.425    1.180    0.024\n",
      " 220    1.172 |    1.334    1.079    0.027 |    1.425    1.183    0.024\n",
      " 230    1.170 |    1.330    1.073    0.024 |    1.427    1.188    0.021\n",
      " 240    1.163 |    1.328    1.073    0.024 |    1.422    1.182    0.021\n",
      " 250    1.162 |    1.322    1.079    0.024 |    1.419    1.187    0.021\n",
      " 260    1.156 |    1.319    1.080    0.023 |    1.419    1.195    0.020\n",
      " 270    1.149 |    1.314    1.067    0.026 |    1.410    1.172    0.023\n",
      " 280    1.146 |    1.310    1.064    0.023 |    1.413    1.179    0.020\n",
      " 290    1.147 |    1.308    1.065    0.023 |    1.418    1.186    0.021\n",
      " 300    1.145 |    1.310    1.076    0.024 |    1.408    1.182    0.021\n",
      " 310    1.144 |    1.308    1.077    0.022 |    1.409    1.191    0.019\n",
      " 320    1.138 |    1.317    1.103    0.018 |    1.425    1.225    0.016\n",
      " 330    1.133 |    1.295    1.071    0.022 |    1.400    1.184    0.019\n",
      " 340    1.135 |    1.292    1.047    0.025 |    1.392    1.155    0.022\n",
      " 350    1.136 |    1.294    1.049    0.024 |    1.393    1.159    0.021\n",
      " 360    1.134 |    1.294    1.044    0.023 |    1.400    1.161    0.021\n",
      " 370    1.132 |    1.297    1.056    0.025 |    1.394    1.163    0.022\n",
      " 380    1.133 |    1.292    1.081    0.021 |    1.399    1.199    0.018\n",
      " 390    1.122 |    1.285    1.042    0.023 |    1.388    1.154    0.021\n",
      " 400    1.128 |    1.284    1.062    0.021 |    1.391    1.176    0.019\n",
      " 410    1.128 |    1.276    1.047    0.020 |    1.385    1.167    0.018\n",
      " 420    1.123 |    1.293    1.073    0.018 |    1.400    1.192    0.016\n",
      " 430    1.119 |    1.272    1.054    0.019 |    1.391    1.185    0.016\n",
      " 440    1.118 |    1.278    1.060    0.020 |    1.386    1.175    0.018\n",
      " 450    1.120 |    1.292    1.088    0.017 |    1.400    1.208    0.015\n",
      " 460    1.114 |    1.284    1.059    0.019 |    1.387    1.171    0.017\n",
      " 470    1.117 |    1.274    1.053    0.019 |    1.390    1.178    0.017\n",
      " 480    1.111 |    1.278    1.069    0.017 |    1.390    1.189    0.015\n",
      " 490    1.113 |    1.267    1.031    0.024 |    1.378    1.150    0.022\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(valid_dl))\n",
    "#model = LSTMDecoderBlock(\n",
    "model = TransformerDecoderBlock(\n",
    "    feature_dim=DS.num_precursor_feat, \n",
    "    context_dim=DS.num_condition_feat,\n",
    "    output_dim=DS.NUM_LABEL)\n",
    "\n",
    "tr = SequenceTrainer(model, 1e-3, device='cuda')\n",
    "for i in range(500):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, valid_out = tr.test(valid_dl)\n",
    "    test_loss, test_out = tr.test(test_dl)\n",
    "    vp = valid_out['pos']\n",
    "    vn = valid_out['neg']\n",
    "    tp = test_out['pos']\n",
    "    tn = test_out['neg']\n",
    "    if i % 10 == 0:\n",
    "        print('{:4d} {:8.3f} | {:8.3f} {:8.3f} {:8.3f} | {:8.3f} {:8.3f} {:8.3f}'.format(\n",
    "            i, train_loss, valid_loss, np.mean(vp), np.mean(vn), test_loss, np.mean(tp), np.mean(tn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, info = next(iter(test_dl))\n",
    "#feat['x']\n",
    "context = feat['context']\n",
    "# case 1 - uncertainty based beam generation\n",
    "def beam_search(model, contexts, embed_fn, beam_width=50, max_seq_len=8):\n",
    "    n, c = contexts.shape\n",
    "    model.train()\n",
    "    sequences = - torch.ones(contexts.shape[0], beam_width, 1).long().cpu() + model.output_dim\n",
    "    scores = torch.zeros_like(sequences).float().to('cuda')\n",
    "\n",
    "    extended_contexts = contexts.unsqueeze(1).repeat(1, beam_width, 1).reshape(n * beam_width, c).to('cuda')\n",
    "    for _ in range(max_seq_len):\n",
    "        last_tokens = embed_fn(sequences[:, :, -1].reshape(-1, 1).cpu()).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            logits = model(last_tokens, extended_contexts)\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1).reshape(n, beam_width, -1)\n",
    "        total_scores = scores + log_probs\n",
    "        topk_scores, topk_indices = total_scores.reshape(n, -1).topk(beam_width, dim=-1)\n",
    "        beam_idx = torch.div(topk_indices, model.output_dim, rounding_mode='trunc')\n",
    "        token_idx = topk_indices % model.output_dim\n",
    "        return sequences, beam_idx, token_idx\n",
    "#        torch.cat([sequences], dim=-1)\n",
    "        scores = topk_scores.unsqueeze(-1)\n",
    "        return total_scores\n",
    "\n",
    "s, i, j = beam_search(model, feat['context'], DS.get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import ReactionDataset\n",
    "from src.networks import TransformerDecoderBlock\n",
    "import torch, gc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset(feat_type='cgcnn')\n",
    "DS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "             heat_temp_key=('heat_temp','median'))\n",
    "#DS.to('cuda')\n",
    "\n",
    "years = np.array([d.year for d in DS])\n",
    "train_mask = years < 2016\n",
    "valid_mask = (years >= 2016) & years < 2018\n",
    "test_mask = years >= 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.90%     322.000us        79.97%      28.589ms       9.530ms       0.000us         0.00%       1.259ms     419.667us      29.19 Kb     -61.88 Kb           0 b     -97.50 Kb             3  \n",
      "                                          backward_pass        28.79%      10.293ms        36.80%      13.157ms       4.386ms       0.000us         0.00%     124.000us      41.333us        -192 b        -192 b     -10.62 Mb      -9.93 Mb             3  \n",
      "                                        model_inference         6.24%       2.229ms        23.56%       8.424ms       2.808ms       0.000us         0.00%       1.132ms     377.333us         192 b      -2.46 Kb      10.62 Mb      -8.77 Mb             3  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        11.20%       4.005ms        17.81%       6.367ms       2.122ms       0.000us         0.00%       0.000us       0.000us      87.56 Kb    -582.65 Kb           0 b           0 b             3  \n",
      "                              Optimizer.step#AdamW.step         4.21%       1.504ms         7.47%       2.670ms     890.000us       0.000us         0.00%     121.000us      40.333us           0 b         -12 b           0 b    -705.00 Kb             3  \n",
      "                                            aten::index         4.44%       1.589ms         5.24%       1.874ms       4.880us       0.000us         0.00%       0.000us       0.000us     535.25 Kb     513.00 Kb           0 b           0 b           384  \n",
      "                                             aten::triu         4.71%       1.682ms         4.71%       1.682ms     280.333us       0.000us         0.00%       0.000us       0.000us       1.15 Kb       1.15 Kb           0 b           0 b             6  \n",
      "                                          aten::reshape         1.41%     503.000us         4.25%       1.518ms       2.517us       0.000us         0.00%     120.000us       0.199us      10.66 Kb      10.53 Kb       6.95 Mb    -280.00 Kb           603  \n",
      "                                            aten::clone         0.87%     310.000us         3.97%       1.421ms       8.458us       0.000us         0.00%     167.000us       0.994us      21.00 Kb           0 b      10.32 Mb      -2.65 Mb           168  \n",
      "                                           aten::linear         0.58%     207.000us         3.96%       1.417ms      29.521us       0.000us         0.00%     248.000us       5.167us           0 b           0 b       6.90 Mb     168.00 Kb            48  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.33%     118.000us         3.60%       1.288ms       9.135us       0.000us         0.00%     102.000us       0.723us           0 b           0 b    -280.00 Kb      -7.16 Mb           141  \n",
      "                                       cudaLaunchKernel         3.49%       1.249ms         3.49%       1.249ms       1.699us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      -4.07 Mb      -4.07 Mb           735  \n",
      "                                            aten::copy_         1.48%     529.000us         3.03%       1.084ms       4.818us     214.000us         5.25%     214.000us       0.951us      10.50 Kb     -10.50 Kb     -56.00 Kb     -56.00 Kb           225  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.50%     179.000us         2.90%       1.035ms      34.500us       0.000us         0.00%     544.000us      18.133us           0 b           0 b      -3.59 Mb      -5.70 Mb            30  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.03%       9.000us         2.76%     986.000us      82.167us       0.000us         0.00%       1.422ms     118.500us        -192 b        -192 b      -2.06 Mb      -4.20 Mb            12  \n",
      "                                          ViewBackward0         0.72%     258.000us         2.70%     966.000us       6.851us       0.000us         0.00%      78.000us       0.553us           0 b           0 b       5.91 Mb       1.04 Mb           141  \n",
      "            ScaledDotProductEfficientAttentionBackward0         0.29%     104.000us         2.54%     909.000us      75.750us       0.000us         0.00%       1.304ms     108.667us           0 b           0 b       1.97 Mb     168.00 Kb            12  \n",
      "                                               aten::mm         2.07%     739.000us         2.49%     890.000us       8.018us     755.000us        18.52%     755.000us       6.802us           0 b           0 b       5.24 Mb       5.24 Mb           111  \n",
      "aten::_scaled_dot_product_efficient_attention_backwa...         0.41%     147.000us         2.44%     873.000us      72.750us       0.000us         0.00%       1.422ms     118.500us           0 b           0 b       1.97 Mb     -56.00 Kb            12  \n",
      "                                            aten::empty         2.42%     864.000us         2.42%     864.000us       1.946us       0.000us         0.00%       0.000us       0.000us      16.54 Kb      16.54 Kb     118.00 Mb     118.00 Mb           444  \n",
      "                    aten::_efficient_attention_backward         0.57%     203.000us         1.98%     708.000us      59.000us       1.357ms        33.29%       1.422ms     118.500us           0 b           0 b       2.02 Mb     -97.67 Mb            12  \n",
      "                                       aten::empty_like         0.82%     292.000us         1.92%     685.000us       2.751us       0.000us         0.00%       0.000us       0.000us      17.50 Kb       3.50 Kb      14.08 Mb       3.45 Mb           249  \n",
      "autograd::engine::evaluate_function: SelectBackward0...        -0.08%     -28.000us         1.85%     663.000us      22.100us       0.000us         0.00%      78.000us       2.600us           0 b           0 b     168.00 Kb      -4.54 Mb            30  \n",
      "                                         AddmmBackward0         0.39%     140.000us         1.83%     655.000us      21.833us       0.000us         0.00%     401.000us      13.367us           0 b           0 b       2.14 Mb           0 b            30  \n",
      "                                           aten::matmul         0.18%      65.000us         1.66%     593.000us      32.944us       0.000us         0.00%      96.000us       5.333us           0 b           0 b       2.95 Mb           0 b            18  \n",
      "                                        SelectBackward0         0.39%     141.000us         1.39%     498.000us      16.600us       0.000us         0.00%      43.000us       1.433us           0 b           0 b       4.10 Mb     896.00 Kb            30  \n",
      "                                       aten::contiguous         0.30%     108.000us         1.37%     491.000us       9.093us       0.000us         0.00%      48.000us       0.889us      21.00 Kb       3.50 Kb       3.10 Mb     854.00 Kb            54  \n",
      "                                    aten::_foreach_sqrt         0.59%     212.000us         1.34%     479.000us     159.667us      15.000us         0.37%      15.000us       5.000us           0 b           0 b     705.00 Kb           0 b             3  \n",
      "                                  aten::select_backward         0.20%      71.000us         1.34%     478.000us      15.933us       0.000us         0.00%      59.000us       1.967us           0 b           0 b       4.16 Mb     -56.00 Kb            30  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.15%      53.000us         1.26%     449.000us      24.944us       0.000us         0.00%     282.000us      15.667us           0 b           0 b      -1.83 Mb      -2.95 Mb            18  \n",
      "                     aten::scaled_dot_product_attention         0.25%      88.000us         1.23%     439.000us      36.583us       0.000us         0.00%     592.000us      49.333us         192 b          16 b       1.41 Mb     120.00 Kb            12  \n",
      "                                    aten::empty_strided         1.23%     438.000us         1.23%     438.000us       2.179us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       2.88 Mb       2.88 Mb           201  \n",
      "                                            aten::addmm         1.05%     377.000us         1.19%     424.000us      14.133us     143.000us         3.51%     143.000us       4.767us           0 b           0 b       3.95 Mb       7.95 Mb            30  \n",
      "                                              aten::sum         0.91%     325.000us         1.17%     420.000us       6.667us     287.000us         7.04%     287.000us       4.556us           0 b           0 b     220.50 Kb     220.50 Kb            63  \n",
      "                                          aten::dropout         0.15%      52.000us         1.17%     419.000us      17.458us       0.000us         0.00%      33.000us       1.375us           0 b           0 b       2.05 Mb     210.00 Kb            24  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.24%      87.000us         1.17%     419.000us      23.278us       0.000us         0.00%     115.000us       6.389us           0 b           0 b      -1.04 Mb      -2.04 Mb            18  \n",
      "                                   aten::native_dropout         0.48%     171.000us         1.11%     396.000us      16.500us      36.000us         0.88%      36.000us       1.500us           0 b           0 b       2.05 Mb    -322.00 Kb            24  \n",
      "                                            MmBackward0         0.23%      83.000us         1.11%     396.000us      22.000us       0.000us         0.00%     282.000us      15.667us           0 b           0 b       1.12 Mb           0 b            18  \n",
      "                                       aten::layer_norm         0.18%      65.000us         1.08%     386.000us      21.444us       0.000us         0.00%      48.000us       2.667us           0 b           0 b       1.05 Mb     120.00 Kb            18  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.31%     111.000us         1.07%     381.000us      31.750us       0.000us         0.00%     645.000us      53.750us         192 b           0 b       1.41 Mb           0 b            12  \n",
      "                                aten::native_layer_norm         0.55%     198.000us         1.00%     359.000us      19.944us      54.000us         1.32%      54.000us       3.000us           0 b           0 b       1.05 Mb           0 b            18  \n",
      "                                        aten::transpose         0.93%     331.000us         0.94%     337.000us       0.614us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           549  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.45%     160.000us         0.93%     333.000us       2.581us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           129  \n",
      "                               NativeLayerNormBackward0         0.15%      52.000us         0.93%     332.000us      18.444us       0.000us         0.00%     115.000us       6.389us           0 b           0 b       1.00 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: TransposeBackwa...         0.44%     159.000us         0.92%     328.000us       4.205us       0.000us         0.00%      15.000us       0.192us           0 b           0 b    -672.00 Kb    -840.00 Kb            78  \n",
      "                                             aten::add_         0.65%     232.000us         0.90%     320.000us       1.693us      70.000us         1.72%      70.000us       0.370us           0 b           0 b    -336.00 Kb    -336.00 Kb           189  \n",
      "                                               aten::to         0.04%      14.000us         0.87%     310.000us       0.698us       0.000us         0.00%       2.000us       0.005us       3.50 Kb           0 b      97.50 Kb           0 b           444  \n",
      "                                                aten::t         0.67%     240.000us         0.84%     302.000us       1.274us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           237  \n",
      "                                         aten::_to_copy         0.14%      51.000us         0.80%     287.000us      19.133us       0.000us         0.00%       1.000us       0.067us       3.50 Kb      -7.00 Kb      97.50 Kb           0 b            15  \n",
      "                       aten::native_layer_norm_backward         0.40%     142.000us         0.78%     280.000us      15.556us     115.000us         2.82%     115.000us       6.389us           0 b           0 b       1.00 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: NativeDropoutBa...         0.07%      25.000us         0.77%     276.000us      11.500us       0.000us         0.00%      33.000us       1.375us           0 b           0 b     616.00 Kb      -1.37 Mb            24  \n",
      "                                            aten::slice         0.71%     253.000us         0.72%     257.000us       0.635us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           405  \n",
      "                                            aten::zeros         0.16%      56.000us         0.69%     248.000us       7.515us       0.000us         0.00%      30.000us       0.909us           0 b           0 b       4.43 Mb           0 b            33  \n",
      "                     aten::_efficient_attention_forward         0.36%     129.000us         0.64%     228.000us      19.000us     645.000us        15.82%     645.000us      53.750us         192 b          -8 b       1.41 Mb           0 b            12  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.31%     110.000us         0.62%     222.000us       5.692us       0.000us         0.00%     106.000us       2.718us           0 b           0 b       9.00 Kb           0 b            39  \n",
      "                                 NativeDropoutBackward0         0.15%      52.000us         0.61%     219.000us       9.125us       0.000us         0.00%      28.000us       1.167us           0 b           0 b       1.64 Mb     336.00 Kb            24  \n",
      "                          aten::native_dropout_backward         0.24%      85.000us         0.56%     199.000us       8.292us      33.000us         0.81%      33.000us       1.375us           0 b           0 b       1.64 Mb    -392.00 Kb            24  \n",
      "                                    aten::_foreach_add_         0.48%     172.000us         0.54%     192.000us      32.000us      11.000us         0.27%      11.000us       1.833us           0 b           0 b           0 b           0 b             6  \n",
      "                                            aten::zero_         0.15%      55.000us         0.51%     182.000us       4.667us       0.000us         0.00%      39.000us       1.000us           0 b           0 b           0 b           0 b            39  \n",
      "                                    aten::_foreach_mul_         0.37%     134.000us         0.48%     172.000us      28.667us      24.000us         0.59%      24.000us       4.000us           0 b           0 b           0 b           0 b             6  \n",
      "                                              aten::cat         0.39%     138.000us         0.45%     161.000us       7.667us      15.000us         0.37%      15.000us       0.714us      81.75 Kb      81.75 Kb      75.00 Kb      75.00 Kb            21  \n",
      "                                           aten::repeat         0.19%      69.000us         0.44%     156.000us      26.000us       0.000us         0.00%       3.000us       0.500us       5.25 Kb       3.50 Kb     168.00 Kb           0 b             6  \n",
      "                                            aten::fill_         0.27%      95.000us         0.43%     154.000us       3.020us      42.000us         1.03%      42.000us       0.824us           0 b           0 b           0 b           0 b            51  \n",
      "                        torch::autograd::AccumulateGrad         0.34%     123.000us         0.42%     150.000us       1.163us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           129  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         0.41%     148.000us         0.41%     148.000us      49.333us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -705.00 Kb    -705.00 Kb             3  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.21%      75.000us         0.41%     145.000us       3.021us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            48  \n",
      "                                        cudaMemcpyAsync         0.40%     144.000us         0.40%     144.000us       3.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            45  \n",
      "                                        aten::embedding         0.09%      31.000us         0.40%     143.000us      47.667us       0.000us         0.00%       6.000us       2.000us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                              aten::add         0.30%     108.000us         0.40%     142.000us       6.762us      33.000us         0.81%      33.000us       1.571us           0 b           0 b       1.15 Mb       1.15 Mb            21  \n",
      "     autograd::engine::evaluate_function: MeanBackward0         0.06%      20.000us         0.39%     140.000us      46.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b       6.00 Kb           0 b             3  \n",
      "                                             aten::view         0.37%     132.000us         0.37%     132.000us       0.186us       0.000us         0.00%       0.000us       0.000us      11.84 Kb      11.84 Kb     -56.00 Kb     -56.00 Kb           708  \n",
      "autograd::engine::evaluate_function: SplitWithSizesB...         0.06%      22.000us         0.36%     129.000us      10.750us       0.000us         0.00%      15.000us       1.250us           0 b           0 b      -3.00 Kb     -69.50 Kb            12  \n",
      "                                       aten::batch_norm         0.02%       6.000us         0.35%     125.000us      41.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                          MeanBackward0         0.05%      18.000us         0.34%     120.000us      40.000us       0.000us         0.00%       3.000us       1.000us           0 b          -8 b       6.00 Kb           0 b             3  \n",
      "                           aten::_batch_norm_impl_index         0.03%      11.000us         0.33%     119.000us      39.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                              aten::mul         0.24%      86.000us         0.32%     116.000us       6.444us      18.000us         0.44%      18.000us       1.000us           0 b           0 b     684.00 Kb     684.00 Kb            18  \n",
      "                               aten::cross_entropy_loss         0.03%       9.000us         0.32%     115.000us      38.333us       0.000us         0.00%      12.000us       4.000us           0 b           0 b       2.29 Mb           0 b             3  \n",
      "                                     aten::index_select         0.13%      46.000us         0.29%     103.000us      34.333us       6.000us         0.15%       6.000us       2.000us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                aten::native_batch_norm         0.16%      56.000us         0.29%     102.000us      34.000us      18.000us         0.44%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "autograd::engine::evaluate_function: UnsafeViewBackw...         0.16%      58.000us         0.28%     101.000us       2.806us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            36  \n",
      "                                SplitWithSizesBackward0         0.05%      18.000us         0.28%     101.000us       8.417us       0.000us         0.00%      14.000us       1.167us           0 b           0 b      54.50 Kb      -8.50 Kb            12  \n",
      "                                           aten::concat         0.17%      62.000us         0.28%     100.000us      11.111us       0.000us         0.00%       0.000us       0.000us      81.75 Kb      46.50 Kb           0 b           0 b             9  \n",
      "autograd::engine::evaluate_function: NativeBatchNorm...         0.03%      11.000us         0.27%      97.000us      32.333us       0.000us         0.00%      13.000us       4.333us           0 b           0 b     -24.00 Kb     -51.00 Kb             3  \n",
      "                                           aten::select         0.26%      93.000us         0.27%      95.000us       1.583us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            60  \n",
      "                                aten::_foreach_addcmul_         0.24%      85.000us         0.26%      93.000us      31.000us      12.000us         0.29%      12.000us       4.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                              aten::div         0.15%      55.000us         0.24%      86.000us      28.667us       3.000us         0.07%       3.000us       1.000us           8 b           8 b       6.00 Kb       6.00 Kb             3  \n",
      "                               NativeBatchNormBackward0         0.02%       7.000us         0.24%      86.000us      28.667us       0.000us         0.00%      13.000us       4.333us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                     TransposeBackward0         0.10%      34.000us         0.24%      85.000us       1.090us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                                aten::_foreach_addcdiv_         0.21%      74.000us         0.23%      82.000us      27.333us      23.000us         0.56%      23.000us       7.667us           0 b           0 b           0 b           0 b             3  \n",
      "                       aten::native_batch_norm_backward         0.11%      39.000us         0.22%      79.000us      26.333us      13.000us         0.32%      13.000us       4.333us           0 b           0 b      27.00 Kb      -1.50 Kb             3  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.05%      17.000us         0.21%      76.000us      25.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b       2.27 Mb      -7.50 Kb             3  \n",
      "                                    aten::_foreach_div_         0.19%      67.000us         0.21%      74.000us      24.667us      21.000us         0.52%      21.000us       7.000us           0 b           0 b           0 b           0 b             3  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.04%      14.000us         0.19%      68.000us      11.333us       0.000us         0.00%       6.000us       1.000us           0 b           0 b    -560.00 Kb      -1.20 Mb             6  \n",
      "                                      aten::nll_loss_nd         0.01%       3.000us         0.18%      64.000us      21.333us       0.000us         0.00%       6.000us       2.000us           0 b           0 b       7.50 Kb           0 b             3  \n",
      "                                             aten::relu         0.06%      21.000us         0.18%      63.000us      10.500us       0.000us         0.00%       6.000us       1.000us           0 b           0 b     672.00 Kb           0 b             6  \n",
      "                                         aten::nll_loss         0.02%       7.000us         0.17%      61.000us      20.333us       0.000us         0.00%       6.000us       2.000us           0 b           0 b       7.50 Kb           0 b             3  \n",
      "                                 aten::split_with_sizes         0.16%      56.000us         0.17%      60.000us       5.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                       NllLossBackward0         0.02%       8.000us         0.17%      59.000us      19.667us       0.000us         0.00%       9.000us       3.000us           0 b           0 b       2.28 Mb           0 b             3  \n",
      "                                        aten::unsqueeze         0.15%      54.000us         0.16%      58.000us       1.933us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...        -0.03%     -11.000us         0.16%      58.000us      19.333us       0.000us         0.00%      53.000us      17.667us           0 b           0 b           0 b    -168.00 Kb             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 35.749ms\n",
      "Self CUDA time total: 4.076ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.init()\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=64, sampler=SubsetRandomSampler(np.where(train_mask)[0]), \n",
    "                      collate_fn=DS.cfn)#num_workers=1, prefetch_factor=4, collate_fn=DS.cfn)\n",
    "\n",
    "valid_dl = DataLoader(DS, batch_size=2048, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=2048, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "\n",
    "model = TransformerDecoderBlock(DS.num_condition_feat)\n",
    "model.to('cuda')\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "crit = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "with profile(\n",
    "    activities=[\n",
    "        ProfilerActivity.CPU,\n",
    "        ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('../dump/log_dir'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    "    ) as prof:\n",
    "\n",
    "\n",
    "    for _feat, _ in train_dl:\n",
    "        feat = {k:v.to('cuda') for k,v in _feat.items()}\n",
    "        n_batch, l_seq = feat['label'].shape\n",
    "        with record_function('model_inference'):\n",
    "            pred = model(**feat)\n",
    "#        with record_function('0_compute_loss'):\n",
    "            _loss = crit(pred.reshape(n_batch * l_seq, -1), feat['label'].view(-1))\n",
    "#            loss = (_loss * weight)[mask].mean()\n",
    "            loss = (_loss * feat['weight']).mean()\n",
    "        with record_function('backward_pass'):\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        prof.step()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=100))\n",
    "#print(prof.key_averages().table(row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.58%     434.000us        90.70%      68.182ms      22.727ms       0.000us         0.00%       3.002ms       1.001ms     116.75 Kb    -233.50 Kb    -725.00 Kb           0 b             3  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        30.82%      23.168ms        47.79%      35.928ms      11.976ms       0.000us         0.00%       0.000us       0.000us     350.25 Kb      -2.08 Mb           0 b           0 b             3  \n",
      "                                        model_inference         5.89%       4.426ms        21.41%      16.096ms       5.365ms       0.000us         0.00%       2.884ms     961.333us         192 b     -16.46 Kb      43.13 Mb     -34.24 Mb             3  \n",
      "                                          backward_pass        13.76%      10.342ms        20.92%      15.724ms       5.241ms       0.000us         0.00%     118.000us      39.333us        -192 b        -192 b     -43.84 Mb     -43.15 Mb             3  \n",
      "                              Optimizer.step#AdamW.step         3.44%       2.587ms         6.65%       5.001ms       1.667ms       0.000us         0.00%     115.000us      38.333us           0 b         -12 b           0 b    -705.00 Kb             3  \n",
      "                                             aten::triu         2.72%       2.045ms         2.72%       2.045ms     340.833us       0.000us         0.00%       0.000us       0.000us       1.15 Kb       1.15 Kb           0 b           0 b             6  \n",
      "                                    aten::_foreach_sqrt         0.45%     341.000us         1.09%     822.000us     274.000us      15.000us         0.14%      15.000us       5.000us           0 b           0 b     705.00 Kb           0 b             3  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         0.37%     281.000us         0.37%     281.000us      93.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -705.00 Kb    -705.00 Kb             3  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.09%      65.000us         1.26%     946.000us      78.833us       0.000us         0.00%       4.985ms     415.417us        -192 b        -192 b      -8.25 Mb     -15.47 Mb            12  \n",
      "                               aten::cross_entropy_loss         0.01%       7.000us         0.30%     227.000us      75.667us       0.000us         0.00%      30.000us      10.000us           0 b           0 b      10.56 Mb      -3.75 Mb             3  \n",
      "            ScaledDotProductEfficientAttentionBackward0         0.04%      32.000us         1.17%     881.000us      73.417us       0.000us         0.00%       4.985ms     415.417us           0 b           0 b       7.22 Mb    -672.00 Kb            12  \n",
      "                                           aten::matmul         0.18%     133.000us         1.75%       1.312ms      72.889us       0.000us         0.00%      96.000us       5.333us           0 b           0 b      11.81 Mb           0 b            18  \n",
      "                     aten::scaled_dot_product_attention         0.20%     148.000us         1.16%     869.000us      72.417us       0.000us         0.00%       1.925ms     160.417us         192 b          16 b       5.62 Mb     480.00 Kb            12  \n",
      "                                        aten::embedding         0.06%      44.000us         0.28%     213.000us      71.000us       0.000us         0.00%       6.000us       2.000us           0 b           0 b     672.00 Kb           0 b             3  \n",
      "aten::_scaled_dot_product_efficient_attention_backwa...         0.20%     149.000us         1.13%     849.000us      70.750us       0.000us         0.00%       4.985ms     415.417us           0 b           0 b       7.88 Mb    -448.00 Kb            12  \n",
      "                                    aten::_foreach_add_         0.41%     311.000us         0.55%     415.000us      69.167us      10.000us         0.09%      10.000us       1.667us           0 b           0 b           0 b           0 b             6  \n",
      "                                           aten::linear         0.37%     277.000us         4.14%       3.111ms      64.812us       0.000us         0.00%     411.000us       8.562us           0 b           0 b      28.30 Mb           0 b            48  \n",
      "                                    aten::_foreach_mul_         0.44%     329.000us         0.51%     383.000us      63.833us      24.000us         0.23%      24.000us       4.000us           0 b           0 b           0 b           0 b             6  \n",
      "                                       aten::batch_norm         0.01%       8.000us         0.25%     190.000us      63.333us       0.000us         0.00%      24.000us       8.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.27%     202.000us         1.00%     751.000us      62.583us       0.000us         0.00%       2.100ms     175.000us         192 b           0 b       5.62 Mb           0 b            12  \n",
      "                                aten::_foreach_addcdiv_         0.22%     167.000us         0.24%     184.000us      61.333us      21.000us         0.20%      21.000us       7.000us           0 b           0 b           0 b           0 b             3  \n",
      "     autograd::engine::evaluate_function: MeanBackward0         0.04%      27.000us         0.24%     183.000us      61.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                aten::_foreach_addcmul_         0.22%     165.000us         0.24%     183.000us      61.000us      12.000us         0.11%      12.000us       4.000us           0 b           0 b           0 b           0 b             3  \n",
      "                           aten::_batch_norm_impl_index         0.03%      20.000us         0.24%     182.000us      60.667us       0.000us         0.00%      24.000us       8.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "                    aten::_efficient_attention_backward         0.25%     185.000us         0.93%     696.000us      58.000us       4.910ms        46.48%       4.985ms     415.417us           0 b           0 b       8.31 Mb    -390.83 Mb            12  \n",
      "                                    aten::_foreach_div_         0.20%     147.000us         0.22%     164.000us      54.667us      18.000us         0.17%      18.000us       6.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                     aten::index_select         0.10%      73.000us         0.21%     160.000us      53.333us       6.000us         0.06%       6.000us       2.000us           0 b           0 b     672.00 Kb           0 b             3  \n",
      "                                          MeanBackward0         0.03%      23.000us         0.21%     156.000us      52.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                           aten::concat         0.16%     122.000us         0.61%     457.000us      50.778us       0.000us         0.00%       0.000us       0.000us     327.00 Kb           0 b           0 b           0 b             9  \n",
      "                                aten::native_batch_norm         0.10%      76.000us         0.20%     152.000us      50.667us      24.000us         0.23%      24.000us       8.000us           0 b           0 b      99.00 Kb     -32.00 Kb             3  \n",
      "                                      aten::nll_loss_nd         0.01%       9.000us         0.17%     130.000us      43.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      22.50 Kb           0 b             3  \n",
      "                                       aten::layer_norm         0.13%      94.000us         1.01%     762.000us      42.333us       0.000us         0.00%     120.000us       6.667us           0 b           0 b       4.18 Mb     238.00 Kb            18  \n",
      "                                         aten::nll_loss         0.01%       8.000us         0.16%     121.000us      40.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      22.50 Kb           0 b             3  \n",
      "                                aten::native_layer_norm         0.53%     396.000us         0.95%     717.000us      39.833us     127.000us         1.20%     127.000us       7.056us           0 b           0 b       4.18 Mb           0 b            18  \n",
      "                     aten::_efficient_attention_forward         0.32%     237.000us         0.61%     460.000us      38.333us       2.100ms        19.88%       2.100ms     175.000us         192 b           8 b       5.62 Mb           0 b            12  \n",
      "                                              aten::div         0.09%      70.000us         0.15%     115.000us      38.333us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      21.00 Kb      21.00 Kb             3  \n",
      "                                 aten::nll_loss_forward         0.07%      52.000us         0.15%     113.000us      37.667us       6.000us         0.06%       9.000us       3.000us           0 b           0 b      22.50 Kb      22.50 Kb             3  \n",
      "                                          aten::dropout         0.15%     111.000us         1.20%     902.000us      37.583us       0.000us         0.00%      42.000us       1.750us           0 b           0 b       8.20 Mb     840.00 Kb            24  \n",
      "                                         aten::_to_copy         0.05%      34.000us         0.60%     450.000us      37.500us       0.000us         0.00%      21.000us       1.750us           0 b     -28.00 Kb     381.00 Kb           0 b            12  \n",
      "                                   aten::native_dropout         0.51%     386.000us         1.12%     845.000us      35.208us      48.000us         0.45%      48.000us       2.000us           0 b           0 b       8.20 Mb    -672.00 Kb            24  \n",
      "                                           aten::repeat         0.12%      87.000us         0.28%     211.000us      35.167us       0.000us         0.00%       3.000us       0.500us      21.00 Kb           0 b     672.00 Kb           0 b             6  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.22%     167.000us         1.39%       1.043ms      34.767us       0.000us         0.00%       1.030ms      34.333us           0 b           0 b     -15.42 Mb     -22.56 Mb            30  \n",
      "autograd::engine::evaluate_function: NativeBatchNorm...         0.01%      10.000us         0.14%     102.000us      34.000us       0.000us         0.00%      18.000us       6.000us           0 b           0 b     -96.00 Kb    -195.00 Kb             3  \n",
      "                                        aten::ones_like         0.02%      13.000us         0.13%     100.000us      33.333us       0.000us         0.00%       3.000us       1.000us           0 b           0 b       1.50 Kb           0 b             3  \n",
      "                                            aten::addmm         1.01%     759.000us         1.23%     921.000us      30.700us     279.000us         2.64%     279.000us       9.300us           0 b           0 b      16.49 Mb      16.49 Mb            30  \n",
      "                               NativeBatchNormBackward0         0.01%      10.000us         0.12%      92.000us      30.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "                                             aten::mean         0.09%      67.000us         0.11%      84.000us      28.000us       9.000us         0.09%       9.000us       3.000us           0 b           0 b       1.50 Kb       1.50 Kb             3  \n",
      "                       aten::native_batch_norm_backward         0.06%      44.000us         0.11%      82.000us      27.333us      18.000us         0.17%      18.000us       6.000us           0 b           0 b      99.00 Kb      -3.00 Kb             3  \n",
      "                                      aten::log_softmax         0.03%      25.000us         0.11%      79.000us      26.333us       0.000us         0.00%      14.000us       4.667us           0 b           0 b      10.54 Mb       3.75 Mb             3  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.07%      50.000us         0.61%     457.000us      25.389us       0.000us         0.00%     396.000us      22.000us           0 b           0 b      -7.73 Mb     -11.81 Mb            18  \n",
      "                                             aten::relu         0.06%      48.000us         0.20%     147.000us      24.500us       0.000us         0.00%       6.000us       1.000us           0 b           0 b       2.62 Mb           0 b             6  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.01%       9.000us         0.10%      72.000us      24.000us       0.000us         0.00%      16.000us       5.333us           0 b           0 b       9.10 Mb     -22.50 Kb             3  \n",
      "                                   aten::_foreach_lerp_         0.07%      49.000us         0.09%      68.000us      22.667us      15.000us         0.14%      15.000us       5.000us           0 b           0 b           0 b           0 b             3  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.04%      27.000us         0.54%     403.000us      22.389us       0.000us         0.00%     252.000us      14.000us           0 b           0 b      -4.17 Mb      -8.56 Mb            18  \n",
      "                                         AddmmBackward0         0.16%     121.000us         0.89%     669.000us      22.300us       0.000us         0.00%     732.000us      24.400us           0 b           0 b       7.56 Mb           0 b            30  \n",
      "                                            MmBackward0         0.11%      82.000us         0.53%     401.000us      22.278us       0.000us         0.00%     391.000us      21.722us           0 b           0 b       3.86 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...        -0.04%     -29.000us         0.09%      66.000us      22.000us       0.000us         0.00%     197.000us      65.667us           0 b           0 b    -504.00 Kb    -784.00 Kb             3  \n",
      "                                     aten::_log_softmax         0.06%      46.000us         0.09%      65.000us      21.667us      21.000us         0.20%      21.000us       7.000us           0 b           0 b      10.54 Mb      10.54 Mb             3  \n",
      "autograd::engine::evaluate_function: SelectBackward0...        -0.03%     -20.000us         0.86%     644.000us      21.467us       0.000us         0.00%      84.000us       2.800us           0 b           0 b       2.41 Mb     -17.72 Mb            30  \n",
      "                                       NllLossBackward0         0.01%       7.000us         0.08%      63.000us      21.000us       0.000us         0.00%      16.000us       5.333us           0 b           0 b       9.13 Mb           0 b             3  \n",
      "                                              aten::cat         0.54%     403.000us         0.57%     430.000us      20.476us      18.000us         0.17%      18.000us       0.857us     327.00 Kb     327.00 Kb      74.50 Kb      74.50 Kb            21  \n",
      "                                     EmbeddingBackward0         0.05%      41.000us         0.08%      58.000us      19.333us       0.000us         0.00%      66.000us      22.000us           0 b           0 b     168.00 Kb     112.00 Kb             3  \n",
      "                                aten::nll_loss_backward         0.03%      24.000us         0.07%      56.000us      18.667us       6.000us         0.06%      16.000us       5.333us           0 b           0 b       9.13 Mb       9.13 Mb             3  \n",
      "                               NativeLayerNormBackward0         0.13%      95.000us         0.43%     326.000us      18.111us       0.000us         0.00%     210.000us      11.667us           0 b           0 b       3.74 Mb     451.00 Kb            18  \n",
      "                               aten::embedding_backward         0.01%       5.000us         0.07%      54.000us      18.000us       0.000us         0.00%     197.000us      65.667us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                       aten::leaky_relu         0.05%      37.000us         0.07%      51.000us      17.000us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      96.00 Kb      96.00 Kb             3  \n",
      "                                        aten::clamp_min         0.09%      69.000us         0.13%      99.000us      16.500us       6.000us         0.06%       6.000us       1.000us           0 b           0 b       2.62 Mb       2.62 Mb             6  \n",
      "                         aten::embedding_dense_backward         0.03%      22.000us         0.07%      49.000us      16.333us     194.000us         1.84%     197.000us      65.667us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                        SelectBackward0         0.16%     120.000us         0.64%     484.000us      16.133us       0.000us         0.00%      45.000us       1.500us           0 b           0 b      16.62 Mb       3.72 Mb            30  \n",
      "                                       aten::contiguous         0.20%     153.000us         1.15%     863.000us      15.981us       0.000us         0.00%      66.000us       1.222us      84.00 Kb           0 b      12.36 Mb       2.13 Mb            54  \n",
      "                                              aten::add         0.32%     244.000us         0.44%     328.000us      15.619us      34.000us         0.32%      34.000us       1.619us           0 b           0 b       4.38 Mb       4.38 Mb            21  \n",
      "                       aten::native_layer_norm_backward         0.19%     141.000us         0.37%     281.000us      15.611us     252.000us         2.39%     252.000us      14.000us           0 b           0 b       3.96 Mb           0 b            18  \n",
      "                                  aten::select_backward         0.09%      65.000us         0.62%     465.000us      15.500us       0.000us         0.00%      58.000us       1.933us           0 b           0 b      16.62 Mb    -448.00 Kb            30  \n",
      "autograd::engine::evaluate_function: RepeatBackward0...         0.01%       6.000us         0.06%      45.000us      15.000us       0.000us         0.00%       9.000us       3.000us           0 b           0 b    -576.00 Kb    -672.00 Kb             3  \n",
      "autograd::engine::evaluate_function: LogSoftmaxBackw...         0.01%       7.000us         0.06%      43.000us      14.333us       0.000us         0.00%      19.000us       6.333us           0 b           0 b     -10.54 Mb     -13.58 Mb             3  \n",
      "autograd::engine::evaluate_function: LeakyReluBackwa...         0.01%      11.000us         0.06%      43.000us      14.333us       0.000us         0.00%       3.000us       1.000us           0 b           0 b     -96.00 Kb    -160.00 Kb             3  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.01%      10.000us         0.06%      42.000us      14.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b           0 b     -21.00 Kb             3  \n",
      "                                              aten::all         0.05%      39.000us         0.05%      41.000us      13.667us       0.000us         0.00%       0.000us       0.000us           3 b           3 b           0 b           0 b             3  \n",
      "                                        RepeatBackward0         0.01%       8.000us         0.05%      39.000us      13.000us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      96.00 Kb           0 b             3  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.02%      12.000us         0.10%      74.000us      12.333us       0.000us         0.00%      12.000us       2.000us           0 b           0 b      -2.62 Mb      -3.94 Mb             6  \n",
      "                                            aten::clone         0.60%     452.000us         2.73%       2.053ms      12.220us       0.000us         0.00%     233.000us       1.387us      84.00 Kb     -42.00 Kb      41.18 Mb      -7.60 Mb           168  \n",
      "                                    LogSoftmaxBackward0         0.01%       9.000us         0.05%      36.000us      12.000us       0.000us         0.00%      19.000us       6.333us           0 b           0 b       3.04 Mb      -6.08 Mb             3  \n",
      "autograd::engine::evaluate_function: NativeDropoutBa...        -0.05%     -35.000us         0.36%     274.000us      11.417us       0.000us         0.00%      39.000us       1.625us           0 b           0 b       2.35 Mb      -6.29 Mb            24  \n",
      "autograd::engine::evaluate_function: SplitWithSizesB...         0.02%      16.000us         0.18%     133.000us      11.083us       0.000us         0.00%      18.000us       1.500us           0 b           0 b      -3.00 Kb     -78.00 Kb            12  \n",
      "                                               aten::mm         1.25%     937.000us         1.61%       1.208ms      10.883us       1.200ms        11.36%       1.200ms      10.811us           0 b           0 b      19.51 Mb      19.51 Mb           111  \n",
      "                                           MulBackward0         0.01%       7.000us         0.04%      32.000us      10.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                     LeakyReluBackward0         0.01%       6.000us         0.04%      32.000us      10.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      64.00 Kb     -32.00 Kb             3  \n",
      "                                          ReluBackward0         0.01%      10.000us         0.08%      62.000us      10.333us       0.000us         0.00%      12.000us       2.000us           0 b           0 b       1.31 Mb      -1.31 Mb             6  \n",
      "                                 NativeDropoutBackward0         0.13%     100.000us         0.30%     224.000us       9.333us       0.000us         0.00%      24.000us       1.000us           0 b           0 b       6.51 Mb       2.13 Mb            24  \n",
      "                       aten::_log_softmax_backward_data         0.03%      20.000us         0.04%      27.000us       9.000us      19.000us         0.18%      19.000us       6.333us           0 b           0 b       9.13 Mb       9.13 Mb             3  \n",
      "                                  cudaDeviceSynchronize         0.01%       9.000us         0.01%       9.000us       9.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                SplitWithSizesBackward0         0.03%      22.000us         0.14%     106.000us       8.833us       0.000us         0.00%      17.000us       1.417us           0 b           0 b      75.00 Kb         512 b            12  \n",
      "                          aten::native_dropout_backward         0.14%     102.000us         0.28%     209.000us       8.708us      39.000us         0.37%      39.000us       1.625us           0 b           0 b       6.51 Mb      -1.37 Mb            24  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.05%      41.000us         1.63%       1.222ms       8.667us       0.000us         0.00%     170.000us       1.206us           0 b           0 b    -896.00 Kb     -26.69 Mb           141  \n",
      "                               aten::threshold_backward         0.05%      39.000us         0.07%      52.000us       8.667us      12.000us         0.11%      12.000us       2.000us           0 b           0 b       2.62 Mb       2.62 Mb             6  \n",
      "                              aten::leaky_relu_backward         0.03%      20.000us         0.03%      26.000us       8.667us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      96.00 Kb      96.00 Kb             3  \n",
      "                                              aten::mul         0.14%     107.000us         0.20%     147.000us       8.167us      20.000us         0.19%      20.000us       1.111us           0 b           0 b       2.67 Mb       2.67 Mb            18  \n",
      "                                               aten::ne         0.03%      24.000us         0.03%      24.000us       8.000us       0.000us         0.00%       0.000us       0.000us       5.27 Kb       5.27 Kb           0 b           0 b             3  \n",
      "                                 aten::split_with_sizes         0.12%      89.000us         0.12%      92.000us       7.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                            aten::copy_         1.06%     796.000us         2.21%       1.663ms       7.491us     321.000us         3.04%     325.000us       1.464us      56.00 Kb     -28.00 Kb     -56.00 Kb     -56.00 Kb           222  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 75.177ms\n",
      "Self CUDA time total: 10.564ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dump/profiler_B64_NW1_PF0_indexed.txt','w') as f: \n",
    "    f.write(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isyn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
