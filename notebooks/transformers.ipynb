{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, os\n",
    "sys.path.append('..')\n",
    "from src.networks import TransformerDecoderBlock\n",
    "from src.data import ReactionDataset\n",
    "from src.feature import PrecursorDataset\n",
    "from src.trainer import BaseTrainer, SequenceTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset(feat_type='cgcnn')\n",
    "DS.from_file('../data/scrxn.pkl.gz', \n",
    "             heat_temp_key=('heat_temp','median'))\n",
    "\n",
    "years = np.array([d.year for d in DS])\n",
    "train_mask = years < 2016\n",
    "valid_mask = (years >= 2016) & years < 2018\n",
    "test_mask = years >= 2018\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=256, sampler=SubsetRandomSampler(np.where(train_mask)[0]), collate_fn=DS.cfn)\n",
    "valid_dl = DataLoader(DS, batch_size=32, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=32, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target torch.Size([32, 7, 91]) <class 'torch.Tensor'>\n",
      "precursor_feat torch.Size([32, 8]) <class 'torch.Tensor'>\n",
      "label torch.Size([20384]) <class 'torch.Tensor'>\n",
      "context torch.Size([32, 92]) <class 'torch.Tensor'>\n",
      "weight torch.Size([224]) <class 'torch.Tensor'>\n",
      "mask torch.Size([224]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "feat, info = next(iter(valid_dl))\n",
    "for k,v in feat.items():\n",
    "    print(k, v.shape, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tested classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import BaseTrainer\n",
    "from src.networks import BaseNetwork\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, num_dim, max_len=100):\n",
    "        pe = torch.zeros((max_len, num_dim)).float()\n",
    "        pos = torch.arange(0, max_len).reshape(-1,1).float()\n",
    "        div = torch.exp(- torch.arange(0, num_dim, 2) * torch.log(torch.tensor([10000])) / num_dim).float()\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.pe = torch.nn.Parameter(pe, requires_grad=False)\n",
    "\n",
    "    def __call__(self, seq_len):\n",
    "        return self.pe[:seq_len].unsqueeze(0)\n",
    "\n",
    "class TestTransformerDecoderBlock(BaseNetwork):\n",
    "    def __init__(self,\n",
    "                 feature_dim:int,\n",
    "                 context_dim:int,\n",
    "                 vocab_dim:int, \n",
    "                 num_heads:int = 4,\n",
    "                 hidden_dim:int = 32,\n",
    "                 ff_dim_mul:int = 2,\n",
    "                 hidden_layers:int = 2,\n",
    "                 positional_encoding:bool = True,\n",
    "                 batch_norm:bool = False,\n",
    "                 dropout = 0.2,\n",
    "                 activation:str = 'LeakyReLU',\n",
    "                 negative_slope:float = 0.1,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__(context_dim = context_dim,\n",
    "                         vocab_dim = vocab_dim,\n",
    "                         num_heads = num_heads,\n",
    "                         hidden_dim = hidden_dim,\n",
    "                         hidden_layers = hidden_layers,\n",
    "                         positional_encoding = positional_encoding,\n",
    "                         batch_norm = batch_norm,\n",
    "                         dropout = dropout,\n",
    "                         activation = activation,\n",
    "                         negative_slope = negative_slope,\n",
    "        )\n",
    "\n",
    "        self.vocab_dim = vocab_dim\n",
    "        try:\n",
    "            activation = eval(f'torch.nn.{activation}({negative_slope})')\n",
    "        except:\n",
    "            activation = eval(f'torch.nn.{activation}()')\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim) if positional_encoding else False\n",
    "\n",
    "        self.feature_embed_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "#            torch.nn.Dropout(dropout),\n",
    "            activation,            \n",
    "        )\n",
    "\n",
    "        self.context_embed_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(context_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "#            torch.nn.Dropout(dropout),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(\n",
    "            torch.nn.TransformerDecoderLayer(\n",
    "                d_model = hidden_dim,\n",
    "                nhead = num_heads,\n",
    "                dim_feedforward = hidden_dim * 2,\n",
    "                dropout = dropout,\n",
    "                activation = activation,\n",
    "                batch_first = True,\n",
    "            ), \n",
    "            num_layers = hidden_layers, \n",
    "            norm = torch.nn.BatchNorm1d(hidden_dim) if batch_norm else None,\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, vocab_dim)\n",
    "\n",
    "    def forward(self, target, context, *args, **kwargs):\n",
    "        n = target.shape[1]\n",
    "        target_mask = torch.nn.Transformer.generate_square_subsequent_mask(n)\n",
    "\n",
    "        if self.positional_encoding: \n",
    "            target += self.positional_encoding(n)\n",
    "        context_embed = self.context_embed_layer(context).unsqueeze(1)#.repeat(1, n, 1)\n",
    "\n",
    "        h = self.transformer_decoder(target, context_embed, tgt_mask=target_mask)\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, context, embed_fn, max_len=20, *args, **kwargs):\n",
    "        output_seq = torch.ones(context.shape[0], 1).long().to(self.device) + self.vocab_dim \n",
    "        for _ in range(max_len):\n",
    "            output = self.forward(output_seq, context)\n",
    "            output_seq = torch.hstack([output_seq, output.argmax(-1)[:, -1:]])\n",
    "        return output_seq.cpu().numpy()[:, 1:]\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        if self.positional_encoding:\n",
    "            self.positional_encoding.pe = self.positional_encoding.pe.to(*args, **kwargs)\n",
    "\n",
    "    def save(self, path, prefix, overwrite=True):\n",
    "        self._save(path, f'{prefix}.model', overwrite)\n",
    "\n",
    "class TestSequenceTrainer(BaseTrainer):\n",
    "    def __init__(self, model, lr, device='cuda', crit=torch.nn.CrossEntropyLoss(reduction='none')):\n",
    "        super().__init__(model, lr, device, crit)\n",
    "    \n",
    "    def _eval_batch(self, batch, compute_loss=True):\n",
    "        _feat, _ = batch\n",
    "        feat = {k:v.to(self.device) for k,v in _feat.items()}\n",
    "        pred = self.model(**feat)\n",
    "        if compute_loss:\n",
    "            _loss = self.crit(pred.view(feat['label'].shape[0], -1), feat['label'])[feat['mask']]\n",
    "#            print(_loss.shape, feat['weight'].shape, feat['mask'].shape)\n",
    "            loss = (_loss * feat['weight'][feat['mask']]).mean()\n",
    "            return loss, pred.detach().cpu().numpy()\n",
    "        else:\n",
    "            return pred.detach().cpu().numpy()\n",
    "    \n",
    "    def _parse_output(self, batch, output):\n",
    "        feat, info = batch\n",
    "        if self._output is None:\n",
    "            self._output = {\n",
    "                'info' : info,\n",
    "                'pred' : output\n",
    "            }\n",
    "            if feat['weight'] is not None:\n",
    "                n = feat['context'].shape[0]\n",
    "                self._output.update({\n",
    "                    'label': feat['label'].cpu().numpy().reshape(n, -1),\n",
    "                    'weight': feat['weight'].cpu().numpy().reshape(n, -1)[:, 0],\n",
    "                })\n",
    "        else:\n",
    "            self._output['info'].extend(info)\n",
    "            self._output['pred'] = np.vstack([self._output['pred'], output])\n",
    "            if feat['weight'] is not None:\n",
    "                n = feat['context'].shape[0]\n",
    "                self._output['label'] = np.vstack([self._output['label'], feat['label'].cpu().numpy().reshape(n, -1)])\n",
    "                self._output['weight'] = np.hstack([self._output['weight'], feat['weight'].cpu().numpy().reshape(n, -1)[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.5063e-01,  5.7888e-01,  5.0418e-01,  ..., -3.0448e-01,\n",
       "          -3.8087e-01, -7.0146e-02],\n",
       "         [ 2.0862e-01,  8.1354e-01,  5.6663e-01,  ...,  2.0481e-01,\n",
       "           2.8978e-01,  9.8414e-02],\n",
       "         [-2.5659e-01,  8.2889e-01,  1.0078e+00,  ..., -4.5890e-01,\n",
       "           9.8797e-01, -2.6617e-01],\n",
       "         ...,\n",
       "         [-1.0256e-01,  1.0854e+00,  7.8896e-01,  ..., -5.8150e-01,\n",
       "           7.1035e-01,  2.4221e-01],\n",
       "         [ 8.1696e-02,  8.7025e-01,  9.6989e-01,  ..., -8.9164e-01,\n",
       "           3.5854e-01,  5.2547e-01],\n",
       "         [-4.7880e-02,  1.0109e+00,  8.1174e-01,  ..., -3.8134e-01,\n",
       "           9.7241e-01,  4.4252e-01]],\n",
       "\n",
       "        [[-6.0453e-01,  3.7663e-01,  4.2093e-01,  ..., -3.4342e-01,\n",
       "          -4.8789e-01, -3.8142e-01],\n",
       "         [-5.7945e-01,  4.0511e-01,  5.2471e-01,  ...,  6.0483e-01,\n",
       "           8.2484e-01,  2.2529e-01],\n",
       "         [-6.9531e-01,  1.0187e-01,  1.3416e+00,  ..., -1.5803e-01,\n",
       "           1.0385e+00, -3.0737e-01],\n",
       "         ...,\n",
       "         [-4.5443e-02,  8.2883e-01,  8.5366e-01,  ..., -3.6183e-01,\n",
       "           1.0879e+00,  2.0756e-01],\n",
       "         [ 4.5660e-02,  1.0660e+00,  7.4485e-01,  ..., -3.1218e-01,\n",
       "           7.1630e-01,  2.7931e-01],\n",
       "         [-2.0388e-01,  1.4728e+00,  4.8429e-01,  ..., -5.7454e-02,\n",
       "           1.2911e+00,  2.5474e-01]],\n",
       "\n",
       "        [[-7.8521e-02,  4.1652e-01,  6.1406e-01,  ..., -3.9077e-01,\n",
       "          -7.3893e-01, -3.7068e-01],\n",
       "         [-6.1304e-01,  2.5837e-01,  5.3384e-01,  ..., -4.3305e-01,\n",
       "          -9.6832e-01, -4.5525e-01],\n",
       "         [-3.0341e-01, -2.6652e-02,  1.1801e+00,  ..., -3.1938e-01,\n",
       "          -7.5187e-01, -1.7494e-01],\n",
       "         ...,\n",
       "         [-2.6470e-03,  1.3195e+00,  4.8633e-01,  ..., -8.2486e-01,\n",
       "          -3.1494e-01,  2.6735e-01],\n",
       "         [ 3.0977e-02,  9.1938e-01,  5.5523e-01,  ..., -5.7362e-01,\n",
       "          -4.9403e-01,  3.2120e-01],\n",
       "         [ 5.3105e-01,  6.5497e-01,  3.9414e-01,  ..., -2.0300e-01,\n",
       "          -2.6921e-01,  3.5656e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.8423e-01,  1.6127e-01,  3.9419e-01,  ..., -4.4312e-01,\n",
       "          -1.5548e+00, -1.0038e-01],\n",
       "         [-4.1078e-02,  8.4117e-02,  4.3205e-01,  ..., -1.0651e+00,\n",
       "          -1.2426e+00,  3.2261e-01],\n",
       "         [-3.8831e-01,  3.3068e-01,  5.5749e-01,  ...,  1.7411e-01,\n",
       "          -6.6789e-01, -9.2671e-02],\n",
       "         ...,\n",
       "         [ 1.1379e-01,  1.0979e+00,  7.2509e-01,  ..., -6.3810e-01,\n",
       "          -6.5577e-02,  2.6211e-01],\n",
       "         [ 7.6289e-01,  8.7634e-01,  8.9856e-01,  ..., -4.0718e-01,\n",
       "          -3.9178e-01,  6.8957e-01],\n",
       "         [ 2.3181e-01,  7.7598e-01,  6.2806e-01,  ..., -7.3994e-01,\n",
       "          -3.5627e-01,  5.1874e-01]],\n",
       "\n",
       "        [[ 4.0753e-01,  6.8117e-01,  8.9936e-01,  ..., -2.7125e-01,\n",
       "          -1.1120e+00,  6.4905e-01],\n",
       "         [ 2.4358e-01,  5.8536e-01,  2.5616e-01,  ..., -1.8406e-01,\n",
       "          -1.1760e+00,  4.3645e-01],\n",
       "         [ 7.4048e-02,  1.0846e+00,  8.0253e-01,  ...,  1.7832e-01,\n",
       "          -6.4170e-01,  5.6647e-01],\n",
       "         ...,\n",
       "         [ 8.2323e-02,  8.6506e-01,  7.9245e-01,  ..., -8.3815e-02,\n",
       "          -1.2597e+00,  4.7310e-01],\n",
       "         [ 5.7524e-01,  1.0947e+00,  4.3541e-01,  ..., -1.4894e-01,\n",
       "          -9.9438e-01,  3.5903e-01],\n",
       "         [-5.9646e-02,  8.5212e-01,  1.2641e+00,  ..., -5.2564e-01,\n",
       "          -4.4244e-01,  7.7781e-01]],\n",
       "\n",
       "        [[-9.1616e-02,  4.2188e-01,  5.3892e-01,  ..., -6.2434e-01,\n",
       "          -1.1346e+00, -1.1440e-01],\n",
       "         [-5.8117e-01,  1.7388e-01,  5.8472e-01,  ..., -6.8364e-01,\n",
       "          -8.5956e-01,  3.7909e-01],\n",
       "         [-4.6879e-01,  4.3209e-01,  6.3396e-01,  ...,  3.4106e-01,\n",
       "          -1.1205e-03, -3.9556e-01],\n",
       "         ...,\n",
       "         [ 5.3107e-01,  9.8593e-01,  7.0523e-01,  ..., -9.0810e-01,\n",
       "          -7.0006e-01,  5.4260e-01],\n",
       "         [ 2.9262e-01,  1.0575e+00,  2.2236e-01,  ..., -2.9628e-01,\n",
       "          -3.8064e-01,  7.5366e-01],\n",
       "         [ 9.9487e-02,  1.1166e+00,  2.2295e-01,  ..., -6.6044e-01,\n",
       "          -2.9750e-01,  5.1776e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TestTransformerDecoderBlock(DS.num_condition_feat, DS.precursor_dataset.NUM_LABEL, num_heads=4, hidden_dim=64, hidden_layers=4)\n",
    "model.forward(target  = feat['target'], \n",
    "              context = feat['context'],)\n",
    "#tr = TestSequenceTrainer(model, lr=1e-4)\n",
    "#for i in range(5):\n",
    "#    train_loss = tr.train(train_dl)\n",
    "#    valid_loss, out = tr.test(valid_dl)\n",
    "#    test_loss, out = tr.test(test_dl)\n",
    "#    print('{:4d} {:9.5f} {:9.5f} {:9.5f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27967, (27967,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = out['pred'].argmax(-1)\n",
    "label = out['label']\n",
    "weight = out['weight']\n",
    "n_data, l_seq = pred.shape\n",
    "mask = np.hstack([np.ones((n_data, 1), dtype=bool), (label != EOS_LABEL)[..., :-1]])\n",
    "acc_rxn = []\n",
    "for p, l, m, w in zip(pred, label, mask, weight):\n",
    "    hit = (p[m] != l[m]).sum() == 0\n",
    "    acc_rxn.append(hit)\n",
    "len(acc_rxn), weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.99951   1.44498   1.44493\n",
      "   1   1.32844   1.18909   1.18908\n",
      "   2   1.15493   1.06664   1.06657\n",
      "   3   1.05857   0.98836   0.98805\n",
      "   4   0.99526   0.93284   0.93303\n",
      "   5   0.94666   0.88761   0.88701\n",
      "   6   0.90799   0.85305   0.85327\n",
      "   7   0.87553   0.82254   0.82096\n",
      "   8   0.85084   0.79534   0.79469\n",
      "   9   0.82855   0.77896   0.77786\n",
      "  10   0.80872   0.75424   0.75411\n",
      "  11   0.79169   0.73576   0.73554\n",
      "  12   0.77698   0.72410   0.72531\n",
      "  13   0.76392   0.70815   0.70855\n",
      "  14   0.75060   0.69569   0.69644\n",
      "  15   0.73886   0.68417   0.68380\n",
      "  16   0.73034   0.67255   0.67398\n",
      "  17   0.72001   0.66320   0.66334\n",
      "  18   0.71236   0.65547   0.65561\n",
      "  19   0.70242   0.64979   0.64865\n",
      "  20   0.69550   0.64075   0.64030\n",
      "  21   0.69033   0.63113   0.63170\n",
      "  22   0.68170   0.62679   0.62562\n",
      "  23   0.67514   0.61784   0.61814\n",
      "  24   0.66914   0.61233   0.61180\n",
      "  25   0.66434   0.60926   0.60969\n",
      "  26   0.65751   0.60311   0.60178\n",
      "  27   0.65465   0.59412   0.59472\n",
      "  28   0.64847   0.59000   0.59011\n",
      "  29   0.64517   0.58822   0.58704\n",
      "  30   0.63950   0.58575   0.58497\n",
      "  31   0.63599   0.57945   0.57940\n",
      "  32   0.63208   0.57600   0.57547\n",
      "  33   0.62906   0.57450   0.57423\n",
      "  34   0.62433   0.56648   0.56641\n",
      "  35   0.62137   0.56111   0.56050\n",
      "  36   0.61803   0.55912   0.55978\n",
      "  37   0.61653   0.55797   0.55798\n",
      "  38   0.61084   0.55661   0.55716\n",
      "  39   0.60936   0.55276   0.55382\n",
      "  40   0.60655   0.55022   0.55036\n",
      "  41   0.60315   0.54482   0.54513\n",
      "  42   0.60058   0.54696   0.54719\n",
      "  43   0.59875   0.54501   0.54520\n",
      "  44   0.59448   0.53836   0.53844\n",
      "  45   0.59076   0.53233   0.53326\n",
      "  46   0.58934   0.52979   0.52889\n",
      "  47   0.58716   0.53203   0.53233\n",
      "  48   0.58503   0.52671   0.52761\n",
      "  49   0.58242   0.52384   0.52404\n",
      "  50   0.58009   0.52084   0.52117\n",
      "  51   0.57880   0.52606   0.52457\n",
      "  52   0.57587   0.51772   0.51758\n",
      "  53   0.57417   0.51880   0.51896\n",
      "  54   0.57150   0.51318   0.51399\n",
      "  55   0.57121   0.51275   0.51327\n",
      "  56   0.57068   0.50851   0.50873\n",
      "  57   0.56630   0.50676   0.50752\n",
      "  58   0.56614   0.50819   0.50707\n",
      "  59   0.56388   0.50407   0.50411\n",
      "  60   0.56163   0.50279   0.50174\n",
      "  61   0.56025   0.50586   0.50551\n",
      "  62   0.55872   0.50086   0.50016\n",
      "  63   0.55761   0.49874   0.49882\n",
      "  64   0.55613   0.50020   0.50024\n",
      "  65   0.55494   0.49490   0.49479\n",
      "  66   0.55083   0.49229   0.49129\n",
      "  67   0.55134   0.49408   0.49454\n",
      "  68   0.55016   0.49049   0.48975\n",
      "  69   0.54970   0.48922   0.48841\n",
      "  70   0.54782   0.48929   0.48865\n",
      "  71   0.54489   0.48741   0.48870\n",
      "  72   0.54564   0.48565   0.48569\n",
      "  73   0.54372   0.48766   0.48704\n",
      "  74   0.54349   0.48518   0.48502\n",
      "  75   0.53987   0.48115   0.48079\n",
      "  76   0.53964   0.48018   0.47986\n",
      "  77   0.53940   0.48518   0.48496\n",
      "  78   0.53763   0.47931   0.47872\n",
      "  79   0.53708   0.47658   0.47676\n",
      "  80   0.53553   0.47503   0.47396\n",
      "  81   0.53260   0.47430   0.47522\n",
      "  82   0.53346   0.47301   0.47389\n",
      "  83   0.53290   0.47494   0.47383\n",
      "  84   0.53128   0.47643   0.47676\n",
      "  85   0.53098   0.46930   0.47010\n",
      "  86   0.52902   0.46807   0.46844\n",
      "  87   0.52821   0.46770   0.46837\n",
      "  88   0.52718   0.46771   0.46738\n",
      "  89   0.52610   0.46675   0.46745\n",
      "  90   0.52580   0.46681   0.46685\n",
      "  91   0.52480   0.46604   0.46637\n",
      "  92   0.52458   0.46370   0.46419\n",
      "  93   0.52457   0.46431   0.46415\n",
      "  94   0.51971   0.46270   0.46372\n",
      "  95   0.52000   0.46052   0.45975\n",
      "  96   0.51942   0.45893   0.45915\n",
      "  97   0.51971   0.45970   0.45935\n",
      "  98   0.51883   0.45833   0.45912\n",
      "  99   0.51738   0.45895   0.45705\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderBlock(DS.num_condition_feat, num_heads=4, hidden_dim=64, hidden_layers=4)\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(100):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, 5, 8, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5],\n",
       "       [5, 5, 5, 5, 8, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5],\n",
       "       [5, 5, 5, 8, 5, 5, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(self, context, max_len=20):\n",
    "        output_seq = torch.ones(context.shape[0], 1).long().to(self.device) * 443\n",
    "        for _ in range(max_len - 1):\n",
    "            output = self.forward(output_seq, context)\n",
    "            output_seq = torch.hstack([output_seq, output.argmax(-1)[:, -1:]])\n",
    "        seq = output_seq.cpu().numpy()[:, 1:]\n",
    "        j = (seq != EOS_LABEL).sum(1).max()\n",
    "        return seq[:, :j]\n",
    "\n",
    "generate(model, feat['context'][:3].to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5] [  5 148]\n",
      "[2 5] [  5 148]\n",
      "[2 5] [  2 107]\n",
      "[2 5] [  5 108]\n",
      "[5 8] [  8 124]\n",
      "[  5 106] [  5 189]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 113]\n",
      "[ 0 16 29] [  0  16 182]\n",
      "[ 0 16 29] [  0  16 182]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[  8  14 126 161] [ 14  66 108 250]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 8 51] [ 8 24 51]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  29 177]\n",
      "[ 7 18 29] [  7  88 303]\n",
      "[ 7 18 29] [  7  88 113]\n",
      "[ 7 18 29] [  7  88 113]\n",
      "[ 7 18 29] [  7  88 113]\n"
     ]
    }
   ],
   "source": [
    "for _prd, _lbl in zip(out_gen, feat['label'].cpu().numpy()):\n",
    "    prd = np.array(sorted(_prd[_prd != EOS_LABEL]))\n",
    "    lbl = np.array(sorted(_lbl[_lbl != EOS_LABEL]))\n",
    "    if len(prd) != len(lbl):\n",
    "        print(prd, lbl)\n",
    "    elif np.sum(prd != lbl) != 0:\n",
    "        print(prd, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,   2, 444, 444],\n",
       "       [  5,   2, 444, 444],\n",
       "       [  5,   2, 444, 444],\n",
       "       ...,\n",
       "       [ 12,   9, 444, 444],\n",
       "       [ 12,   9, 444, 444],\n",
       "       [ 12,   9, 444, 444]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,  13,  14,   4, 444]) tensor([  0,  13,  14,   4, 444, 444, 444], device='cuda:0')\n",
      "tensor([  5,  65, 444]) tensor([  5,  65, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,  76, 444]) tensor([  4,  76, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 27,  49, 444]) tensor([ 27,  49, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6, 197,  18, 444]) tensor([  6, 197,  18, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 50,   2,   5,   0, 444]) tensor([ 50,   2,   5,   0, 444, 444, 444], device='cuda:0')\n",
      "tensor([  1,  32,  25, 444]) tensor([  1,  32,  25, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,   0,   1, 444]) tensor([  3,   0,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   7,  13, 444]) tensor([  2,   7,  13, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 79,  27,  49, 444]) tensor([ 79,  27,  49, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   1,  30, 444]) tensor([  6,   1,  30, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,   8, 444]) tensor([ 10,   8, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 14, 108,   9, 444]) tensor([ 14, 108,   9, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  14,   4, 444]) tensor([  0,  14,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 13,  20,  25, 444]) tensor([ 13,  20,  25, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 16,  11,   1,  30, 444]) tensor([ 16,  11,   1,  30, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,  10, 444]) tensor([  4,  10, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 61,  45, 444]) tensor([ 61,  45, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,  42, 444]) tensor([  6,  42, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 75,  18,   0,  11, 444]) tensor([ 75,  18,   0,  11, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 18,  10,   4, 444]) tensor([ 18,  10,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,  15,   4, 444]) tensor([  3,  15,   4, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 11,  20, 444]) tensor([ 11,  20, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  50, 444]) tensor([  8,  50, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  1,  50,   5, 444]) tensor([  1,  50,   5, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 30,   0,   3,   7,   2, 444]) tensor([ 30,   0,   3,   7,   2, 444, 444], device='cuda:0')\n",
      "tensor([  7,  29,  18, 444]) tensor([  7,  29,  18, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  11,   7, 444]) tensor([  0,  11,   7, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  4,   0,  14, 444]) tensor([  4,   0,  14, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 80,   5, 444]) tensor([ 80,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   5, 444]) tensor([  6,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 38,   4,  13,   3, 444]) tensor([ 38,   4,  13,   3, 444, 444, 444], device='cuda:0')\n",
      "tensor([  7,   4,  22,  16, 444]) tensor([  7,   4,  22,  16, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   7,   6,  22,   1, 444]) tensor([  2,   7,   6,  22,   1, 444, 444], device='cuda:0')\n",
      "tensor([ 53,  58,   0, 444]) tensor([ 53,  58,   0, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,   2, 444]) tensor([  0,   2, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 33,   4, 444]) tensor([ 33,   4, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 11,  12, 444]) tensor([ 11,  12, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  3,  12,  16,  68, 444]) tensor([  3,  12,  16,  68, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  84,   5, 444]) tensor([  8,  84,   5, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 35,  33,  29, 444]) tensor([ 35,  33,  29, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 35,  68, 444]) tensor([ 35,  68, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  6,   2, 444]) tensor([  6,   2, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  2,   5, 444]) tensor([  2,   5, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,   6, 444]) tensor([ 10,   6, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  35, 444]) tensor([  8,  35, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 23,  27,  52, 444]) tensor([ 23,  27,  52, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 32,  20,  25,   2, 444]) tensor([ 32,  20,  25,   2, 444, 444, 444], device='cuda:0')\n",
      "tensor([135,  79,  40, 444]) tensor([135,  79,  40, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([168,   7, 444]) tensor([168,   7, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,   2,  32, 444]) tensor([  0,   2,  32, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 44,  20,   1, 444]) tensor([ 44,  20,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([370,  50,   3,   5, 444]) tensor([370,  50,   3,   5, 444, 444, 444], device='cuda:0')\n",
      "tensor([  0,  46,  16, 444]) tensor([  0,  46,  16, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 17,   0, 444]) tensor([ 17,   0, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  8,  24, 444]) tensor([  8,  24, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([  5,  13,  46, 444]) tensor([  5,  13,  46, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 10,  18,   1, 444]) tensor([ 10,  18,   1, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 14,   1,   3,   5, 444]) tensor([ 14,   1,   3,   5, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 29,   6,  42, 444]) tensor([ 29,   6,  42, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 70,  27, 444]) tensor([ 70,  27, 444, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([ 15,  11,   9, 444]) tensor([ 15,  11,   9, 444, 444, 444, 444], device='cuda:0')\n",
      "tensor([149,  98, 444]) tensor([149,  98, 444, 444, 444, 444, 444], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for l, m in zip(feat['label'], feat['target'] != 444):\n",
    "    print(l[m].cpu(), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import ReactionDataset\n",
    "from src.networks import TransformerDecoderBlock\n",
    "import torch, gc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset(feat_type='cgcnn')\n",
    "DS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "             heat_temp_key=('heat_temp','median'))\n",
    "#DS.to('cuda')\n",
    "\n",
    "years = np.array([d.year for d in DS])\n",
    "train_mask = years < 2016\n",
    "valid_mask = (years >= 2016) & years < 2018\n",
    "test_mask = years >= 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-14 14:23:23 787949:787949 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-14 14:23:24 787949:787949 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.90%     322.000us        79.97%      28.589ms       9.530ms       0.000us         0.00%       1.259ms     419.667us      29.19 Kb     -61.88 Kb           0 b     -97.50 Kb             3  \n",
      "                                          backward_pass        28.79%      10.293ms        36.80%      13.157ms       4.386ms       0.000us         0.00%     124.000us      41.333us        -192 b        -192 b     -10.62 Mb      -9.93 Mb             3  \n",
      "                                        model_inference         6.24%       2.229ms        23.56%       8.424ms       2.808ms       0.000us         0.00%       1.132ms     377.333us         192 b      -2.46 Kb      10.62 Mb      -8.77 Mb             3  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        11.20%       4.005ms        17.81%       6.367ms       2.122ms       0.000us         0.00%       0.000us       0.000us      87.56 Kb    -582.65 Kb           0 b           0 b             3  \n",
      "                              Optimizer.step#AdamW.step         4.21%       1.504ms         7.47%       2.670ms     890.000us       0.000us         0.00%     121.000us      40.333us           0 b         -12 b           0 b    -705.00 Kb             3  \n",
      "                                            aten::index         4.44%       1.589ms         5.24%       1.874ms       4.880us       0.000us         0.00%       0.000us       0.000us     535.25 Kb     513.00 Kb           0 b           0 b           384  \n",
      "                                             aten::triu         4.71%       1.682ms         4.71%       1.682ms     280.333us       0.000us         0.00%       0.000us       0.000us       1.15 Kb       1.15 Kb           0 b           0 b             6  \n",
      "                                          aten::reshape         1.41%     503.000us         4.25%       1.518ms       2.517us       0.000us         0.00%     120.000us       0.199us      10.66 Kb      10.53 Kb       6.95 Mb    -280.00 Kb           603  \n",
      "                                            aten::clone         0.87%     310.000us         3.97%       1.421ms       8.458us       0.000us         0.00%     167.000us       0.994us      21.00 Kb           0 b      10.32 Mb      -2.65 Mb           168  \n",
      "                                           aten::linear         0.58%     207.000us         3.96%       1.417ms      29.521us       0.000us         0.00%     248.000us       5.167us           0 b           0 b       6.90 Mb     168.00 Kb            48  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.33%     118.000us         3.60%       1.288ms       9.135us       0.000us         0.00%     102.000us       0.723us           0 b           0 b    -280.00 Kb      -7.16 Mb           141  \n",
      "                                       cudaLaunchKernel         3.49%       1.249ms         3.49%       1.249ms       1.699us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      -4.07 Mb      -4.07 Mb           735  \n",
      "                                            aten::copy_         1.48%     529.000us         3.03%       1.084ms       4.818us     214.000us         5.25%     214.000us       0.951us      10.50 Kb     -10.50 Kb     -56.00 Kb     -56.00 Kb           225  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.50%     179.000us         2.90%       1.035ms      34.500us       0.000us         0.00%     544.000us      18.133us           0 b           0 b      -3.59 Mb      -5.70 Mb            30  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.03%       9.000us         2.76%     986.000us      82.167us       0.000us         0.00%       1.422ms     118.500us        -192 b        -192 b      -2.06 Mb      -4.20 Mb            12  \n",
      "                                          ViewBackward0         0.72%     258.000us         2.70%     966.000us       6.851us       0.000us         0.00%      78.000us       0.553us           0 b           0 b       5.91 Mb       1.04 Mb           141  \n",
      "            ScaledDotProductEfficientAttentionBackward0         0.29%     104.000us         2.54%     909.000us      75.750us       0.000us         0.00%       1.304ms     108.667us           0 b           0 b       1.97 Mb     168.00 Kb            12  \n",
      "                                               aten::mm         2.07%     739.000us         2.49%     890.000us       8.018us     755.000us        18.52%     755.000us       6.802us           0 b           0 b       5.24 Mb       5.24 Mb           111  \n",
      "aten::_scaled_dot_product_efficient_attention_backwa...         0.41%     147.000us         2.44%     873.000us      72.750us       0.000us         0.00%       1.422ms     118.500us           0 b           0 b       1.97 Mb     -56.00 Kb            12  \n",
      "                                            aten::empty         2.42%     864.000us         2.42%     864.000us       1.946us       0.000us         0.00%       0.000us       0.000us      16.54 Kb      16.54 Kb     118.00 Mb     118.00 Mb           444  \n",
      "                    aten::_efficient_attention_backward         0.57%     203.000us         1.98%     708.000us      59.000us       1.357ms        33.29%       1.422ms     118.500us           0 b           0 b       2.02 Mb     -97.67 Mb            12  \n",
      "                                       aten::empty_like         0.82%     292.000us         1.92%     685.000us       2.751us       0.000us         0.00%       0.000us       0.000us      17.50 Kb       3.50 Kb      14.08 Mb       3.45 Mb           249  \n",
      "autograd::engine::evaluate_function: SelectBackward0...        -0.08%     -28.000us         1.85%     663.000us      22.100us       0.000us         0.00%      78.000us       2.600us           0 b           0 b     168.00 Kb      -4.54 Mb            30  \n",
      "                                         AddmmBackward0         0.39%     140.000us         1.83%     655.000us      21.833us       0.000us         0.00%     401.000us      13.367us           0 b           0 b       2.14 Mb           0 b            30  \n",
      "                                           aten::matmul         0.18%      65.000us         1.66%     593.000us      32.944us       0.000us         0.00%      96.000us       5.333us           0 b           0 b       2.95 Mb           0 b            18  \n",
      "                                        SelectBackward0         0.39%     141.000us         1.39%     498.000us      16.600us       0.000us         0.00%      43.000us       1.433us           0 b           0 b       4.10 Mb     896.00 Kb            30  \n",
      "                                       aten::contiguous         0.30%     108.000us         1.37%     491.000us       9.093us       0.000us         0.00%      48.000us       0.889us      21.00 Kb       3.50 Kb       3.10 Mb     854.00 Kb            54  \n",
      "                                    aten::_foreach_sqrt         0.59%     212.000us         1.34%     479.000us     159.667us      15.000us         0.37%      15.000us       5.000us           0 b           0 b     705.00 Kb           0 b             3  \n",
      "                                  aten::select_backward         0.20%      71.000us         1.34%     478.000us      15.933us       0.000us         0.00%      59.000us       1.967us           0 b           0 b       4.16 Mb     -56.00 Kb            30  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.15%      53.000us         1.26%     449.000us      24.944us       0.000us         0.00%     282.000us      15.667us           0 b           0 b      -1.83 Mb      -2.95 Mb            18  \n",
      "                     aten::scaled_dot_product_attention         0.25%      88.000us         1.23%     439.000us      36.583us       0.000us         0.00%     592.000us      49.333us         192 b          16 b       1.41 Mb     120.00 Kb            12  \n",
      "                                    aten::empty_strided         1.23%     438.000us         1.23%     438.000us       2.179us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       2.88 Mb       2.88 Mb           201  \n",
      "                                            aten::addmm         1.05%     377.000us         1.19%     424.000us      14.133us     143.000us         3.51%     143.000us       4.767us           0 b           0 b       3.95 Mb       7.95 Mb            30  \n",
      "                                              aten::sum         0.91%     325.000us         1.17%     420.000us       6.667us     287.000us         7.04%     287.000us       4.556us           0 b           0 b     220.50 Kb     220.50 Kb            63  \n",
      "                                          aten::dropout         0.15%      52.000us         1.17%     419.000us      17.458us       0.000us         0.00%      33.000us       1.375us           0 b           0 b       2.05 Mb     210.00 Kb            24  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.24%      87.000us         1.17%     419.000us      23.278us       0.000us         0.00%     115.000us       6.389us           0 b           0 b      -1.04 Mb      -2.04 Mb            18  \n",
      "                                   aten::native_dropout         0.48%     171.000us         1.11%     396.000us      16.500us      36.000us         0.88%      36.000us       1.500us           0 b           0 b       2.05 Mb    -322.00 Kb            24  \n",
      "                                            MmBackward0         0.23%      83.000us         1.11%     396.000us      22.000us       0.000us         0.00%     282.000us      15.667us           0 b           0 b       1.12 Mb           0 b            18  \n",
      "                                       aten::layer_norm         0.18%      65.000us         1.08%     386.000us      21.444us       0.000us         0.00%      48.000us       2.667us           0 b           0 b       1.05 Mb     120.00 Kb            18  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.31%     111.000us         1.07%     381.000us      31.750us       0.000us         0.00%     645.000us      53.750us         192 b           0 b       1.41 Mb           0 b            12  \n",
      "                                aten::native_layer_norm         0.55%     198.000us         1.00%     359.000us      19.944us      54.000us         1.32%      54.000us       3.000us           0 b           0 b       1.05 Mb           0 b            18  \n",
      "                                        aten::transpose         0.93%     331.000us         0.94%     337.000us       0.614us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           549  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.45%     160.000us         0.93%     333.000us       2.581us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           129  \n",
      "                               NativeLayerNormBackward0         0.15%      52.000us         0.93%     332.000us      18.444us       0.000us         0.00%     115.000us       6.389us           0 b           0 b       1.00 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: TransposeBackwa...         0.44%     159.000us         0.92%     328.000us       4.205us       0.000us         0.00%      15.000us       0.192us           0 b           0 b    -672.00 Kb    -840.00 Kb            78  \n",
      "                                             aten::add_         0.65%     232.000us         0.90%     320.000us       1.693us      70.000us         1.72%      70.000us       0.370us           0 b           0 b    -336.00 Kb    -336.00 Kb           189  \n",
      "                                               aten::to         0.04%      14.000us         0.87%     310.000us       0.698us       0.000us         0.00%       2.000us       0.005us       3.50 Kb           0 b      97.50 Kb           0 b           444  \n",
      "                                                aten::t         0.67%     240.000us         0.84%     302.000us       1.274us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           237  \n",
      "                                         aten::_to_copy         0.14%      51.000us         0.80%     287.000us      19.133us       0.000us         0.00%       1.000us       0.067us       3.50 Kb      -7.00 Kb      97.50 Kb           0 b            15  \n",
      "                       aten::native_layer_norm_backward         0.40%     142.000us         0.78%     280.000us      15.556us     115.000us         2.82%     115.000us       6.389us           0 b           0 b       1.00 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: NativeDropoutBa...         0.07%      25.000us         0.77%     276.000us      11.500us       0.000us         0.00%      33.000us       1.375us           0 b           0 b     616.00 Kb      -1.37 Mb            24  \n",
      "                                            aten::slice         0.71%     253.000us         0.72%     257.000us       0.635us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           405  \n",
      "                                            aten::zeros         0.16%      56.000us         0.69%     248.000us       7.515us       0.000us         0.00%      30.000us       0.909us           0 b           0 b       4.43 Mb           0 b            33  \n",
      "                     aten::_efficient_attention_forward         0.36%     129.000us         0.64%     228.000us      19.000us     645.000us        15.82%     645.000us      53.750us         192 b          -8 b       1.41 Mb           0 b            12  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.31%     110.000us         0.62%     222.000us       5.692us       0.000us         0.00%     106.000us       2.718us           0 b           0 b       9.00 Kb           0 b            39  \n",
      "                                 NativeDropoutBackward0         0.15%      52.000us         0.61%     219.000us       9.125us       0.000us         0.00%      28.000us       1.167us           0 b           0 b       1.64 Mb     336.00 Kb            24  \n",
      "                          aten::native_dropout_backward         0.24%      85.000us         0.56%     199.000us       8.292us      33.000us         0.81%      33.000us       1.375us           0 b           0 b       1.64 Mb    -392.00 Kb            24  \n",
      "                                    aten::_foreach_add_         0.48%     172.000us         0.54%     192.000us      32.000us      11.000us         0.27%      11.000us       1.833us           0 b           0 b           0 b           0 b             6  \n",
      "                                            aten::zero_         0.15%      55.000us         0.51%     182.000us       4.667us       0.000us         0.00%      39.000us       1.000us           0 b           0 b           0 b           0 b            39  \n",
      "                                    aten::_foreach_mul_         0.37%     134.000us         0.48%     172.000us      28.667us      24.000us         0.59%      24.000us       4.000us           0 b           0 b           0 b           0 b             6  \n",
      "                                              aten::cat         0.39%     138.000us         0.45%     161.000us       7.667us      15.000us         0.37%      15.000us       0.714us      81.75 Kb      81.75 Kb      75.00 Kb      75.00 Kb            21  \n",
      "                                           aten::repeat         0.19%      69.000us         0.44%     156.000us      26.000us       0.000us         0.00%       3.000us       0.500us       5.25 Kb       3.50 Kb     168.00 Kb           0 b             6  \n",
      "                                            aten::fill_         0.27%      95.000us         0.43%     154.000us       3.020us      42.000us         1.03%      42.000us       0.824us           0 b           0 b           0 b           0 b            51  \n",
      "                        torch::autograd::AccumulateGrad         0.34%     123.000us         0.42%     150.000us       1.163us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           129  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         0.41%     148.000us         0.41%     148.000us      49.333us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -705.00 Kb    -705.00 Kb             3  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.21%      75.000us         0.41%     145.000us       3.021us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            48  \n",
      "                                        cudaMemcpyAsync         0.40%     144.000us         0.40%     144.000us       3.200us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            45  \n",
      "                                        aten::embedding         0.09%      31.000us         0.40%     143.000us      47.667us       0.000us         0.00%       6.000us       2.000us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                              aten::add         0.30%     108.000us         0.40%     142.000us       6.762us      33.000us         0.81%      33.000us       1.571us           0 b           0 b       1.15 Mb       1.15 Mb            21  \n",
      "     autograd::engine::evaluate_function: MeanBackward0         0.06%      20.000us         0.39%     140.000us      46.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b       6.00 Kb           0 b             3  \n",
      "                                             aten::view         0.37%     132.000us         0.37%     132.000us       0.186us       0.000us         0.00%       0.000us       0.000us      11.84 Kb      11.84 Kb     -56.00 Kb     -56.00 Kb           708  \n",
      "autograd::engine::evaluate_function: SplitWithSizesB...         0.06%      22.000us         0.36%     129.000us      10.750us       0.000us         0.00%      15.000us       1.250us           0 b           0 b      -3.00 Kb     -69.50 Kb            12  \n",
      "                                       aten::batch_norm         0.02%       6.000us         0.35%     125.000us      41.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                          MeanBackward0         0.05%      18.000us         0.34%     120.000us      40.000us       0.000us         0.00%       3.000us       1.000us           0 b          -8 b       6.00 Kb           0 b             3  \n",
      "                           aten::_batch_norm_impl_index         0.03%      11.000us         0.33%     119.000us      39.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                              aten::mul         0.24%      86.000us         0.32%     116.000us       6.444us      18.000us         0.44%      18.000us       1.000us           0 b           0 b     684.00 Kb     684.00 Kb            18  \n",
      "                               aten::cross_entropy_loss         0.03%       9.000us         0.32%     115.000us      38.333us       0.000us         0.00%      12.000us       4.000us           0 b           0 b       2.29 Mb           0 b             3  \n",
      "                                     aten::index_select         0.13%      46.000us         0.29%     103.000us      34.333us       6.000us         0.15%       6.000us       2.000us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                aten::native_batch_norm         0.16%      56.000us         0.29%     102.000us      34.000us      18.000us         0.44%      18.000us       6.000us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "autograd::engine::evaluate_function: UnsafeViewBackw...         0.16%      58.000us         0.28%     101.000us       2.806us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            36  \n",
      "                                SplitWithSizesBackward0         0.05%      18.000us         0.28%     101.000us       8.417us       0.000us         0.00%      14.000us       1.167us           0 b           0 b      54.50 Kb      -8.50 Kb            12  \n",
      "                                           aten::concat         0.17%      62.000us         0.28%     100.000us      11.111us       0.000us         0.00%       0.000us       0.000us      81.75 Kb      46.50 Kb           0 b           0 b             9  \n",
      "autograd::engine::evaluate_function: NativeBatchNorm...         0.03%      11.000us         0.27%      97.000us      32.333us       0.000us         0.00%      13.000us       4.333us           0 b           0 b     -24.00 Kb     -51.00 Kb             3  \n",
      "                                           aten::select         0.26%      93.000us         0.27%      95.000us       1.583us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            60  \n",
      "                                aten::_foreach_addcmul_         0.24%      85.000us         0.26%      93.000us      31.000us      12.000us         0.29%      12.000us       4.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                              aten::div         0.15%      55.000us         0.24%      86.000us      28.667us       3.000us         0.07%       3.000us       1.000us           8 b           8 b       6.00 Kb       6.00 Kb             3  \n",
      "                               NativeBatchNormBackward0         0.02%       7.000us         0.24%      86.000us      28.667us       0.000us         0.00%      13.000us       4.333us           0 b           0 b      27.00 Kb           0 b             3  \n",
      "                                     TransposeBackward0         0.10%      34.000us         0.24%      85.000us       1.090us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            78  \n",
      "                                aten::_foreach_addcdiv_         0.21%      74.000us         0.23%      82.000us      27.333us      23.000us         0.56%      23.000us       7.667us           0 b           0 b           0 b           0 b             3  \n",
      "                       aten::native_batch_norm_backward         0.11%      39.000us         0.22%      79.000us      26.333us      13.000us         0.32%      13.000us       4.333us           0 b           0 b      27.00 Kb      -1.50 Kb             3  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.05%      17.000us         0.21%      76.000us      25.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b       2.27 Mb      -7.50 Kb             3  \n",
      "                                    aten::_foreach_div_         0.19%      67.000us         0.21%      74.000us      24.667us      21.000us         0.52%      21.000us       7.000us           0 b           0 b           0 b           0 b             3  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.04%      14.000us         0.19%      68.000us      11.333us       0.000us         0.00%       6.000us       1.000us           0 b           0 b    -560.00 Kb      -1.20 Mb             6  \n",
      "                                      aten::nll_loss_nd         0.01%       3.000us         0.18%      64.000us      21.333us       0.000us         0.00%       6.000us       2.000us           0 b           0 b       7.50 Kb           0 b             3  \n",
      "                                             aten::relu         0.06%      21.000us         0.18%      63.000us      10.500us       0.000us         0.00%       6.000us       1.000us           0 b           0 b     672.00 Kb           0 b             6  \n",
      "                                         aten::nll_loss         0.02%       7.000us         0.17%      61.000us      20.333us       0.000us         0.00%       6.000us       2.000us           0 b           0 b       7.50 Kb           0 b             3  \n",
      "                                 aten::split_with_sizes         0.16%      56.000us         0.17%      60.000us       5.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                       NllLossBackward0         0.02%       8.000us         0.17%      59.000us      19.667us       0.000us         0.00%       9.000us       3.000us           0 b           0 b       2.28 Mb           0 b             3  \n",
      "                                        aten::unsqueeze         0.15%      54.000us         0.16%      58.000us       1.933us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...        -0.03%     -11.000us         0.16%      58.000us      19.333us       0.000us         0.00%      53.000us      17.667us           0 b           0 b           0 b    -168.00 Kb             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 35.749ms\n",
      "Self CUDA time total: 4.076ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.cuda.init()\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=64, sampler=SubsetRandomSampler(np.where(train_mask)[0]), \n",
    "                      collate_fn=DS.cfn)#num_workers=1, prefetch_factor=4, collate_fn=DS.cfn)\n",
    "\n",
    "valid_dl = DataLoader(DS, batch_size=2048, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=2048, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "\n",
    "model = TransformerDecoderBlock(DS.num_condition_feat)\n",
    "model.to('cuda')\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "crit = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "with profile(\n",
    "    activities=[\n",
    "        ProfilerActivity.CPU,\n",
    "        ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('../dump/log_dir'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    "    ) as prof:\n",
    "\n",
    "\n",
    "    for _feat, _ in train_dl:\n",
    "        feat = {k:v.to('cuda') for k,v in _feat.items()}\n",
    "        n_batch, l_seq = feat['label'].shape\n",
    "        with record_function('model_inference'):\n",
    "            pred = model(**feat)\n",
    "#        with record_function('0_compute_loss'):\n",
    "            _loss = crit(pred.reshape(n_batch * l_seq, -1), feat['label'].view(-1))\n",
    "#            loss = (_loss * weight)[mask].mean()\n",
    "            loss = (_loss * feat['weight']).mean()\n",
    "        with record_function('backward_pass'):\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        prof.step()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=100))\n",
    "#print(prof.key_averages().table(row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.58%     434.000us        90.70%      68.182ms      22.727ms       0.000us         0.00%       3.002ms       1.001ms     116.75 Kb    -233.50 Kb    -725.00 Kb           0 b             3  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        30.82%      23.168ms        47.79%      35.928ms      11.976ms       0.000us         0.00%       0.000us       0.000us     350.25 Kb      -2.08 Mb           0 b           0 b             3  \n",
      "                                        model_inference         5.89%       4.426ms        21.41%      16.096ms       5.365ms       0.000us         0.00%       2.884ms     961.333us         192 b     -16.46 Kb      43.13 Mb     -34.24 Mb             3  \n",
      "                                          backward_pass        13.76%      10.342ms        20.92%      15.724ms       5.241ms       0.000us         0.00%     118.000us      39.333us        -192 b        -192 b     -43.84 Mb     -43.15 Mb             3  \n",
      "                              Optimizer.step#AdamW.step         3.44%       2.587ms         6.65%       5.001ms       1.667ms       0.000us         0.00%     115.000us      38.333us           0 b         -12 b           0 b    -705.00 Kb             3  \n",
      "                                             aten::triu         2.72%       2.045ms         2.72%       2.045ms     340.833us       0.000us         0.00%       0.000us       0.000us       1.15 Kb       1.15 Kb           0 b           0 b             6  \n",
      "                                    aten::_foreach_sqrt         0.45%     341.000us         1.09%     822.000us     274.000us      15.000us         0.14%      15.000us       5.000us           0 b           0 b     705.00 Kb           0 b             3  \n",
      "                    Optimizer.zero_grad#AdamW.zero_grad         0.37%     281.000us         0.37%     281.000us      93.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -705.00 Kb    -705.00 Kb             3  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.09%      65.000us         1.26%     946.000us      78.833us       0.000us         0.00%       4.985ms     415.417us        -192 b        -192 b      -8.25 Mb     -15.47 Mb            12  \n",
      "                               aten::cross_entropy_loss         0.01%       7.000us         0.30%     227.000us      75.667us       0.000us         0.00%      30.000us      10.000us           0 b           0 b      10.56 Mb      -3.75 Mb             3  \n",
      "            ScaledDotProductEfficientAttentionBackward0         0.04%      32.000us         1.17%     881.000us      73.417us       0.000us         0.00%       4.985ms     415.417us           0 b           0 b       7.22 Mb    -672.00 Kb            12  \n",
      "                                           aten::matmul         0.18%     133.000us         1.75%       1.312ms      72.889us       0.000us         0.00%      96.000us       5.333us           0 b           0 b      11.81 Mb           0 b            18  \n",
      "                     aten::scaled_dot_product_attention         0.20%     148.000us         1.16%     869.000us      72.417us       0.000us         0.00%       1.925ms     160.417us         192 b          16 b       5.62 Mb     480.00 Kb            12  \n",
      "                                        aten::embedding         0.06%      44.000us         0.28%     213.000us      71.000us       0.000us         0.00%       6.000us       2.000us           0 b           0 b     672.00 Kb           0 b             3  \n",
      "aten::_scaled_dot_product_efficient_attention_backwa...         0.20%     149.000us         1.13%     849.000us      70.750us       0.000us         0.00%       4.985ms     415.417us           0 b           0 b       7.88 Mb    -448.00 Kb            12  \n",
      "                                    aten::_foreach_add_         0.41%     311.000us         0.55%     415.000us      69.167us      10.000us         0.09%      10.000us       1.667us           0 b           0 b           0 b           0 b             6  \n",
      "                                           aten::linear         0.37%     277.000us         4.14%       3.111ms      64.812us       0.000us         0.00%     411.000us       8.562us           0 b           0 b      28.30 Mb           0 b            48  \n",
      "                                    aten::_foreach_mul_         0.44%     329.000us         0.51%     383.000us      63.833us      24.000us         0.23%      24.000us       4.000us           0 b           0 b           0 b           0 b             6  \n",
      "                                       aten::batch_norm         0.01%       8.000us         0.25%     190.000us      63.333us       0.000us         0.00%      24.000us       8.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "          aten::_scaled_dot_product_efficient_attention         0.27%     202.000us         1.00%     751.000us      62.583us       0.000us         0.00%       2.100ms     175.000us         192 b           0 b       5.62 Mb           0 b            12  \n",
      "                                aten::_foreach_addcdiv_         0.22%     167.000us         0.24%     184.000us      61.333us      21.000us         0.20%      21.000us       7.000us           0 b           0 b           0 b           0 b             3  \n",
      "     autograd::engine::evaluate_function: MeanBackward0         0.04%      27.000us         0.24%     183.000us      61.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                aten::_foreach_addcmul_         0.22%     165.000us         0.24%     183.000us      61.000us      12.000us         0.11%      12.000us       4.000us           0 b           0 b           0 b           0 b             3  \n",
      "                           aten::_batch_norm_impl_index         0.03%      20.000us         0.24%     182.000us      60.667us       0.000us         0.00%      24.000us       8.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "                    aten::_efficient_attention_backward         0.25%     185.000us         0.93%     696.000us      58.000us       4.910ms        46.48%       4.985ms     415.417us           0 b           0 b       8.31 Mb    -390.83 Mb            12  \n",
      "                                    aten::_foreach_div_         0.20%     147.000us         0.22%     164.000us      54.667us      18.000us         0.17%      18.000us       6.000us           0 b           0 b           0 b           0 b             3  \n",
      "                                     aten::index_select         0.10%      73.000us         0.21%     160.000us      53.333us       6.000us         0.06%       6.000us       2.000us           0 b           0 b     672.00 Kb           0 b             3  \n",
      "                                          MeanBackward0         0.03%      23.000us         0.21%     156.000us      52.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                           aten::concat         0.16%     122.000us         0.61%     457.000us      50.778us       0.000us         0.00%       0.000us       0.000us     327.00 Kb           0 b           0 b           0 b             9  \n",
      "                                aten::native_batch_norm         0.10%      76.000us         0.20%     152.000us      50.667us      24.000us         0.23%      24.000us       8.000us           0 b           0 b      99.00 Kb     -32.00 Kb             3  \n",
      "                                      aten::nll_loss_nd         0.01%       9.000us         0.17%     130.000us      43.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      22.50 Kb           0 b             3  \n",
      "                                       aten::layer_norm         0.13%      94.000us         1.01%     762.000us      42.333us       0.000us         0.00%     120.000us       6.667us           0 b           0 b       4.18 Mb     238.00 Kb            18  \n",
      "                                         aten::nll_loss         0.01%       8.000us         0.16%     121.000us      40.333us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      22.50 Kb           0 b             3  \n",
      "                                aten::native_layer_norm         0.53%     396.000us         0.95%     717.000us      39.833us     127.000us         1.20%     127.000us       7.056us           0 b           0 b       4.18 Mb           0 b            18  \n",
      "                     aten::_efficient_attention_forward         0.32%     237.000us         0.61%     460.000us      38.333us       2.100ms        19.88%       2.100ms     175.000us         192 b           8 b       5.62 Mb           0 b            12  \n",
      "                                              aten::div         0.09%      70.000us         0.15%     115.000us      38.333us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      21.00 Kb      21.00 Kb             3  \n",
      "                                 aten::nll_loss_forward         0.07%      52.000us         0.15%     113.000us      37.667us       6.000us         0.06%       9.000us       3.000us           0 b           0 b      22.50 Kb      22.50 Kb             3  \n",
      "                                          aten::dropout         0.15%     111.000us         1.20%     902.000us      37.583us       0.000us         0.00%      42.000us       1.750us           0 b           0 b       8.20 Mb     840.00 Kb            24  \n",
      "                                         aten::_to_copy         0.05%      34.000us         0.60%     450.000us      37.500us       0.000us         0.00%      21.000us       1.750us           0 b     -28.00 Kb     381.00 Kb           0 b            12  \n",
      "                                   aten::native_dropout         0.51%     386.000us         1.12%     845.000us      35.208us      48.000us         0.45%      48.000us       2.000us           0 b           0 b       8.20 Mb    -672.00 Kb            24  \n",
      "                                           aten::repeat         0.12%      87.000us         0.28%     211.000us      35.167us       0.000us         0.00%       3.000us       0.500us      21.00 Kb           0 b     672.00 Kb           0 b             6  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.22%     167.000us         1.39%       1.043ms      34.767us       0.000us         0.00%       1.030ms      34.333us           0 b           0 b     -15.42 Mb     -22.56 Mb            30  \n",
      "autograd::engine::evaluate_function: NativeBatchNorm...         0.01%      10.000us         0.14%     102.000us      34.000us       0.000us         0.00%      18.000us       6.000us           0 b           0 b     -96.00 Kb    -195.00 Kb             3  \n",
      "                                        aten::ones_like         0.02%      13.000us         0.13%     100.000us      33.333us       0.000us         0.00%       3.000us       1.000us           0 b           0 b       1.50 Kb           0 b             3  \n",
      "                                            aten::addmm         1.01%     759.000us         1.23%     921.000us      30.700us     279.000us         2.64%     279.000us       9.300us           0 b           0 b      16.49 Mb      16.49 Mb            30  \n",
      "                               NativeBatchNormBackward0         0.01%      10.000us         0.12%      92.000us      30.667us       0.000us         0.00%      18.000us       6.000us           0 b           0 b      99.00 Kb           0 b             3  \n",
      "                                             aten::mean         0.09%      67.000us         0.11%      84.000us      28.000us       9.000us         0.09%       9.000us       3.000us           0 b           0 b       1.50 Kb       1.50 Kb             3  \n",
      "                       aten::native_batch_norm_backward         0.06%      44.000us         0.11%      82.000us      27.333us      18.000us         0.17%      18.000us       6.000us           0 b           0 b      99.00 Kb      -3.00 Kb             3  \n",
      "                                      aten::log_softmax         0.03%      25.000us         0.11%      79.000us      26.333us       0.000us         0.00%      14.000us       4.667us           0 b           0 b      10.54 Mb       3.75 Mb             3  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.07%      50.000us         0.61%     457.000us      25.389us       0.000us         0.00%     396.000us      22.000us           0 b           0 b      -7.73 Mb     -11.81 Mb            18  \n",
      "                                             aten::relu         0.06%      48.000us         0.20%     147.000us      24.500us       0.000us         0.00%       6.000us       1.000us           0 b           0 b       2.62 Mb           0 b             6  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.01%       9.000us         0.10%      72.000us      24.000us       0.000us         0.00%      16.000us       5.333us           0 b           0 b       9.10 Mb     -22.50 Kb             3  \n",
      "                                   aten::_foreach_lerp_         0.07%      49.000us         0.09%      68.000us      22.667us      15.000us         0.14%      15.000us       5.000us           0 b           0 b           0 b           0 b             3  \n",
      "autograd::engine::evaluate_function: NativeLayerNorm...         0.04%      27.000us         0.54%     403.000us      22.389us       0.000us         0.00%     252.000us      14.000us           0 b           0 b      -4.17 Mb      -8.56 Mb            18  \n",
      "                                         AddmmBackward0         0.16%     121.000us         0.89%     669.000us      22.300us       0.000us         0.00%     732.000us      24.400us           0 b           0 b       7.56 Mb           0 b            30  \n",
      "                                            MmBackward0         0.11%      82.000us         0.53%     401.000us      22.278us       0.000us         0.00%     391.000us      21.722us           0 b           0 b       3.86 Mb           0 b            18  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...        -0.04%     -29.000us         0.09%      66.000us      22.000us       0.000us         0.00%     197.000us      65.667us           0 b           0 b    -504.00 Kb    -784.00 Kb             3  \n",
      "                                     aten::_log_softmax         0.06%      46.000us         0.09%      65.000us      21.667us      21.000us         0.20%      21.000us       7.000us           0 b           0 b      10.54 Mb      10.54 Mb             3  \n",
      "autograd::engine::evaluate_function: SelectBackward0...        -0.03%     -20.000us         0.86%     644.000us      21.467us       0.000us         0.00%      84.000us       2.800us           0 b           0 b       2.41 Mb     -17.72 Mb            30  \n",
      "                                       NllLossBackward0         0.01%       7.000us         0.08%      63.000us      21.000us       0.000us         0.00%      16.000us       5.333us           0 b           0 b       9.13 Mb           0 b             3  \n",
      "                                              aten::cat         0.54%     403.000us         0.57%     430.000us      20.476us      18.000us         0.17%      18.000us       0.857us     327.00 Kb     327.00 Kb      74.50 Kb      74.50 Kb            21  \n",
      "                                     EmbeddingBackward0         0.05%      41.000us         0.08%      58.000us      19.333us       0.000us         0.00%      66.000us      22.000us           0 b           0 b     168.00 Kb     112.00 Kb             3  \n",
      "                                aten::nll_loss_backward         0.03%      24.000us         0.07%      56.000us      18.667us       6.000us         0.06%      16.000us       5.333us           0 b           0 b       9.13 Mb       9.13 Mb             3  \n",
      "                               NativeLayerNormBackward0         0.13%      95.000us         0.43%     326.000us      18.111us       0.000us         0.00%     210.000us      11.667us           0 b           0 b       3.74 Mb     451.00 Kb            18  \n",
      "                               aten::embedding_backward         0.01%       5.000us         0.07%      54.000us      18.000us       0.000us         0.00%     197.000us      65.667us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                       aten::leaky_relu         0.05%      37.000us         0.07%      51.000us      17.000us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      96.00 Kb      96.00 Kb             3  \n",
      "                                        aten::clamp_min         0.09%      69.000us         0.13%      99.000us      16.500us       6.000us         0.06%       6.000us       1.000us           0 b           0 b       2.62 Mb       2.62 Mb             6  \n",
      "                         aten::embedding_dense_backward         0.03%      22.000us         0.07%      49.000us      16.333us     194.000us         1.84%     197.000us      65.667us           0 b           0 b     168.00 Kb           0 b             3  \n",
      "                                        SelectBackward0         0.16%     120.000us         0.64%     484.000us      16.133us       0.000us         0.00%      45.000us       1.500us           0 b           0 b      16.62 Mb       3.72 Mb            30  \n",
      "                                       aten::contiguous         0.20%     153.000us         1.15%     863.000us      15.981us       0.000us         0.00%      66.000us       1.222us      84.00 Kb           0 b      12.36 Mb       2.13 Mb            54  \n",
      "                                              aten::add         0.32%     244.000us         0.44%     328.000us      15.619us      34.000us         0.32%      34.000us       1.619us           0 b           0 b       4.38 Mb       4.38 Mb            21  \n",
      "                       aten::native_layer_norm_backward         0.19%     141.000us         0.37%     281.000us      15.611us     252.000us         2.39%     252.000us      14.000us           0 b           0 b       3.96 Mb           0 b            18  \n",
      "                                  aten::select_backward         0.09%      65.000us         0.62%     465.000us      15.500us       0.000us         0.00%      58.000us       1.933us           0 b           0 b      16.62 Mb    -448.00 Kb            30  \n",
      "autograd::engine::evaluate_function: RepeatBackward0...         0.01%       6.000us         0.06%      45.000us      15.000us       0.000us         0.00%       9.000us       3.000us           0 b           0 b    -576.00 Kb    -672.00 Kb             3  \n",
      "autograd::engine::evaluate_function: LogSoftmaxBackw...         0.01%       7.000us         0.06%      43.000us      14.333us       0.000us         0.00%      19.000us       6.333us           0 b           0 b     -10.54 Mb     -13.58 Mb             3  \n",
      "autograd::engine::evaluate_function: LeakyReluBackwa...         0.01%      11.000us         0.06%      43.000us      14.333us       0.000us         0.00%       3.000us       1.000us           0 b           0 b     -96.00 Kb    -160.00 Kb             3  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.01%      10.000us         0.06%      42.000us      14.000us       0.000us         0.00%       3.000us       1.000us           0 b           0 b           0 b     -21.00 Kb             3  \n",
      "                                              aten::all         0.05%      39.000us         0.05%      41.000us      13.667us       0.000us         0.00%       0.000us       0.000us           3 b           3 b           0 b           0 b             3  \n",
      "                                        RepeatBackward0         0.01%       8.000us         0.05%      39.000us      13.000us       0.000us         0.00%       9.000us       3.000us           0 b           0 b      96.00 Kb           0 b             3  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.02%      12.000us         0.10%      74.000us      12.333us       0.000us         0.00%      12.000us       2.000us           0 b           0 b      -2.62 Mb      -3.94 Mb             6  \n",
      "                                            aten::clone         0.60%     452.000us         2.73%       2.053ms      12.220us       0.000us         0.00%     233.000us       1.387us      84.00 Kb     -42.00 Kb      41.18 Mb      -7.60 Mb           168  \n",
      "                                    LogSoftmaxBackward0         0.01%       9.000us         0.05%      36.000us      12.000us       0.000us         0.00%      19.000us       6.333us           0 b           0 b       3.04 Mb      -6.08 Mb             3  \n",
      "autograd::engine::evaluate_function: NativeDropoutBa...        -0.05%     -35.000us         0.36%     274.000us      11.417us       0.000us         0.00%      39.000us       1.625us           0 b           0 b       2.35 Mb      -6.29 Mb            24  \n",
      "autograd::engine::evaluate_function: SplitWithSizesB...         0.02%      16.000us         0.18%     133.000us      11.083us       0.000us         0.00%      18.000us       1.500us           0 b           0 b      -3.00 Kb     -78.00 Kb            12  \n",
      "                                               aten::mm         1.25%     937.000us         1.61%       1.208ms      10.883us       1.200ms        11.36%       1.200ms      10.811us           0 b           0 b      19.51 Mb      19.51 Mb           111  \n",
      "                                           MulBackward0         0.01%       7.000us         0.04%      32.000us      10.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      21.00 Kb           0 b             3  \n",
      "                                     LeakyReluBackward0         0.01%       6.000us         0.04%      32.000us      10.667us       0.000us         0.00%       3.000us       1.000us           0 b           0 b      64.00 Kb     -32.00 Kb             3  \n",
      "                                          ReluBackward0         0.01%      10.000us         0.08%      62.000us      10.333us       0.000us         0.00%      12.000us       2.000us           0 b           0 b       1.31 Mb      -1.31 Mb             6  \n",
      "                                 NativeDropoutBackward0         0.13%     100.000us         0.30%     224.000us       9.333us       0.000us         0.00%      24.000us       1.000us           0 b           0 b       6.51 Mb       2.13 Mb            24  \n",
      "                       aten::_log_softmax_backward_data         0.03%      20.000us         0.04%      27.000us       9.000us      19.000us         0.18%      19.000us       6.333us           0 b           0 b       9.13 Mb       9.13 Mb             3  \n",
      "                                  cudaDeviceSynchronize         0.01%       9.000us         0.01%       9.000us       9.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                SplitWithSizesBackward0         0.03%      22.000us         0.14%     106.000us       8.833us       0.000us         0.00%      17.000us       1.417us           0 b           0 b      75.00 Kb         512 b            12  \n",
      "                          aten::native_dropout_backward         0.14%     102.000us         0.28%     209.000us       8.708us      39.000us         0.37%      39.000us       1.625us           0 b           0 b       6.51 Mb      -1.37 Mb            24  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.05%      41.000us         1.63%       1.222ms       8.667us       0.000us         0.00%     170.000us       1.206us           0 b           0 b    -896.00 Kb     -26.69 Mb           141  \n",
      "                               aten::threshold_backward         0.05%      39.000us         0.07%      52.000us       8.667us      12.000us         0.11%      12.000us       2.000us           0 b           0 b       2.62 Mb       2.62 Mb             6  \n",
      "                              aten::leaky_relu_backward         0.03%      20.000us         0.03%      26.000us       8.667us       3.000us         0.03%       3.000us       1.000us           0 b           0 b      96.00 Kb      96.00 Kb             3  \n",
      "                                              aten::mul         0.14%     107.000us         0.20%     147.000us       8.167us      20.000us         0.19%      20.000us       1.111us           0 b           0 b       2.67 Mb       2.67 Mb            18  \n",
      "                                               aten::ne         0.03%      24.000us         0.03%      24.000us       8.000us       0.000us         0.00%       0.000us       0.000us       5.27 Kb       5.27 Kb           0 b           0 b             3  \n",
      "                                 aten::split_with_sizes         0.12%      89.000us         0.12%      92.000us       7.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                            aten::copy_         1.06%     796.000us         2.21%       1.663ms       7.491us     321.000us         3.04%     325.000us       1.464us      56.00 Kb     -28.00 Kb     -56.00 Kb     -56.00 Kb           222  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 75.177ms\n",
      "Self CUDA time total: 10.564ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dump/profiler_B64_NW1_PF0_indexed.txt','w') as f: \n",
    "    f.write(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isyn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
