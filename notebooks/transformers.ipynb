{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, os\n",
    "sys.path.append('..')\n",
    "from src.networks import TransformerDecoderBlock\n",
    "from src.data import ReactionDataset\n",
    "from src.feature import NUM_LABEL, EOS_LABEL, SOS_LABEL\n",
    "from src.trainer import BaseTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = ReactionDataset(feat_type='cgcnn')\n",
    "DS.from_file('../data/screened_conditional_reaction.pkl.gz', \n",
    "             heat_temp_key=('heat_temp','median'))\n",
    "DS.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.array([d.year for d in DS])\n",
    "train_mask = years < 2016\n",
    "valid_mask = (years >= 2016) & years < 2018\n",
    "test_mask = years >= 2018\n",
    "\n",
    "train_dl = DataLoader(DS, batch_size=64, sampler=SubsetRandomSampler(np.where(train_mask)[0]), collate_fn=DS.cfn)\n",
    "valid_dl = DataLoader(DS, batch_size=256, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)\n",
    "test_dl = DataLoader(DS, batch_size=256, sampler=np.where(train_mask)[0], collate_fn=DS.cfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target torch.Size([256, 7]) cuda:0\n",
      "label torch.Size([256, 7]) cuda:0\n",
      "context torch.Size([256, 92]) cuda:0\n",
      "weight torch.Size([1792]) cuda:0\n",
      "mask torch.Size([1792]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "feat, info = next(iter(valid_dl))\n",
    "for k,v in feat.items():\n",
    "    print(k, v.shape, v.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTrainer(BaseTrainer):\n",
    "    def __init__(self, model, lr, device='cuda', crit=torch.nn.CrossEntropyLoss(reduction='none')):\n",
    "        super().__init__(model, lr, device, crit)\n",
    "    \n",
    "    def _eval_batch(self, batch, compute_loss=True):\n",
    "        feat, _ = batch\n",
    "        n_batch, l_seq = feat['label'].shape\n",
    "        pred = self.model(**feat)\n",
    "        if compute_loss:\n",
    "            _loss = self.crit(pred.reshape(n_batch * l_seq, -1), feat['label'].reshape(-1))\n",
    "#            loss = (_loss * feat['weight'])[feat['mask']].mean()\n",
    "            loss = (_loss * feat['weight']).mean()\n",
    "            return loss, pred.detach().cpu().numpy()\n",
    "        else:\n",
    "            return pred.detach().cpu().numpy()\n",
    "    \n",
    "    def _parse_output(self, batch, output):\n",
    "        feat, info = batch\n",
    "        if self._output is None:\n",
    "            self._output = {\n",
    "                'info' : info,\n",
    "                'pred' : output\n",
    "            }\n",
    "            if feat['weight'] is not None:\n",
    "                self._output.update({\n",
    "                    'label': feat['label'].cpu().numpy(),\n",
    "                    'weight': feat['weight'].cpu().numpy(),\n",
    "                    'mask': feat['mask'].cpu().numpy(),\n",
    "                })\n",
    "        else:\n",
    "            self._output['info'].extend(info)\n",
    "            self._output['pred'] = np.vstack([self._output['pred'], output])\n",
    "            if feat['weight'] is not None:\n",
    "                self._output['label'] = np.vstack([self._output['label'], feat['label'].cpu().numpy()])\n",
    "                self._output['weight'] = np.hstack([self._output['weight'], feat['weight'].cpu().numpy()])\n",
    "                self._output['mask'] = np.hstack([self._output['mask'], feat['mask'].cpu().numpy()])\n",
    "\n",
    "#    def predict(self, dataloader, n_samples=1000):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   3.13832   2.52148   2.52187\n",
      "   1   2.47638   2.18071   2.18079\n",
      "   2   2.21579   1.98539   1.98508\n",
      "   3   2.05256   1.85142   1.85024\n",
      "   4   1.94766   1.75977   1.75768\n",
      "   5   1.86269   1.68644   1.68594\n",
      "   6   1.79632   1.62716   1.62804\n",
      "   7   1.74248   1.57574   1.57592\n",
      "   8   1.69888   1.53373   1.53657\n",
      "   9   1.66051   1.49665   1.49627\n",
      "  10   1.62338   1.46147   1.45949\n",
      "  11   1.59392   1.43348   1.43367\n",
      "  12   1.56763   1.40294   1.40394\n",
      "  13   1.54363   1.38331   1.38248\n",
      "  14   1.52056   1.36490   1.36676\n",
      "  15   1.50192   1.33983   1.34271\n",
      "  16   1.48459   1.32154   1.32077\n",
      "  17   1.46405   1.30720   1.31126\n",
      "  18   1.45042   1.29123   1.29500\n",
      "  19   1.43399   1.27776   1.27806\n",
      "  20   1.42522   1.26168   1.26235\n",
      "  21   1.40874   1.25202   1.25043\n",
      "  22   1.39563   1.24982   1.25063\n",
      "  23   1.38589   1.22967   1.23000\n",
      "  24   1.37682   1.21290   1.21432\n",
      "  25   1.36640   1.20665   1.20634\n",
      "  26   1.35416   1.19996   1.20039\n",
      "  27   1.34338   1.18648   1.18534\n",
      "  28   1.33754   1.18225   1.18132\n",
      "  29   1.32805   1.17092   1.17076\n",
      "  30   1.32365   1.16325   1.16567\n",
      "  31   1.31546   1.15355   1.15267\n",
      "  32   1.30480   1.14646   1.14706\n",
      "  33   1.29977   1.14353   1.14290\n",
      "  34   1.29226   1.13685   1.13619\n",
      "  35   1.28371   1.13285   1.13172\n",
      "  36   1.28019   1.12433   1.12628\n",
      "  37   1.27266   1.11433   1.11477\n",
      "  38   1.27161   1.11844   1.11625\n",
      "  39   1.26288   1.10574   1.10777\n",
      "  40   1.25778   1.10578   1.10494\n",
      "  41   1.25265   1.09596   1.09041\n",
      "  42   1.24914   1.09371   1.09463\n",
      "  43   1.24329   1.08656   1.08373\n",
      "  44   1.23349   1.08259   1.08024\n",
      "  45   1.23197   1.08169   1.07845\n",
      "  46   1.22936   1.06930   1.06921\n",
      "  47   1.22466   1.06754   1.06970\n",
      "  48   1.22558   1.06450   1.06626\n",
      "  49   1.21766   1.05600   1.05800\n",
      "  50   1.21361   1.05736   1.05855\n",
      "  51   1.20827   1.05354   1.05460\n",
      "  52   1.20589   1.05142   1.05113\n",
      "  53   1.20542   1.04306   1.04263\n",
      "  54   1.19963   1.04251   1.03867\n",
      "  55   1.19672   1.03335   1.03813\n",
      "  56   1.19341   1.03248   1.03312\n",
      "  57   1.19074   1.03480   1.03768\n",
      "  58   1.18616   1.03097   1.02785\n",
      "  59   1.18432   1.02597   1.02375\n",
      "  60   1.17686   1.01971   1.02334\n",
      "  61   1.17690   1.01546   1.01735\n",
      "  62   1.17060   1.01793   1.01912\n",
      "  63   1.17228   1.01690   1.01413\n",
      "  64   1.16835   1.01258   1.01234\n",
      "  65   1.16801   1.01021   1.00908\n",
      "  66   1.16462   1.00674   1.00730\n",
      "  67   1.16108   1.00421   1.00179\n",
      "  68   1.15929   1.00214   1.00323\n",
      "  69   1.15616   0.99590   0.99635\n",
      "  70   1.15506   0.99370   0.99565\n",
      "  71   1.14644   0.98601   0.98780\n",
      "  72   1.14850   0.99198   0.99109\n",
      "  73   1.14740   0.99249   0.99107\n",
      "  74   1.14521   0.98791   0.98854\n",
      "  75   1.14262   0.98742   0.98761\n",
      "  76   1.13892   0.98144   0.98456\n",
      "  77   1.13760   0.97871   0.98242\n",
      "  78   1.13417   0.97907   0.97879\n",
      "  79   1.13980   0.97945   0.97920\n",
      "  80   1.13109   0.97370   0.97389\n",
      "  81   1.12957   0.96968   0.97247\n",
      "  82   1.12971   0.97017   0.97158\n",
      "  83   1.12282   0.97100   0.97066\n",
      "  84   1.12311   0.96739   0.96406\n",
      "  85   1.12194   0.97198   0.97261\n",
      "  86   1.12043   0.96696   0.96356\n",
      "  87   1.11813   0.96157   0.96289\n",
      "  88   1.11805   0.95984   0.95607\n",
      "  89   1.11437   0.95654   0.95447\n",
      "  90   1.11069   0.95811   0.95745\n",
      "  91   1.11505   0.95210   0.95519\n",
      "  92   1.10895   0.95507   0.95073\n",
      "  93   1.11023   0.95073   0.95089\n",
      "  94   1.10638   0.94975   0.94970\n",
      "  95   1.10526   0.94587   0.94964\n",
      "  96   1.10292   0.94018   0.94348\n",
      "  97   1.09901   0.94550   0.94346\n",
      "  98   1.09794   0.94748   0.94760\n",
      "  99   1.09638   0.94455   0.94508\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderBlock(DS.num_condition_feat, num_heads=4, hidden_dim=64, hidden_layers=4)\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(100):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.99951   1.44498   1.44493\n",
      "   1   1.32844   1.18909   1.18908\n",
      "   2   1.15493   1.06664   1.06657\n",
      "   3   1.05857   0.98836   0.98805\n",
      "   4   0.99526   0.93284   0.93303\n",
      "   5   0.94666   0.88761   0.88701\n",
      "   6   0.90799   0.85305   0.85327\n",
      "   7   0.87553   0.82254   0.82096\n",
      "   8   0.85084   0.79534   0.79469\n",
      "   9   0.82855   0.77896   0.77786\n",
      "  10   0.80872   0.75424   0.75411\n",
      "  11   0.79169   0.73576   0.73554\n",
      "  12   0.77698   0.72410   0.72531\n",
      "  13   0.76392   0.70815   0.70855\n",
      "  14   0.75060   0.69569   0.69644\n",
      "  15   0.73886   0.68417   0.68380\n",
      "  16   0.73034   0.67255   0.67398\n",
      "  17   0.72001   0.66320   0.66334\n",
      "  18   0.71236   0.65547   0.65561\n",
      "  19   0.70242   0.64979   0.64865\n",
      "  20   0.69550   0.64075   0.64030\n",
      "  21   0.69033   0.63113   0.63170\n",
      "  22   0.68170   0.62679   0.62562\n",
      "  23   0.67514   0.61784   0.61814\n",
      "  24   0.66914   0.61233   0.61180\n",
      "  25   0.66434   0.60926   0.60969\n",
      "  26   0.65751   0.60311   0.60178\n",
      "  27   0.65465   0.59412   0.59472\n",
      "  28   0.64847   0.59000   0.59011\n",
      "  29   0.64517   0.58822   0.58704\n",
      "  30   0.63950   0.58575   0.58497\n",
      "  31   0.63599   0.57945   0.57940\n",
      "  32   0.63208   0.57600   0.57547\n",
      "  33   0.62906   0.57450   0.57423\n",
      "  34   0.62433   0.56648   0.56641\n",
      "  35   0.62137   0.56111   0.56050\n",
      "  36   0.61803   0.55912   0.55978\n",
      "  37   0.61653   0.55797   0.55798\n",
      "  38   0.61084   0.55661   0.55716\n",
      "  39   0.60936   0.55276   0.55382\n",
      "  40   0.60655   0.55022   0.55036\n",
      "  41   0.60315   0.54482   0.54513\n",
      "  42   0.60058   0.54696   0.54719\n",
      "  43   0.59875   0.54501   0.54520\n",
      "  44   0.59448   0.53836   0.53844\n",
      "  45   0.59076   0.53233   0.53326\n",
      "  46   0.58934   0.52979   0.52889\n",
      "  47   0.58716   0.53203   0.53233\n",
      "  48   0.58503   0.52671   0.52761\n",
      "  49   0.58242   0.52384   0.52404\n",
      "  50   0.58009   0.52084   0.52117\n",
      "  51   0.57880   0.52606   0.52457\n",
      "  52   0.57587   0.51772   0.51758\n",
      "  53   0.57417   0.51880   0.51896\n",
      "  54   0.57150   0.51318   0.51399\n",
      "  55   0.57121   0.51275   0.51327\n",
      "  56   0.57068   0.50851   0.50873\n",
      "  57   0.56630   0.50676   0.50752\n",
      "  58   0.56614   0.50819   0.50707\n",
      "  59   0.56388   0.50407   0.50411\n",
      "  60   0.56163   0.50279   0.50174\n",
      "  61   0.56025   0.50586   0.50551\n",
      "  62   0.55872   0.50086   0.50016\n",
      "  63   0.55761   0.49874   0.49882\n",
      "  64   0.55613   0.50020   0.50024\n",
      "  65   0.55494   0.49490   0.49479\n",
      "  66   0.55083   0.49229   0.49129\n",
      "  67   0.55134   0.49408   0.49454\n",
      "  68   0.55016   0.49049   0.48975\n",
      "  69   0.54970   0.48922   0.48841\n",
      "  70   0.54782   0.48929   0.48865\n",
      "  71   0.54489   0.48741   0.48870\n",
      "  72   0.54564   0.48565   0.48569\n",
      "  73   0.54372   0.48766   0.48704\n",
      "  74   0.54349   0.48518   0.48502\n",
      "  75   0.53987   0.48115   0.48079\n",
      "  76   0.53964   0.48018   0.47986\n",
      "  77   0.53940   0.48518   0.48496\n",
      "  78   0.53763   0.47931   0.47872\n",
      "  79   0.53708   0.47658   0.47676\n",
      "  80   0.53553   0.47503   0.47396\n",
      "  81   0.53260   0.47430   0.47522\n",
      "  82   0.53346   0.47301   0.47389\n",
      "  83   0.53290   0.47494   0.47383\n",
      "  84   0.53128   0.47643   0.47676\n",
      "  85   0.53098   0.46930   0.47010\n",
      "  86   0.52902   0.46807   0.46844\n",
      "  87   0.52821   0.46770   0.46837\n",
      "  88   0.52718   0.46771   0.46738\n",
      "  89   0.52610   0.46675   0.46745\n",
      "  90   0.52580   0.46681   0.46685\n",
      "  91   0.52480   0.46604   0.46637\n",
      "  92   0.52458   0.46370   0.46419\n",
      "  93   0.52457   0.46431   0.46415\n",
      "  94   0.51971   0.46270   0.46372\n",
      "  95   0.52000   0.46052   0.45975\n",
      "  96   0.51942   0.45893   0.45915\n",
      "  97   0.51971   0.45970   0.45935\n",
      "  98   0.51883   0.45833   0.45912\n",
      "  99   0.51738   0.45895   0.45705\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoderBlock(DS.num_condition_feat, num_heads=4, hidden_dim=64, hidden_layers=4)\n",
    "tr = SequenceTrainer(model, lr=1e-4)\n",
    "for i in range(100):\n",
    "    train_loss = tr.train(train_dl)\n",
    "    valid_loss, out = tr.test(valid_dl)\n",
    "    test_loss, out = tr.test(test_dl)\n",
    "    print('{:4d} {:9.5f} {:9.5f} {:9.5f}'.format(i, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0],\n",
       "       [3, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pred'].argmax(-1)[:30, :5] - out['label'][:30, :5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isyn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
